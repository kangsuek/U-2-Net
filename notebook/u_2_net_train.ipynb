{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "u_2_net_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLJNgbtSG-e2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a9e14d5c-69c8-42e9-8468-b9dfeb73b242"
      },
      "source": [
        "# clone git\n",
        "!git clone https://github.com/kangsuek/U-2-Net"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'U-2-Net'...\n",
            "remote: Enumerating objects: 117, done.\u001b[K\n",
            "remote: Counting objects: 100% (117/117), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 117 (delta 16), reused 108 (delta 7), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (117/117), 7.77 MiB | 9.10 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fShcznzKKbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ea6df882-1bba-41df-d0c4-1b8dbf0a565a"
      },
      "source": [
        "# CUDA version 확인\n",
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkuXtnKfKNAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GPU 정보 조회\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxEuPOrqKR0E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bbda5c38-3a10-4fbc-d048-9e0b66187dff"
      },
      "source": [
        "# google drive 연결\n",
        "%cd ..\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KDGJCzoKS3Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99d95192-fba9-4373-fb3e-4f9522dde910"
      },
      "source": [
        "# google drive와 symbolic link를 생성한다. /mydrive 가 google drive의 root dir임\n",
        "!ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "!ls /mydrive"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " colab\t'Colab Notebooks'   생활비.gsheet  'My Drive'   다함께Pyton\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAzb3l7rKedM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd3670b0-0a39-4040-beb2-d1bd996d9ab0"
      },
      "source": [
        "# GOOGLE DRIVE DOWNLOAD\n",
        "# pretrained model을 복사한다.\n",
        "!cp -r /mydrive/colab/U-2-Net/saved_models /content/U-2-Net/\n",
        "\n",
        "! ls /content/U-2-Net/saved_models"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "u2net  u2netp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu4LnpJJVpUs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e5ada04a-1866-40ae-f387-27c115351060"
      },
      "source": [
        "# train_data 디렉토리를 복사한다.\n",
        "!cp /mydrive/colab/U-2-Net/test_data/test_images/*.* /content/U-2-Net/train_data/images/\n",
        "!ls /content/U-2-Net/train_data/images/ -l | grep ^- | wc -l\n",
        "\n",
        "!cp /mydrive/colab/U-2-Net/test_data/u2net_results/*.* /content/U-2-Net/train_data/labels/\n",
        "!ls /content/U-2-Net/train_data/labels/ -l | grep ^- | wc -l\n",
        "\n",
        "# empty 파일을 삭제\n",
        "!rm -f /content/U-2-Net/train_data/images/empty\n",
        "!rm -f /content/U-2-Net/train_data/labels/empty"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "841\n",
            "841\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUwXcTcvV3dE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e51bc728-4869-4505-ba42-a3f868344e65"
      },
      "source": [
        "# training\n",
        "!cd /content/U-2-Net\n",
        "\n",
        "!python /content/U-2-Net/u2net_train.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "\n",
            "[epoch:   6/100000, batch:   408/  840, ite: 384] train loss: 2.639904, tar: 0.355768 \n",
            "l0: 0.411035, l1: 0.439305, l2: 0.430840, l3: 0.474158, l4: 0.455227, l5: 0.459278, l6: 0.486483\n",
            "\n",
            "[epoch:   6/100000, batch:   420/  840, ite: 385] train loss: 2.641245, tar: 0.355912 \n",
            "l0: 0.215275, l1: 0.201362, l2: 0.215913, l3: 0.224485, l4: 0.225863, l5: 0.295088, l6: 0.334680\n",
            "\n",
            "[epoch:   6/100000, batch:   432/  840, ite: 386] train loss: 2.638839, tar: 0.355547 \n",
            "l0: 0.269839, l1: 0.272470, l2: 0.284568, l3: 0.302662, l4: 0.315148, l5: 0.323800, l6: 0.344840\n",
            "\n",
            "[epoch:   6/100000, batch:   444/  840, ite: 387] train loss: 2.637481, tar: 0.355326 \n",
            "l0: 0.262505, l1: 0.296557, l2: 0.266085, l3: 0.282421, l4: 0.291704, l5: 0.272123, l6: 0.301263\n",
            "\n",
            "[epoch:   6/100000, batch:   456/  840, ite: 388] train loss: 2.635768, tar: 0.355087 \n",
            "l0: 0.284749, l1: 0.337953, l2: 0.290136, l3: 0.290756, l4: 0.281207, l5: 0.281800, l6: 0.310481\n",
            "\n",
            "[epoch:   6/100000, batch:   468/  840, ite: 389] train loss: 2.634332, tar: 0.354906 \n",
            "l0: 0.187027, l1: 0.186296, l2: 0.200125, l3: 0.214039, l4: 0.203546, l5: 0.206248, l6: 0.223990\n",
            "\n",
            "[epoch:   6/100000, batch:   480/  840, ite: 390] train loss: 2.631221, tar: 0.354475 \n",
            "l0: 0.384110, l1: 0.392457, l2: 0.379292, l3: 0.392499, l4: 0.400292, l5: 0.437165, l6: 0.463299\n",
            "\n",
            "[epoch:   6/100000, batch:   492/  840, ite: 391] train loss: 2.631778, tar: 0.354551 \n",
            "l0: 0.330457, l1: 0.346784, l2: 0.336239, l3: 0.339789, l4: 0.333058, l5: 0.362546, l6: 0.407837\n",
            "\n",
            "[epoch:   6/100000, batch:   504/  840, ite: 392] train loss: 2.631332, tar: 0.354490 \n",
            "l0: 0.295381, l1: 0.285186, l2: 0.286328, l3: 0.307040, l4: 0.320239, l5: 0.356267, l6: 0.387874\n",
            "\n",
            "[epoch:   6/100000, batch:   516/  840, ite: 393] train loss: 2.630332, tar: 0.354339 \n",
            "l0: 0.480847, l1: 0.471200, l2: 0.464059, l3: 0.532731, l4: 0.535372, l5: 0.570660, l6: 0.577983\n",
            "\n",
            "[epoch:   6/100000, batch:   528/  840, ite: 394] train loss: 2.632876, tar: 0.354660 \n",
            "l0: 0.322542, l1: 0.338265, l2: 0.325346, l3: 0.360750, l4: 0.359378, l5: 0.370375, l6: 0.378613\n",
            "\n",
            "[epoch:   6/100000, batch:   540/  840, ite: 395] train loss: 2.632427, tar: 0.354579 \n",
            "l0: 0.404352, l1: 0.400953, l2: 0.434069, l3: 0.423969, l4: 0.428689, l5: 0.411827, l6: 0.418793\n",
            "\n",
            "[epoch:   6/100000, batch:   552/  840, ite: 396] train loss: 2.633159, tar: 0.354705 \n",
            "l0: 0.216052, l1: 0.223330, l2: 0.229749, l3: 0.231136, l4: 0.244849, l5: 0.245449, l6: 0.265550\n",
            "\n",
            "[epoch:   6/100000, batch:   564/  840, ite: 397] train loss: 2.630698, tar: 0.354355 \n",
            "l0: 0.315813, l1: 0.298062, l2: 0.311322, l3: 0.326029, l4: 0.335191, l5: 0.362730, l6: 0.390474\n",
            "\n",
            "[epoch:   6/100000, batch:   576/  840, ite: 398] train loss: 2.629967, tar: 0.354259 \n",
            "l0: 0.430853, l1: 0.378594, l2: 0.384536, l3: 0.439916, l4: 0.437244, l5: 0.492087, l6: 0.563371\n",
            "\n",
            "[epoch:   6/100000, batch:   588/  840, ite: 399] train loss: 2.631212, tar: 0.354451 \n",
            "l0: 0.334619, l1: 0.341362, l2: 0.347493, l3: 0.344398, l4: 0.347769, l5: 0.366657, l6: 0.372251\n",
            "\n",
            "[epoch:   6/100000, batch:   600/  840, ite: 400] train loss: 2.630770, tar: 0.354401 \n",
            "l0: 0.412130, l1: 0.395197, l2: 0.415980, l3: 0.430825, l4: 0.416785, l5: 0.453686, l6: 0.485402\n",
            "\n",
            "[epoch:   6/100000, batch:   612/  840, ite: 401] train loss: 2.631716, tar: 0.354545 \n",
            "l0: 0.328781, l1: 0.304918, l2: 0.321894, l3: 0.369899, l4: 0.354056, l5: 0.378367, l6: 0.408351\n",
            "\n",
            "[epoch:   6/100000, batch:   624/  840, ite: 402] train loss: 2.631304, tar: 0.354481 \n",
            "l0: 0.223624, l1: 0.222741, l2: 0.221830, l3: 0.266405, l4: 0.260542, l5: 0.272556, l6: 0.305013\n",
            "\n",
            "[epoch:   6/100000, batch:   636/  840, ite: 403] train loss: 2.629174, tar: 0.354156 \n",
            "l0: 0.269594, l1: 0.262170, l2: 0.269468, l3: 0.303597, l4: 0.287295, l5: 0.319129, l6: 0.360129\n",
            "\n",
            "[epoch:   6/100000, batch:   648/  840, ite: 404] train loss: 2.627793, tar: 0.353947 \n",
            "l0: 0.354009, l1: 0.376193, l2: 0.369462, l3: 0.382145, l4: 0.367011, l5: 0.395921, l6: 0.434552\n",
            "\n",
            "[epoch:   6/100000, batch:   660/  840, ite: 405] train loss: 2.627920, tar: 0.353947 \n",
            "l0: 0.314371, l1: 0.324499, l2: 0.320732, l3: 0.351993, l4: 0.331892, l5: 0.348239, l6: 0.377041\n",
            "\n",
            "[epoch:   6/100000, batch:   672/  840, ite: 406] train loss: 2.627282, tar: 0.353850 \n",
            "l0: 0.318536, l1: 0.333737, l2: 0.355650, l3: 0.335917, l4: 0.333241, l5: 0.330627, l6: 0.335634\n",
            "\n",
            "[epoch:   6/100000, batch:   684/  840, ite: 407] train loss: 2.626585, tar: 0.353763 \n",
            "l0: 0.293694, l1: 0.292508, l2: 0.295398, l3: 0.319702, l4: 0.306779, l5: 0.329088, l6: 0.378627\n",
            "\n",
            "[epoch:   6/100000, batch:   696/  840, ite: 408] train loss: 2.625578, tar: 0.353616 \n",
            "l0: 0.250001, l1: 0.256335, l2: 0.282738, l3: 0.278852, l4: 0.279280, l5: 0.282411, l6: 0.299122\n",
            "\n",
            "[epoch:   6/100000, batch:   708/  840, ite: 409] train loss: 2.623874, tar: 0.353362 \n",
            "l0: 0.291246, l1: 0.277412, l2: 0.270970, l3: 0.326218, l4: 0.302953, l5: 0.365294, l6: 0.431098\n",
            "\n",
            "[epoch:   6/100000, batch:   720/  840, ite: 410] train loss: 2.622999, tar: 0.353211 \n",
            "l0: 0.269314, l1: 0.273266, l2: 0.284181, l3: 0.279184, l4: 0.279115, l5: 0.305901, l6: 0.339116\n",
            "\n",
            "[epoch:   6/100000, batch:   732/  840, ite: 411] train loss: 2.621556, tar: 0.353007 \n",
            "l0: 0.464416, l1: 0.464348, l2: 0.508566, l3: 0.438063, l4: 0.466735, l5: 0.452824, l6: 0.469227\n",
            "\n",
            "[epoch:   6/100000, batch:   744/  840, ite: 412] train loss: 2.623116, tar: 0.353277 \n",
            "l0: 0.274750, l1: 0.296796, l2: 0.287085, l3: 0.307600, l4: 0.295667, l5: 0.298798, l6: 0.320916\n",
            "\n",
            "[epoch:   6/100000, batch:   756/  840, ite: 413] train loss: 2.621805, tar: 0.353087 \n",
            "l0: 0.258512, l1: 0.249187, l2: 0.263706, l3: 0.269659, l4: 0.273175, l5: 0.296317, l6: 0.331569\n",
            "\n",
            "[epoch:   6/100000, batch:   768/  840, ite: 414] train loss: 2.620163, tar: 0.352858 \n",
            "l0: 0.251400, l1: 0.256581, l2: 0.258893, l3: 0.276003, l4: 0.266099, l5: 0.289957, l6: 0.316235\n",
            "\n",
            "[epoch:   6/100000, batch:   780/  840, ite: 415] train loss: 2.618464, tar: 0.352614 \n",
            "l0: 0.223623, l1: 0.222440, l2: 0.227527, l3: 0.246533, l4: 0.247265, l5: 0.258596, l6: 0.296907\n",
            "\n",
            "[epoch:   6/100000, batch:   792/  840, ite: 416] train loss: 2.616312, tar: 0.352304 \n",
            "l0: 0.200681, l1: 0.205891, l2: 0.197890, l3: 0.224451, l4: 0.220490, l5: 0.248729, l6: 0.300440\n",
            "\n",
            "[epoch:   6/100000, batch:   804/  840, ite: 417] train loss: 2.613871, tar: 0.351940 \n",
            "l0: 0.275643, l1: 0.277929, l2: 0.281587, l3: 0.303693, l4: 0.300901, l5: 0.322826, l6: 0.336041\n",
            "\n",
            "[epoch:   6/100000, batch:   816/  840, ite: 418] train loss: 2.612638, tar: 0.351758 \n",
            "l0: 0.291052, l1: 0.297011, l2: 0.301030, l3: 0.301113, l4: 0.308472, l5: 0.320559, l6: 0.349750\n",
            "\n",
            "[epoch:   6/100000, batch:   828/  840, ite: 419] train loss: 2.611580, tar: 0.351613 \n",
            "l0: 0.389061, l1: 0.389823, l2: 0.402970, l3: 0.381919, l4: 0.404030, l5: 0.417417, l6: 0.426360\n",
            "\n",
            "[epoch:   6/100000, batch:   840/  840, ite: 420] train loss: 2.612056, tar: 0.351702 \n",
            "l0: 0.349661, l1: 0.330561, l2: 0.356836, l3: 0.396916, l4: 0.389567, l5: 0.414847, l6: 0.439459\n",
            "\n",
            "[epoch:   7/100000, batch:    12/  840, ite: 421] train loss: 2.612212, tar: 0.351697 \n",
            "l0: 0.323387, l1: 0.333420, l2: 0.352541, l3: 0.343526, l4: 0.348316, l5: 0.346587, l6: 0.355789\n",
            "\n",
            "[epoch:   7/100000, batch:    24/  840, ite: 422] train loss: 2.611717, tar: 0.351630 \n",
            "l0: 0.356206, l1: 0.367383, l2: 0.352988, l3: 0.363451, l4: 0.368927, l5: 0.380820, l6: 0.391333\n",
            "\n",
            "[epoch:   7/100000, batch:    36/  840, ite: 423] train loss: 2.611645, tar: 0.351641 \n",
            "l0: 0.329386, l1: 0.326383, l2: 0.354628, l3: 0.345098, l4: 0.344199, l5: 0.344522, l6: 0.342641\n",
            "\n",
            "[epoch:   7/100000, batch:    48/  840, ite: 424] train loss: 2.611115, tar: 0.351588 \n",
            "l0: 0.361354, l1: 0.337143, l2: 0.343226, l3: 0.347621, l4: 0.365415, l5: 0.433124, l6: 0.476812\n",
            "\n",
            "[epoch:   7/100000, batch:    60/  840, ite: 425] train loss: 2.611241, tar: 0.351611 \n",
            "l0: 0.318190, l1: 0.316911, l2: 0.322676, l3: 0.310597, l4: 0.322299, l5: 0.340831, l6: 0.363435\n",
            "\n",
            "[epoch:   7/100000, batch:    72/  840, ite: 426] train loss: 2.610498, tar: 0.351533 \n",
            "l0: 0.263146, l1: 0.271036, l2: 0.266052, l3: 0.266223, l4: 0.270919, l5: 0.288759, l6: 0.307077\n",
            "\n",
            "[epoch:   7/100000, batch:    84/  840, ite: 427] train loss: 2.608912, tar: 0.351326 \n",
            "l0: 0.254846, l1: 0.257670, l2: 0.268175, l3: 0.254644, l4: 0.266502, l5: 0.269980, l6: 0.285281\n",
            "\n",
            "[epoch:   7/100000, batch:    96/  840, ite: 428] train loss: 2.607155, tar: 0.351101 \n",
            "l0: 0.319478, l1: 0.310681, l2: 0.317138, l3: 0.323580, l4: 0.320937, l5: 0.347992, l6: 0.394111\n",
            "\n",
            "[epoch:   7/100000, batch:   108/  840, ite: 429] train loss: 2.606519, tar: 0.351027 \n",
            "l0: 0.244738, l1: 0.225716, l2: 0.247766, l3: 0.255021, l4: 0.270341, l5: 0.287199, l6: 0.305280\n",
            "\n",
            "[epoch:   7/100000, batch:   120/  840, ite: 430] train loss: 2.604727, tar: 0.350780 \n",
            "l0: 0.288745, l1: 0.263993, l2: 0.287154, l3: 0.321711, l4: 0.327243, l5: 0.333409, l6: 0.342321\n",
            "\n",
            "[epoch:   7/100000, batch:   132/  840, ite: 431] train loss: 2.603705, tar: 0.350636 \n",
            "l0: 0.460668, l1: 0.421922, l2: 0.446081, l3: 0.521876, l4: 0.514098, l5: 0.514545, l6: 0.520478\n",
            "\n",
            "[epoch:   7/100000, batch:   144/  840, ite: 432] train loss: 2.605548, tar: 0.350890 \n",
            "l0: 0.239358, l1: 0.233228, l2: 0.237312, l3: 0.239774, l4: 0.253393, l5: 0.266590, l6: 0.313744\n",
            "\n",
            "[epoch:   7/100000, batch:   156/  840, ite: 433] train loss: 2.603649, tar: 0.350633 \n",
            "l0: 0.365576, l1: 0.383954, l2: 0.379058, l3: 0.372875, l4: 0.378140, l5: 0.373447, l6: 0.406645\n",
            "\n",
            "[epoch:   7/100000, batch:   168/  840, ite: 434] train loss: 2.603778, tar: 0.350667 \n",
            "l0: 0.198549, l1: 0.178660, l2: 0.194569, l3: 0.214426, l4: 0.232705, l5: 0.247037, l6: 0.256408\n",
            "\n",
            "[epoch:   7/100000, batch:   180/  840, ite: 435] train loss: 2.601292, tar: 0.350318 \n",
            "l0: 0.234456, l1: 0.215502, l2: 0.233793, l3: 0.259256, l4: 0.275074, l5: 0.297603, l6: 0.303454\n",
            "\n",
            "[epoch:   7/100000, batch:   192/  840, ite: 436] train loss: 2.599498, tar: 0.350052 \n",
            "l0: 0.297369, l1: 0.289028, l2: 0.302464, l3: 0.345097, l4: 0.344910, l5: 0.367240, l6: 0.365402\n",
            "\n",
            "[epoch:   7/100000, batch:   204/  840, ite: 437] train loss: 2.598839, tar: 0.349931 \n",
            "l0: 0.220968, l1: 0.202498, l2: 0.224809, l3: 0.231351, l4: 0.242977, l5: 0.252587, l6: 0.290727\n",
            "\n",
            "[epoch:   7/100000, batch:   216/  840, ite: 438] train loss: 2.596709, tar: 0.349637 \n",
            "l0: 0.281916, l1: 0.262097, l2: 0.278830, l3: 0.312069, l4: 0.320313, l5: 0.331783, l6: 0.378229\n",
            "\n",
            "[epoch:   7/100000, batch:   228/  840, ite: 439] train loss: 2.595726, tar: 0.349483 \n",
            "l0: 0.205561, l1: 0.201284, l2: 0.198477, l3: 0.222758, l4: 0.232265, l5: 0.241649, l6: 0.280442\n",
            "\n",
            "[epoch:   7/100000, batch:   240/  840, ite: 440] train loss: 2.593423, tar: 0.349155 \n",
            "l0: 0.493492, l1: 0.597387, l2: 0.522514, l3: 0.499586, l4: 0.472115, l5: 0.492516, l6: 0.500021\n",
            "\n",
            "[epoch:   7/100000, batch:   252/  840, ite: 441] train loss: 2.595655, tar: 0.349483 \n",
            "l0: 0.271503, l1: 0.270296, l2: 0.272173, l3: 0.288807, l4: 0.299587, l5: 0.306630, l6: 0.332339\n",
            "\n",
            "[epoch:   7/100000, batch:   264/  840, ite: 442] train loss: 2.594401, tar: 0.349306 \n",
            "l0: 0.234655, l1: 0.223330, l2: 0.234726, l3: 0.262886, l4: 0.261424, l5: 0.269304, l6: 0.310502\n",
            "\n",
            "[epoch:   7/100000, batch:   276/  840, ite: 443] train loss: 2.592601, tar: 0.349048 \n",
            "l0: 0.214540, l1: 0.205394, l2: 0.207803, l3: 0.237389, l4: 0.241288, l5: 0.267900, l6: 0.317328\n",
            "\n",
            "[epoch:   7/100000, batch:   288/  840, ite: 444] train loss: 2.590572, tar: 0.348745 \n",
            "l0: 0.398789, l1: 0.402499, l2: 0.391156, l3: 0.383339, l4: 0.391063, l5: 0.425471, l6: 0.493335\n",
            "\n",
            "[epoch:   7/100000, batch:   300/  840, ite: 445] train loss: 2.591235, tar: 0.348857 \n",
            "l0: 0.442184, l1: 0.431648, l2: 0.448731, l3: 0.483604, l4: 0.473565, l5: 0.457067, l6: 0.488378\n",
            "\n",
            "[epoch:   7/100000, batch:   312/  840, ite: 446] train loss: 2.592656, tar: 0.349066 \n",
            "l0: 0.361737, l1: 0.358292, l2: 0.354473, l3: 0.394774, l4: 0.414978, l5: 0.419147, l6: 0.444972\n",
            "\n",
            "[epoch:   7/100000, batch:   324/  840, ite: 447] train loss: 2.593005, tar: 0.349095 \n",
            "l0: 0.218691, l1: 0.219479, l2: 0.219707, l3: 0.237712, l4: 0.233845, l5: 0.253699, l6: 0.276226\n",
            "\n",
            "[epoch:   7/100000, batch:   336/  840, ite: 448] train loss: 2.590921, tar: 0.348804 \n",
            "l0: 0.244144, l1: 0.231410, l2: 0.235608, l3: 0.278008, l4: 0.270952, l5: 0.288925, l6: 0.341481\n",
            "\n",
            "[epoch:   7/100000, batch:   348/  840, ite: 449] train loss: 2.589361, tar: 0.348570 \n",
            "l0: 0.263420, l1: 0.242708, l2: 0.263693, l3: 0.293879, l4: 0.293403, l5: 0.304652, l6: 0.358109\n",
            "\n",
            "[epoch:   7/100000, batch:   360/  840, ite: 450] train loss: 2.588095, tar: 0.348381 \n",
            "l0: 0.288245, l1: 0.282432, l2: 0.306098, l3: 0.348245, l4: 0.332347, l5: 0.317919, l6: 0.331121\n",
            "\n",
            "[epoch:   7/100000, batch:   372/  840, ite: 451] train loss: 2.587249, tar: 0.348248 \n",
            "l0: 0.294006, l1: 0.292645, l2: 0.306042, l3: 0.325296, l4: 0.320955, l5: 0.315109, l6: 0.325509\n",
            "\n",
            "[epoch:   7/100000, batch:   384/  840, ite: 452] train loss: 2.586347, tar: 0.348128 \n",
            "l0: 0.202319, l1: 0.219915, l2: 0.208525, l3: 0.227618, l4: 0.227222, l5: 0.232140, l6: 0.237297\n",
            "\n",
            "[epoch:   7/100000, batch:   396/  840, ite: 453] train loss: 2.584070, tar: 0.347806 \n",
            "l0: 0.315401, l1: 0.298828, l2: 0.309292, l3: 0.328918, l4: 0.341646, l5: 0.353975, l6: 0.378974\n",
            "\n",
            "[epoch:   7/100000, batch:   408/  840, ite: 454] train loss: 2.583504, tar: 0.347735 \n",
            "l0: 0.241836, l1: 0.249333, l2: 0.251321, l3: 0.262687, l4: 0.259268, l5: 0.265941, l6: 0.281756\n",
            "\n",
            "[epoch:   7/100000, batch:   420/  840, ite: 455] train loss: 2.581809, tar: 0.347502 \n",
            "l0: 0.285970, l1: 0.274480, l2: 0.278264, l3: 0.314194, l4: 0.323063, l5: 0.321205, l6: 0.345607\n",
            "\n",
            "[epoch:   7/100000, batch:   432/  840, ite: 456] train loss: 2.580846, tar: 0.347367 \n",
            "l0: 0.213314, l1: 0.198069, l2: 0.198382, l3: 0.220350, l4: 0.238427, l5: 0.259132, l6: 0.297880\n",
            "\n",
            "[epoch:   7/100000, batch:   444/  840, ite: 457] train loss: 2.578756, tar: 0.347074 \n",
            "l0: 0.183984, l1: 0.188884, l2: 0.190941, l3: 0.218287, l4: 0.222430, l5: 0.215614, l6: 0.228286\n",
            "\n",
            "[epoch:   7/100000, batch:   456/  840, ite: 458] train loss: 2.576288, tar: 0.346718 \n",
            "l0: 0.533980, l1: 0.551005, l2: 0.583748, l3: 0.518812, l4: 0.520384, l5: 0.527428, l6: 0.526561\n",
            "\n",
            "[epoch:   7/100000, batch:   468/  840, ite: 459] train loss: 2.578871, tar: 0.347126 \n",
            "l0: 0.310030, l1: 0.301349, l2: 0.311218, l3: 0.329606, l4: 0.334220, l5: 0.335868, l6: 0.360781\n",
            "\n",
            "[epoch:   7/100000, batch:   480/  840, ite: 460] train loss: 2.578228, tar: 0.347045 \n",
            "l0: 0.290881, l1: 0.251296, l2: 0.313425, l3: 0.307841, l4: 0.306845, l5: 0.330086, l6: 0.337594\n",
            "\n",
            "[epoch:   7/100000, batch:   492/  840, ite: 461] train loss: 2.577273, tar: 0.346923 \n",
            "l0: 0.357026, l1: 0.347616, l2: 0.352017, l3: 0.381047, l4: 0.371604, l5: 0.347060, l6: 0.403503\n",
            "\n",
            "[epoch:   7/100000, batch:   504/  840, ite: 462] train loss: 2.577235, tar: 0.346945 \n",
            "l0: 0.341845, l1: 0.317262, l2: 0.347972, l3: 0.333502, l4: 0.340228, l5: 0.346296, l6: 0.389428\n",
            "\n",
            "[epoch:   7/100000, batch:   516/  840, ite: 463] train loss: 2.576888, tar: 0.346934 \n",
            "l0: 0.482541, l1: 0.467415, l2: 0.480464, l3: 0.471126, l4: 0.501277, l5: 0.529437, l6: 0.543467\n",
            "\n",
            "[epoch:   7/100000, batch:   528/  840, ite: 464] train loss: 2.578825, tar: 0.347226 \n",
            "l0: 0.329938, l1: 0.330913, l2: 0.359159, l3: 0.359877, l4: 0.344373, l5: 0.361275, l6: 0.369664\n",
            "\n",
            "[epoch:   7/100000, batch:   540/  840, ite: 465] train loss: 2.578560, tar: 0.347189 \n",
            "l0: 0.198297, l1: 0.206526, l2: 0.203156, l3: 0.214629, l4: 0.220178, l5: 0.239436, l6: 0.260775\n",
            "\n",
            "[epoch:   7/100000, batch:   552/  840, ite: 466] train loss: 2.576337, tar: 0.346869 \n",
            "l0: 0.301925, l1: 0.316856, l2: 0.315734, l3: 0.311851, l4: 0.321223, l5: 0.323553, l6: 0.330155\n",
            "\n",
            "[epoch:   7/100000, batch:   564/  840, ite: 467] train loss: 2.575577, tar: 0.346773 \n",
            "l0: 0.254069, l1: 0.255329, l2: 0.262564, l3: 0.260986, l4: 0.275783, l5: 0.288295, l6: 0.305536\n",
            "\n",
            "[epoch:   7/100000, batch:   576/  840, ite: 468] train loss: 2.574139, tar: 0.346575 \n",
            "l0: 0.276923, l1: 0.274581, l2: 0.294413, l3: 0.278020, l4: 0.279649, l5: 0.304497, l6: 0.340368\n",
            "\n",
            "[epoch:   7/100000, batch:   588/  840, ite: 469] train loss: 2.573018, tar: 0.346427 \n",
            "l0: 0.338022, l1: 0.341292, l2: 0.346892, l3: 0.336759, l4: 0.340609, l5: 0.367428, l6: 0.392611\n",
            "\n",
            "[epoch:   7/100000, batch:   600/  840, ite: 470] train loss: 2.572785, tar: 0.346409 \n",
            "l0: 0.252772, l1: 0.253572, l2: 0.282096, l3: 0.253390, l4: 0.256358, l5: 0.250836, l6: 0.268345\n",
            "\n",
            "[epoch:   7/100000, batch:   612/  840, ite: 471] train loss: 2.571182, tar: 0.346210 \n",
            "l0: 0.353673, l1: 0.332086, l2: 0.370690, l3: 0.380640, l4: 0.382581, l5: 0.388970, l6: 0.403086\n",
            "\n",
            "[epoch:   7/100000, batch:   624/  840, ite: 472] train loss: 2.571267, tar: 0.346226 \n",
            "l0: 0.201115, l1: 0.224243, l2: 0.215366, l3: 0.232937, l4: 0.233974, l5: 0.215883, l6: 0.243273\n",
            "\n",
            "[epoch:   7/100000, batch:   636/  840, ite: 473] train loss: 2.569144, tar: 0.345919 \n",
            "l0: 0.281029, l1: 0.277302, l2: 0.281123, l3: 0.309947, l4: 0.310072, l5: 0.332812, l6: 0.358691\n",
            "\n",
            "[epoch:   7/100000, batch:   648/  840, ite: 474] train loss: 2.568261, tar: 0.345782 \n",
            "l0: 0.338931, l1: 0.351354, l2: 0.332086, l3: 0.385838, l4: 0.379681, l5: 0.373316, l6: 0.399423\n",
            "\n",
            "[epoch:   7/100000, batch:   660/  840, ite: 475] train loss: 2.568245, tar: 0.345768 \n",
            "l0: 0.220723, l1: 0.231334, l2: 0.246227, l3: 0.239946, l4: 0.240581, l5: 0.236248, l6: 0.272273\n",
            "\n",
            "[epoch:   7/100000, batch:   672/  840, ite: 476] train loss: 2.566395, tar: 0.345505 \n",
            "l0: 0.203089, l1: 0.204080, l2: 0.208830, l3: 0.231143, l4: 0.228459, l5: 0.230374, l6: 0.261732\n",
            "\n",
            "[epoch:   7/100000, batch:   684/  840, ite: 477] train loss: 2.564301, tar: 0.345206 \n",
            "l0: 0.252984, l1: 0.261413, l2: 0.266578, l3: 0.285879, l4: 0.274423, l5: 0.300990, l6: 0.317006\n",
            "\n",
            "[epoch:   7/100000, batch:   696/  840, ite: 478] train loss: 2.563035, tar: 0.345013 \n",
            "l0: 0.253212, l1: 0.248647, l2: 0.259539, l3: 0.276860, l4: 0.292818, l5: 0.293112, l6: 0.296895\n",
            "\n",
            "[epoch:   7/100000, batch:   708/  840, ite: 479] train loss: 2.561696, tar: 0.344822 \n",
            "l0: 0.264915, l1: 0.273062, l2: 0.262118, l3: 0.307411, l4: 0.295026, l5: 0.299355, l6: 0.308651\n",
            "\n",
            "[epoch:   7/100000, batch:   720/  840, ite: 480] train loss: 2.560547, tar: 0.344655 \n",
            "l0: 0.438106, l1: 0.455328, l2: 0.446915, l3: 0.471793, l4: 0.455270, l5: 0.499964, l6: 0.499624\n",
            "\n",
            "[epoch:   7/100000, batch:   732/  840, ite: 481] train loss: 2.562016, tar: 0.344850 \n",
            "l0: 0.253933, l1: 0.236022, l2: 0.260136, l3: 0.328466, l4: 0.297745, l5: 0.306935, l6: 0.308870\n",
            "\n",
            "[epoch:   7/100000, batch:   744/  840, ite: 482] train loss: 2.560833, tar: 0.344661 \n",
            "l0: 0.436362, l1: 0.411385, l2: 0.408967, l3: 0.526133, l4: 0.516256, l5: 0.554239, l6: 0.522185\n",
            "\n",
            "[epoch:   7/100000, batch:   756/  840, ite: 483] train loss: 2.562520, tar: 0.344851 \n",
            "l0: 0.222170, l1: 0.219331, l2: 0.238148, l3: 0.246913, l4: 0.229163, l5: 0.292872, l6: 0.344190\n",
            "\n",
            "[epoch:   7/100000, batch:   768/  840, ite: 484] train loss: 2.560930, tar: 0.344597 \n",
            "l0: 0.213465, l1: 0.204898, l2: 0.210529, l3: 0.209724, l4: 0.227838, l5: 0.325002, l6: 0.402646\n",
            "\n",
            "[epoch:   7/100000, batch:   780/  840, ite: 485] train loss: 2.559349, tar: 0.344327 \n",
            "l0: 0.272910, l1: 0.261760, l2: 0.262257, l3: 0.276731, l4: 0.308382, l5: 0.367394, l6: 0.402724\n",
            "\n",
            "[epoch:   7/100000, batch:   792/  840, ite: 486] train loss: 2.558511, tar: 0.344180 \n",
            "l0: 0.238840, l1: 0.247630, l2: 0.239577, l3: 0.267468, l4: 0.273774, l5: 0.300195, l6: 0.322933\n",
            "\n",
            "[epoch:   7/100000, batch:   804/  840, ite: 487] train loss: 2.557139, tar: 0.343964 \n",
            "l0: 0.422101, l1: 0.442746, l2: 0.453258, l3: 0.452196, l4: 0.435244, l5: 0.426346, l6: 0.451240\n",
            "\n",
            "[epoch:   7/100000, batch:   816/  840, ite: 488] train loss: 2.558216, tar: 0.344124 \n",
            "l0: 0.275335, l1: 0.265634, l2: 0.274816, l3: 0.305708, l4: 0.297229, l5: 0.322549, l6: 0.394699\n",
            "\n",
            "[epoch:   7/100000, batch:   828/  840, ite: 489] train loss: 2.557353, tar: 0.343983 \n",
            "l0: 0.471235, l1: 0.492261, l2: 0.499729, l3: 0.485303, l4: 0.496485, l5: 0.483775, l6: 0.537854\n",
            "\n",
            "[epoch:   7/100000, batch:   840/  840, ite: 490] train loss: 2.559209, tar: 0.344243 \n",
            "l0: 0.232956, l1: 0.232052, l2: 0.248696, l3: 0.265005, l4: 0.254941, l5: 0.276285, l6: 0.304026\n",
            "\n",
            "[epoch:   8/100000, batch:    12/  840, ite: 491] train loss: 2.557691, tar: 0.344016 \n",
            "l0: 0.386776, l1: 0.430963, l2: 0.418075, l3: 0.438026, l4: 0.415810, l5: 0.405497, l6: 0.403953\n",
            "\n",
            "[epoch:   8/100000, batch:    24/  840, ite: 492] train loss: 2.558385, tar: 0.344103 \n",
            "l0: 0.209157, l1: 0.214747, l2: 0.220671, l3: 0.247811, l4: 0.231964, l5: 0.237147, l6: 0.251201\n",
            "\n",
            "[epoch:   8/100000, batch:    36/  840, ite: 493] train loss: 2.556467, tar: 0.343829 \n",
            "l0: 0.480533, l1: 0.491878, l2: 0.491825, l3: 0.528789, l4: 0.533989, l5: 0.512533, l6: 0.522863\n",
            "\n",
            "[epoch:   8/100000, batch:    48/  840, ite: 494] train loss: 2.558503, tar: 0.344106 \n",
            "l0: 0.364409, l1: 0.374739, l2: 0.376136, l3: 0.366023, l4: 0.376527, l5: 0.395708, l6: 0.410710\n",
            "\n",
            "[epoch:   8/100000, batch:    60/  840, ite: 495] train loss: 2.558717, tar: 0.344147 \n",
            "l0: 0.224689, l1: 0.219670, l2: 0.242263, l3: 0.249265, l4: 0.259899, l5: 0.234581, l6: 0.252120\n",
            "\n",
            "[epoch:   8/100000, batch:    72/  840, ite: 496] train loss: 2.556950, tar: 0.343906 \n",
            "l0: 0.306404, l1: 0.308611, l2: 0.312550, l3: 0.343413, l4: 0.337075, l5: 0.321700, l6: 0.347322\n",
            "\n",
            "[epoch:   8/100000, batch:    84/  840, ite: 497] train loss: 2.556387, tar: 0.343831 \n",
            "l0: 0.276455, l1: 0.299695, l2: 0.275436, l3: 0.287096, l4: 0.280389, l5: 0.321085, l6: 0.360947\n",
            "\n",
            "[epoch:   8/100000, batch:    96/  840, ite: 498] train loss: 2.555473, tar: 0.343696 \n",
            "l0: 0.289485, l1: 0.286455, l2: 0.304293, l3: 0.306675, l4: 0.300638, l5: 0.310038, l6: 0.346950\n",
            "\n",
            "[epoch:   8/100000, batch:   108/  840, ite: 499] train loss: 2.554649, tar: 0.343587 \n",
            "l0: 0.354757, l1: 0.352596, l2: 0.348549, l3: 0.354438, l4: 0.389415, l5: 0.402726, l6: 0.433917\n",
            "\n",
            "[epoch:   8/100000, batch:   120/  840, ite: 500] train loss: 2.554813, tar: 0.343609 \n",
            "l0: 0.345411, l1: 0.336188, l2: 0.356080, l3: 0.361318, l4: 0.359655, l5: 0.377433, l6: 0.388059\n",
            "\n",
            "[epoch:   8/100000, batch:   132/  840, ite: 501] train loss: 2.554751, tar: 0.343613 \n",
            "l0: 0.281918, l1: 0.286627, l2: 0.286291, l3: 0.286650, l4: 0.294161, l5: 0.297536, l6: 0.316377\n",
            "\n",
            "[epoch:   8/100000, batch:   144/  840, ite: 502] train loss: 2.553745, tar: 0.343490 \n",
            "l0: 0.424095, l1: 0.391318, l2: 0.421676, l3: 0.423008, l4: 0.435854, l5: 0.475949, l6: 0.496822\n",
            "\n",
            "[epoch:   8/100000, batch:   156/  840, ite: 503] train loss: 2.554769, tar: 0.343650 \n",
            "l0: 0.337672, l1: 0.324538, l2: 0.357741, l3: 0.332117, l4: 0.331166, l5: 0.358449, l6: 0.394873\n",
            "\n",
            "[epoch:   8/100000, batch:   168/  840, ite: 504] train loss: 2.554534, tar: 0.343638 \n",
            "l0: 0.241483, l1: 0.230542, l2: 0.246494, l3: 0.248918, l4: 0.251880, l5: 0.263435, l6: 0.305685\n",
            "\n",
            "[epoch:   8/100000, batch:   180/  840, ite: 505] train loss: 2.553017, tar: 0.343436 \n",
            "l0: 0.248846, l1: 0.249546, l2: 0.255959, l3: 0.251887, l4: 0.258083, l5: 0.266121, l6: 0.291721\n",
            "\n",
            "[epoch:   8/100000, batch:   192/  840, ite: 506] train loss: 2.551573, tar: 0.343249 \n",
            "l0: 0.313098, l1: 0.314983, l2: 0.330647, l3: 0.337524, l4: 0.324677, l5: 0.312492, l6: 0.339678\n",
            "\n",
            "[epoch:   8/100000, batch:   204/  840, ite: 507] train loss: 2.551023, tar: 0.343190 \n",
            "l0: 0.184158, l1: 0.176735, l2: 0.189837, l3: 0.198956, l4: 0.195078, l5: 0.210872, l6: 0.242799\n",
            "\n",
            "[epoch:   8/100000, batch:   216/  840, ite: 508] train loss: 2.548755, tar: 0.342877 \n",
            "l0: 0.203828, l1: 0.189413, l2: 0.206896, l3: 0.236588, l4: 0.230598, l5: 0.239575, l6: 0.274142\n",
            "\n",
            "[epoch:   8/100000, batch:   228/  840, ite: 509] train loss: 2.546854, tar: 0.342603 \n",
            "l0: 0.195775, l1: 0.188287, l2: 0.201859, l3: 0.214873, l4: 0.199569, l5: 0.227352, l6: 0.286647\n",
            "\n",
            "[epoch:   8/100000, batch:   240/  840, ite: 510] train loss: 2.544829, tar: 0.342316 \n",
            "l0: 0.260689, l1: 0.261042, l2: 0.259630, l3: 0.286291, l4: 0.288632, l5: 0.320421, l6: 0.349411\n",
            "\n",
            "[epoch:   8/100000, batch:   252/  840, ite: 511] train loss: 2.543814, tar: 0.342156 \n",
            "l0: 0.290240, l1: 0.299374, l2: 0.312011, l3: 0.314703, l4: 0.307733, l5: 0.310182, l6: 0.328163\n",
            "\n",
            "[epoch:   8/100000, batch:   264/  840, ite: 512] train loss: 2.543069, tar: 0.342054 \n",
            "l0: 0.192529, l1: 0.196321, l2: 0.188889, l3: 0.217350, l4: 0.214858, l5: 0.225944, l6: 0.259926\n",
            "\n",
            "[epoch:   8/100000, batch:   276/  840, ite: 513] train loss: 2.541028, tar: 0.341763 \n",
            "l0: 0.253948, l1: 0.235592, l2: 0.274551, l3: 0.277960, l4: 0.269962, l5: 0.281058, l6: 0.316070\n",
            "\n",
            "[epoch:   8/100000, batch:   288/  840, ite: 514] train loss: 2.539798, tar: 0.341592 \n",
            "l0: 0.207494, l1: 0.211376, l2: 0.236503, l3: 0.229221, l4: 0.221566, l5: 0.238506, l6: 0.270896\n",
            "\n",
            "[epoch:   8/100000, batch:   300/  840, ite: 515] train loss: 2.538004, tar: 0.341332 \n",
            "l0: 0.270987, l1: 0.246331, l2: 0.261599, l3: 0.295535, l4: 0.297921, l5: 0.304410, l6: 0.365059\n",
            "\n",
            "[epoch:   8/100000, batch:   312/  840, ite: 516] train loss: 2.537042, tar: 0.341195 \n",
            "l0: 0.426643, l1: 0.457077, l2: 0.453022, l3: 0.460535, l4: 0.475249, l5: 0.449129, l6: 0.421329\n",
            "\n",
            "[epoch:   8/100000, batch:   324/  840, ite: 517] train loss: 2.538214, tar: 0.341361 \n",
            "l0: 0.389775, l1: 0.359025, l2: 0.388951, l3: 0.432450, l4: 0.425847, l5: 0.415460, l6: 0.456337\n",
            "\n",
            "[epoch:   8/100000, batch:   336/  840, ite: 518] train loss: 2.538850, tar: 0.341454 \n",
            "l0: 0.278942, l1: 0.283834, l2: 0.276021, l3: 0.304997, l4: 0.316432, l5: 0.318870, l6: 0.329066\n",
            "\n",
            "[epoch:   8/100000, batch:   348/  840, ite: 519] train loss: 2.538021, tar: 0.341334 \n",
            "l0: 0.288398, l1: 0.312461, l2: 0.308525, l3: 0.320347, l4: 0.310236, l5: 0.312226, l6: 0.308137\n",
            "\n",
            "[epoch:   8/100000, batch:   360/  840, ite: 520] train loss: 2.537294, tar: 0.341232 \n",
            "l0: 0.288411, l1: 0.269414, l2: 0.292522, l3: 0.298065, l4: 0.311565, l5: 0.337610, l6: 0.378507\n",
            "\n",
            "[epoch:   8/100000, batch:   372/  840, ite: 521] train loss: 2.536601, tar: 0.341130 \n",
            "l0: 0.262098, l1: 0.236489, l2: 0.253504, l3: 0.273833, l4: 0.290109, l5: 0.322317, l6: 0.359494\n",
            "\n",
            "[epoch:   8/100000, batch:   384/  840, ite: 522] train loss: 2.535569, tar: 0.340979 \n",
            "l0: 0.292378, l1: 0.301462, l2: 0.298610, l3: 0.309665, l4: 0.308646, l5: 0.329194, l6: 0.377015\n",
            "\n",
            "[epoch:   8/100000, batch:   396/  840, ite: 523] train loss: 2.534960, tar: 0.340886 \n",
            "l0: 0.222953, l1: 0.217007, l2: 0.226362, l3: 0.238200, l4: 0.261554, l5: 0.259908, l6: 0.295943\n",
            "\n",
            "[epoch:   8/100000, batch:   408/  840, ite: 524] train loss: 2.533408, tar: 0.340661 \n",
            "l0: 0.361128, l1: 0.324912, l2: 0.370247, l3: 0.405612, l4: 0.426460, l5: 0.382159, l6: 0.401495\n",
            "\n",
            "[epoch:   8/100000, batch:   420/  840, ite: 525] train loss: 2.533672, tar: 0.340700 \n",
            "l0: 0.237732, l1: 0.231441, l2: 0.249949, l3: 0.291794, l4: 0.293447, l5: 0.276286, l6: 0.282966\n",
            "\n",
            "[epoch:   8/100000, batch:   432/  840, ite: 526] train loss: 2.532398, tar: 0.340504 \n",
            "l0: 0.408255, l1: 0.381451, l2: 0.422273, l3: 0.457720, l4: 0.452593, l5: 0.479625, l6: 0.543680\n",
            "\n",
            "[epoch:   8/100000, batch:   444/  840, ite: 527] train loss: 2.533562, tar: 0.340633 \n",
            "l0: 0.327915, l1: 0.326744, l2: 0.315648, l3: 0.328617, l4: 0.324039, l5: 0.375819, l6: 0.468284\n",
            "\n",
            "[epoch:   8/100000, batch:   456/  840, ite: 528] train loss: 2.533436, tar: 0.340609 \n",
            "l0: 0.240822, l1: 0.226808, l2: 0.244415, l3: 0.275797, l4: 0.276962, l5: 0.276342, l6: 0.329210\n",
            "\n",
            "[epoch:   8/100000, batch:   468/  840, ite: 529] train loss: 2.532182, tar: 0.340420 \n",
            "l0: 0.473087, l1: 0.450466, l2: 0.455287, l3: 0.453858, l4: 0.497031, l5: 0.504126, l6: 0.555033\n",
            "\n",
            "[epoch:   8/100000, batch:   480/  840, ite: 530] train loss: 2.533799, tar: 0.340670 \n",
            "l0: 0.342218, l1: 0.369960, l2: 0.324971, l3: 0.365416, l4: 0.409000, l5: 0.371645, l6: 0.407058\n",
            "\n",
            "[epoch:   8/100000, batch:   492/  840, ite: 531] train loss: 2.533905, tar: 0.340673 \n",
            "l0: 0.362031, l1: 0.356580, l2: 0.350355, l3: 0.361947, l4: 0.348531, l5: 0.459212, l6: 0.537155\n",
            "\n",
            "[epoch:   8/100000, batch:   504/  840, ite: 532] train loss: 2.534360, tar: 0.340713 \n",
            "l0: 0.318965, l1: 0.317203, l2: 0.311414, l3: 0.316007, l4: 0.342537, l5: 0.521028, l6: 0.491078\n",
            "\n",
            "[epoch:   8/100000, batch:   516/  840, ite: 533] train loss: 2.534517, tar: 0.340673 \n",
            "l0: 0.230905, l1: 0.221022, l2: 0.228393, l3: 0.252448, l4: 0.247229, l5: 0.305481, l6: 0.390838\n",
            "\n",
            "[epoch:   8/100000, batch:   528/  840, ite: 534] train loss: 2.533284, tar: 0.340467 \n",
            "l0: 0.195405, l1: 0.198456, l2: 0.195301, l3: 0.205127, l4: 0.214006, l5: 0.232003, l6: 0.276268\n",
            "\n",
            "[epoch:   8/100000, batch:   540/  840, ite: 535] train loss: 2.531384, tar: 0.340196 \n",
            "l0: 0.320057, l1: 0.327319, l2: 0.311577, l3: 0.370720, l4: 0.360624, l5: 0.394849, l6: 0.422939\n",
            "\n",
            "[epoch:   8/100000, batch:   552/  840, ite: 536] train loss: 2.531341, tar: 0.340158 \n",
            "l0: 0.301454, l1: 0.320356, l2: 0.322037, l3: 0.357753, l4: 0.349649, l5: 0.313004, l6: 0.317959\n",
            "\n",
            "[epoch:   8/100000, batch:   564/  840, ite: 537] train loss: 2.530877, tar: 0.340086 \n",
            "l0: 0.481717, l1: 0.449818, l2: 0.469051, l3: 0.483431, l4: 0.501775, l5: 0.598252, l6: 0.628767\n",
            "\n",
            "[epoch:   8/100000, batch:   576/  840, ite: 538] train loss: 2.532888, tar: 0.340350 \n",
            "l0: 0.246772, l1: 0.237084, l2: 0.248950, l3: 0.271697, l4: 0.273835, l5: 0.317195, l6: 0.378865\n",
            "\n",
            "[epoch:   8/100000, batch:   588/  840, ite: 539] train loss: 2.531852, tar: 0.340176 \n",
            "l0: 0.275486, l1: 0.275476, l2: 0.266601, l3: 0.264948, l4: 0.273995, l5: 0.343754, l6: 0.399260\n",
            "\n",
            "[epoch:   8/100000, batch:   600/  840, ite: 540] train loss: 2.531051, tar: 0.340056 \n",
            "l0: 0.282690, l1: 0.266097, l2: 0.284779, l3: 0.310373, l4: 0.319285, l5: 0.359569, l6: 0.362270\n",
            "\n",
            "[epoch:   8/100000, batch:   612/  840, ite: 541] train loss: 2.530411, tar: 0.339950 \n",
            "l0: 0.186736, l1: 0.188076, l2: 0.196696, l3: 0.209487, l4: 0.213340, l5: 0.219889, l6: 0.222947\n",
            "\n",
            "[epoch:   8/100000, batch:   624/  840, ite: 542] train loss: 2.528394, tar: 0.339667 \n",
            "l0: 0.260690, l1: 0.254025, l2: 0.268671, l3: 0.282997, l4: 0.291936, l5: 0.289418, l6: 0.309759\n",
            "\n",
            "[epoch:   8/100000, batch:   636/  840, ite: 543] train loss: 2.527343, tar: 0.339522 \n",
            "l0: 0.407789, l1: 0.359123, l2: 0.403159, l3: 0.421599, l4: 0.441613, l5: 0.487730, l6: 0.523189\n",
            "\n",
            "[epoch:   8/100000, batch:   648/  840, ite: 544] train loss: 2.528293, tar: 0.339647 \n",
            "l0: 0.324128, l1: 0.341931, l2: 0.343979, l3: 0.374728, l4: 0.360968, l5: 0.348783, l6: 0.359445\n",
            "\n",
            "[epoch:   8/100000, batch:   660/  840, ite: 545] train loss: 2.528157, tar: 0.339619 \n",
            "l0: 0.280693, l1: 0.262206, l2: 0.262848, l3: 0.319363, l4: 0.325089, l5: 0.368468, l6: 0.418415\n",
            "\n",
            "[epoch:   8/100000, batch:   672/  840, ite: 546] train loss: 2.527623, tar: 0.339511 \n",
            "l0: 0.184704, l1: 0.196222, l2: 0.190580, l3: 0.212409, l4: 0.208448, l5: 0.226518, l6: 0.249437\n",
            "\n",
            "[epoch:   8/100000, batch:   684/  840, ite: 547] train loss: 2.525687, tar: 0.339228 \n",
            "l0: 0.291228, l1: 0.333755, l2: 0.310918, l3: 0.280912, l4: 0.280220, l5: 0.301768, l6: 0.331276\n",
            "\n",
            "[epoch:   8/100000, batch:   696/  840, ite: 548] train loss: 2.524965, tar: 0.339140 \n",
            "l0: 0.486210, l1: 0.495447, l2: 0.499822, l3: 0.462886, l4: 0.477930, l5: 0.510315, l6: 0.539167\n",
            "\n",
            "[epoch:   8/100000, batch:   708/  840, ite: 549] train loss: 2.526690, tar: 0.339408 \n",
            "l0: 0.315985, l1: 0.315635, l2: 0.329142, l3: 0.349630, l4: 0.353951, l5: 0.362907, l6: 0.382190\n",
            "\n",
            "[epoch:   8/100000, batch:   720/  840, ite: 550] train loss: 2.526476, tar: 0.339366 \n",
            "l0: 0.295330, l1: 0.305607, l2: 0.288696, l3: 0.313168, l4: 0.310758, l5: 0.343189, l6: 0.378592\n",
            "\n",
            "[epoch:   8/100000, batch:   732/  840, ite: 551] train loss: 2.525948, tar: 0.339286 \n",
            "l0: 0.317996, l1: 0.330686, l2: 0.339530, l3: 0.336474, l4: 0.346150, l5: 0.351355, l6: 0.360009\n",
            "\n",
            "[epoch:   8/100000, batch:   744/  840, ite: 552] train loss: 2.525688, tar: 0.339247 \n",
            "l0: 0.224862, l1: 0.216646, l2: 0.222996, l3: 0.247360, l4: 0.248107, l5: 0.284344, l6: 0.306153\n",
            "\n",
            "[epoch:   8/100000, batch:   756/  840, ite: 553] train loss: 2.524286, tar: 0.339040 \n",
            "l0: 0.282756, l1: 0.290255, l2: 0.295318, l3: 0.313552, l4: 0.301935, l5: 0.299220, l6: 0.332658\n",
            "\n",
            "[epoch:   8/100000, batch:   768/  840, ite: 554] train loss: 2.523548, tar: 0.338939 \n",
            "l0: 0.260740, l1: 0.281464, l2: 0.275133, l3: 0.282191, l4: 0.276026, l5: 0.303873, l6: 0.327413\n",
            "\n",
            "[epoch:   8/100000, batch:   780/  840, ite: 555] train loss: 2.522617, tar: 0.338798 \n",
            "l0: 0.308321, l1: 0.271808, l2: 0.294201, l3: 0.368613, l4: 0.359574, l5: 0.379830, l6: 0.412985\n",
            "\n",
            "[epoch:   8/100000, batch:   792/  840, ite: 556] train loss: 2.522388, tar: 0.338743 \n",
            "l0: 0.359152, l1: 0.364950, l2: 0.392982, l3: 0.384812, l4: 0.373956, l5: 0.382890, l6: 0.386046\n",
            "\n",
            "[epoch:   8/100000, batch:   804/  840, ite: 557] train loss: 2.522608, tar: 0.338780 \n",
            "l0: 0.264619, l1: 0.263008, l2: 0.281785, l3: 0.294043, l4: 0.295269, l5: 0.288703, l6: 0.318065\n",
            "\n",
            "[epoch:   8/100000, batch:   816/  840, ite: 558] train loss: 2.521682, tar: 0.338647 \n",
            "l0: 0.323407, l1: 0.342586, l2: 0.334323, l3: 0.341774, l4: 0.339462, l5: 0.333617, l6: 0.352431\n",
            "\n",
            "[epoch:   8/100000, batch:   828/  840, ite: 559] train loss: 2.521406, tar: 0.338620 \n",
            "l0: 0.108319, l1: 0.113002, l2: 0.112519, l3: 0.137046, l4: 0.133041, l5: 0.147728, l6: 0.166836\n",
            "\n",
            "[epoch:   8/100000, batch:   840/  840, ite: 560] train loss: 2.518543, tar: 0.338208 \n",
            "l0: 0.264087, l1: 0.238992, l2: 0.258268, l3: 0.303317, l4: 0.326363, l5: 0.350930, l6: 0.350485\n",
            "\n",
            "[epoch:   9/100000, batch:    12/  840, ite: 561] train loss: 2.517784, tar: 0.338076 \n",
            "l0: 0.342232, l1: 0.349709, l2: 0.367760, l3: 0.375634, l4: 0.367042, l5: 0.352848, l6: 0.374067\n",
            "\n",
            "[epoch:   9/100000, batch:    24/  840, ite: 562] train loss: 2.517804, tar: 0.338084 \n",
            "l0: 0.306127, l1: 0.301621, l2: 0.308688, l3: 0.315611, l4: 0.332118, l5: 0.333883, l6: 0.368783\n",
            "\n",
            "[epoch:   9/100000, batch:    36/  840, ite: 563] train loss: 2.517359, tar: 0.338027 \n",
            "l0: 0.287726, l1: 0.329770, l2: 0.303874, l3: 0.307291, l4: 0.299187, l5: 0.317729, l6: 0.328860\n",
            "\n",
            "[epoch:   9/100000, batch:    48/  840, ite: 564] train loss: 2.516750, tar: 0.337938 \n",
            "l0: 0.215069, l1: 0.231511, l2: 0.220026, l3: 0.256055, l4: 0.248684, l5: 0.266951, l6: 0.281141\n",
            "\n",
            "[epoch:   9/100000, batch:    60/  840, ite: 565] train loss: 2.515339, tar: 0.337720 \n",
            "l0: 0.321042, l1: 0.351194, l2: 0.336723, l3: 0.317446, l4: 0.322983, l5: 0.326652, l6: 0.343764\n",
            "\n",
            "[epoch:   9/100000, batch:    72/  840, ite: 566] train loss: 2.514994, tar: 0.337691 \n",
            "l0: 0.258979, l1: 0.269784, l2: 0.263161, l3: 0.271823, l4: 0.280111, l5: 0.288894, l6: 0.296173\n",
            "\n",
            "[epoch:   9/100000, batch:    84/  840, ite: 567] train loss: 2.513960, tar: 0.337552 \n",
            "l0: 0.205738, l1: 0.202937, l2: 0.207091, l3: 0.206143, l4: 0.211810, l5: 0.248357, l6: 0.269658\n",
            "\n",
            "[epoch:   9/100000, batch:    96/  840, ite: 568] train loss: 2.512266, tar: 0.337320 \n",
            "l0: 0.227372, l1: 0.212934, l2: 0.228354, l3: 0.251269, l4: 0.264814, l5: 0.265181, l6: 0.293406\n",
            "\n",
            "[epoch:   9/100000, batch:   108/  840, ite: 569] train loss: 2.510915, tar: 0.337127 \n",
            "l0: 0.205768, l1: 0.226331, l2: 0.214056, l3: 0.213244, l4: 0.218359, l5: 0.211109, l6: 0.233960\n",
            "\n",
            "[epoch:   9/100000, batch:   120/  840, ite: 570] train loss: 2.509181, tar: 0.336896 \n",
            "l0: 0.300042, l1: 0.292102, l2: 0.311624, l3: 0.353691, l4: 0.341616, l5: 0.310543, l6: 0.330277\n",
            "\n",
            "[epoch:   9/100000, batch:   132/  840, ite: 571] train loss: 2.508710, tar: 0.336832 \n",
            "l0: 0.234133, l1: 0.217699, l2: 0.233113, l3: 0.267717, l4: 0.269843, l5: 0.284308, l6: 0.316319\n",
            "\n",
            "[epoch:   9/100000, batch:   144/  840, ite: 572] train loss: 2.507511, tar: 0.336652 \n",
            "l0: 0.213043, l1: 0.225048, l2: 0.221690, l3: 0.243581, l4: 0.235899, l5: 0.253743, l6: 0.272513\n",
            "\n",
            "[epoch:   9/100000, batch:   156/  840, ite: 573] train loss: 2.506042, tar: 0.336436 \n",
            "l0: 0.248209, l1: 0.231070, l2: 0.252915, l3: 0.272191, l4: 0.277639, l5: 0.300163, l6: 0.307631\n",
            "\n",
            "[epoch:   9/100000, batch:   168/  840, ite: 574] train loss: 2.504968, tar: 0.336283 \n",
            "l0: 0.238402, l1: 0.258054, l2: 0.261153, l3: 0.266180, l4: 0.247320, l5: 0.256136, l6: 0.270789\n",
            "\n",
            "[epoch:   9/100000, batch:   180/  840, ite: 575] train loss: 2.503739, tar: 0.336112 \n",
            "l0: 0.263155, l1: 0.246353, l2: 0.259613, l3: 0.293572, l4: 0.304612, l5: 0.302484, l6: 0.314624\n",
            "\n",
            "[epoch:   9/100000, batch:   192/  840, ite: 576] train loss: 2.502837, tar: 0.335986 \n",
            "l0: 0.287910, l1: 0.299528, l2: 0.287273, l3: 0.310294, l4: 0.303825, l5: 0.307686, l6: 0.330568\n",
            "\n",
            "[epoch:   9/100000, batch:   204/  840, ite: 577] train loss: 2.502186, tar: 0.335902 \n",
            "l0: 0.311454, l1: 0.312444, l2: 0.329798, l3: 0.351545, l4: 0.339241, l5: 0.334571, l6: 0.345710\n",
            "\n",
            "[epoch:   9/100000, batch:   216/  840, ite: 578] train loss: 2.501879, tar: 0.335860 \n",
            "l0: 0.207605, l1: 0.188111, l2: 0.212349, l3: 0.248982, l4: 0.244318, l5: 0.244851, l6: 0.273110\n",
            "\n",
            "[epoch:   9/100000, batch:   228/  840, ite: 579] train loss: 2.500355, tar: 0.335639 \n",
            "l0: 0.206569, l1: 0.225511, l2: 0.210983, l3: 0.234068, l4: 0.223924, l5: 0.218707, l6: 0.252481\n",
            "\n",
            "[epoch:   9/100000, batch:   240/  840, ite: 580] train loss: 2.498755, tar: 0.335416 \n",
            "l0: 0.293116, l1: 0.278261, l2: 0.294050, l3: 0.316551, l4: 0.325162, l5: 0.334209, l6: 0.341416\n",
            "\n",
            "[epoch:   9/100000, batch:   252/  840, ite: 581] train loss: 2.498210, tar: 0.335343 \n",
            "l0: 0.239209, l1: 0.211717, l2: 0.235435, l3: 0.254685, l4: 0.277371, l5: 0.280157, l6: 0.305897\n",
            "\n",
            "[epoch:   9/100000, batch:   264/  840, ite: 582] train loss: 2.497019, tar: 0.335178 \n",
            "l0: 0.189433, l1: 0.189066, l2: 0.193173, l3: 0.198292, l4: 0.207892, l5: 0.203834, l6: 0.224079\n",
            "\n",
            "[epoch:   9/100000, batch:   276/  840, ite: 583] train loss: 2.495147, tar: 0.334928 \n",
            "l0: 0.211206, l1: 0.195428, l2: 0.213864, l3: 0.233853, l4: 0.234160, l5: 0.263272, l6: 0.300748\n",
            "\n",
            "[epoch:   9/100000, batch:   288/  840, ite: 584] train loss: 2.493704, tar: 0.334716 \n",
            "l0: 0.214978, l1: 0.226440, l2: 0.215395, l3: 0.239974, l4: 0.241988, l5: 0.243713, l6: 0.258821\n",
            "\n",
            "[epoch:   9/100000, batch:   300/  840, ite: 585] train loss: 2.492247, tar: 0.334512 \n",
            "l0: 0.334474, l1: 0.332917, l2: 0.337519, l3: 0.346872, l4: 0.354768, l5: 0.368785, l6: 0.381851\n",
            "\n",
            "[epoch:   9/100000, batch:   312/  840, ite: 586] train loss: 2.492187, tar: 0.334511 \n",
            "l0: 0.409031, l1: 0.418112, l2: 0.412149, l3: 0.466102, l4: 0.433017, l5: 0.432101, l6: 0.449823\n",
            "\n",
            "[epoch:   9/100000, batch:   324/  840, ite: 587] train loss: 2.493087, tar: 0.334638 \n",
            "l0: 0.402876, l1: 0.405096, l2: 0.394338, l3: 0.450612, l4: 0.458764, l5: 0.450874, l6: 0.470241\n",
            "\n",
            "[epoch:   9/100000, batch:   336/  840, ite: 588] train loss: 2.494005, tar: 0.334754 \n",
            "l0: 0.342926, l1: 0.336801, l2: 0.345922, l3: 0.373012, l4: 0.366617, l5: 0.374573, l6: 0.404397\n",
            "\n",
            "[epoch:   9/100000, batch:   348/  840, ite: 589] train loss: 2.494090, tar: 0.334768 \n",
            "l0: 0.231236, l1: 0.239647, l2: 0.233717, l3: 0.258783, l4: 0.259434, l5: 0.265344, l6: 0.293033\n",
            "\n",
            "[epoch:   9/100000, batch:   360/  840, ite: 590] train loss: 2.492882, tar: 0.334593 \n",
            "l0: 0.229579, l1: 0.212319, l2: 0.240053, l3: 0.253637, l4: 0.280237, l5: 0.269776, l6: 0.295990\n",
            "\n",
            "[epoch:   9/100000, batch:   372/  840, ite: 591] train loss: 2.491678, tar: 0.334415 \n",
            "l0: 0.163785, l1: 0.143597, l2: 0.170628, l3: 0.200787, l4: 0.218650, l5: 0.208295, l6: 0.228369\n",
            "\n",
            "[epoch:   9/100000, batch:   384/  840, ite: 592] train loss: 2.489723, tar: 0.334127 \n",
            "l0: 0.330640, l1: 0.330045, l2: 0.327734, l3: 0.342524, l4: 0.355210, l5: 0.373274, l6: 0.409018\n",
            "\n",
            "[epoch:   9/100000, batch:   396/  840, ite: 593] train loss: 2.489687, tar: 0.334121 \n",
            "l0: 0.407542, l1: 0.442267, l2: 0.422161, l3: 0.402692, l4: 0.405878, l5: 0.440035, l6: 0.458868\n",
            "\n",
            "[epoch:   9/100000, batch:   408/  840, ite: 594] train loss: 2.490512, tar: 0.334245 \n",
            "l0: 0.266203, l1: 0.268576, l2: 0.287929, l3: 0.262873, l4: 0.266346, l5: 0.274871, l6: 0.298103\n",
            "\n",
            "[epoch:   9/100000, batch:   420/  840, ite: 595] train loss: 2.489561, tar: 0.334130 \n",
            "l0: 0.467387, l1: 0.483745, l2: 0.480870, l3: 0.502193, l4: 0.493989, l5: 0.496376, l6: 0.505106\n",
            "\n",
            "[epoch:   9/100000, batch:   432/  840, ite: 596] train loss: 2.491138, tar: 0.334354 \n",
            "l0: 0.242955, l1: 0.242932, l2: 0.266443, l3: 0.266698, l4: 0.256845, l5: 0.246298, l6: 0.266028\n",
            "\n",
            "[epoch:   9/100000, batch:   444/  840, ite: 597] train loss: 2.489961, tar: 0.334201 \n",
            "l0: 0.397505, l1: 0.429520, l2: 0.420870, l3: 0.410427, l4: 0.400514, l5: 0.419038, l6: 0.443860\n",
            "\n",
            "[epoch:   9/100000, batch:   456/  840, ite: 598] train loss: 2.490683, tar: 0.334307 \n",
            "l0: 0.242383, l1: 0.243146, l2: 0.252085, l3: 0.239435, l4: 0.249481, l5: 0.258400, l6: 0.298251\n",
            "\n",
            "[epoch:   9/100000, batch:   468/  840, ite: 599] train loss: 2.489502, tar: 0.334153 \n",
            "l0: 0.336270, l1: 0.356823, l2: 0.375679, l3: 0.362167, l4: 0.331176, l5: 0.328586, l6: 0.370933\n",
            "\n",
            "[epoch:   9/100000, batch:   480/  840, ite: 600] train loss: 2.489456, tar: 0.334157 \n",
            "l0: 0.249721, l1: 0.250113, l2: 0.251848, l3: 0.253081, l4: 0.274779, l5: 0.275944, l6: 0.312656\n",
            "\n",
            "[epoch:   9/100000, batch:   492/  840, ite: 601] train loss: 2.488422, tar: 0.334016 \n",
            "l0: 0.230712, l1: 0.243253, l2: 0.242654, l3: 0.243952, l4: 0.245115, l5: 0.240689, l6: 0.271401\n",
            "\n",
            "[epoch:   9/100000, batch:   504/  840, ite: 602] train loss: 2.487142, tar: 0.333845 \n",
            "l0: 0.250748, l1: 0.253797, l2: 0.255345, l3: 0.268720, l4: 0.271701, l5: 0.280182, l6: 0.315133\n",
            "\n",
            "[epoch:   9/100000, batch:   516/  840, ite: 603] train loss: 2.486161, tar: 0.333707 \n",
            "l0: 0.293828, l1: 0.289207, l2: 0.289578, l3: 0.314541, l4: 0.329886, l5: 0.337813, l6: 0.350704\n",
            "\n",
            "[epoch:   9/100000, batch:   528/  840, ite: 604] train loss: 2.485696, tar: 0.333641 \n",
            "l0: 0.159158, l1: 0.179873, l2: 0.158657, l3: 0.173842, l4: 0.167186, l5: 0.194694, l6: 0.242146\n",
            "\n",
            "[epoch:   9/100000, batch:   540/  840, ite: 605] train loss: 2.483696, tar: 0.333352 \n",
            "l0: 0.355037, l1: 0.316756, l2: 0.350078, l3: 0.385133, l4: 0.387293, l5: 0.415524, l6: 0.444273\n",
            "\n",
            "[epoch:   9/100000, batch:   552/  840, ite: 606] train loss: 2.483977, tar: 0.333388 \n",
            "l0: 0.312417, l1: 0.305245, l2: 0.328143, l3: 0.329314, l4: 0.318612, l5: 0.325637, l6: 0.343061\n",
            "\n",
            "[epoch:   9/100000, batch:   564/  840, ite: 607] train loss: 2.483612, tar: 0.333354 \n",
            "l0: 0.318604, l1: 0.327912, l2: 0.338506, l3: 0.325734, l4: 0.323567, l5: 0.327875, l6: 0.344720\n",
            "\n",
            "[epoch:   9/100000, batch:   576/  840, ite: 608] train loss: 2.483321, tar: 0.333329 \n",
            "l0: 0.257505, l1: 0.250403, l2: 0.275876, l3: 0.288917, l4: 0.277214, l5: 0.281791, l6: 0.314759\n",
            "\n",
            "[epoch:   9/100000, batch:   588/  840, ite: 609] train loss: 2.482440, tar: 0.333205 \n",
            "l0: 0.276239, l1: 0.273032, l2: 0.281968, l3: 0.306777, l4: 0.302876, l5: 0.317748, l6: 0.355778\n",
            "\n",
            "[epoch:   9/100000, batch:   600/  840, ite: 610] train loss: 2.481836, tar: 0.333112 \n",
            "l0: 0.204731, l1: 0.202506, l2: 0.201830, l3: 0.232059, l4: 0.240172, l5: 0.245788, l6: 0.270882\n",
            "\n",
            "[epoch:   9/100000, batch:   612/  840, ite: 611] train loss: 2.480390, tar: 0.332901 \n",
            "l0: 0.351730, l1: 0.335810, l2: 0.339547, l3: 0.380318, l4: 0.410071, l5: 0.431449, l6: 0.421417\n",
            "\n",
            "[epoch:   9/100000, batch:   624/  840, ite: 612] train loss: 2.480700, tar: 0.332932 \n",
            "l0: 0.379627, l1: 0.366989, l2: 0.366546, l3: 0.422998, l4: 0.448630, l5: 0.460355, l6: 0.467497\n",
            "\n",
            "[epoch:   9/100000, batch:   636/  840, ite: 613] train loss: 2.481405, tar: 0.333008 \n",
            "l0: 0.198322, l1: 0.196904, l2: 0.192906, l3: 0.199165, l4: 0.213058, l5: 0.233322, l6: 0.260534\n",
            "\n",
            "[epoch:   9/100000, batch:   648/  840, ite: 614] train loss: 2.479797, tar: 0.332789 \n",
            "l0: 0.233604, l1: 0.241234, l2: 0.242701, l3: 0.251710, l4: 0.251298, l5: 0.269402, l6: 0.295955\n",
            "\n",
            "[epoch:   9/100000, batch:   660/  840, ite: 615] train loss: 2.478669, tar: 0.332628 \n",
            "l0: 0.186416, l1: 0.195712, l2: 0.190599, l3: 0.202484, l4: 0.202768, l5: 0.225244, l6: 0.252647\n",
            "\n",
            "[epoch:   9/100000, batch:   672/  840, ite: 616] train loss: 2.477008, tar: 0.332390 \n",
            "l0: 0.292293, l1: 0.295261, l2: 0.321227, l3: 0.324497, l4: 0.316925, l5: 0.307034, l6: 0.315066\n",
            "\n",
            "[epoch:   9/100000, batch:   684/  840, ite: 617] train loss: 2.476514, tar: 0.332325 \n",
            "l0: 0.301918, l1: 0.309660, l2: 0.304667, l3: 0.331677, l4: 0.336007, l5: 0.342172, l6: 0.353934\n",
            "\n",
            "[epoch:   9/100000, batch:   696/  840, ite: 618] train loss: 2.476197, tar: 0.332276 \n",
            "l0: 0.175686, l1: 0.170655, l2: 0.181076, l3: 0.191488, l4: 0.191593, l5: 0.216053, l6: 0.246063\n",
            "\n",
            "[epoch:   9/100000, batch:   708/  840, ite: 619] train loss: 2.474414, tar: 0.332023 \n",
            "l0: 0.334931, l1: 0.330534, l2: 0.345901, l3: 0.347542, l4: 0.359005, l5: 0.349518, l6: 0.363670\n",
            "\n",
            "[epoch:   9/100000, batch:   720/  840, ite: 620] train loss: 2.474344, tar: 0.332028 \n",
            "l0: 0.235135, l1: 0.229282, l2: 0.243248, l3: 0.261898, l4: 0.259240, l5: 0.290557, l6: 0.302227\n",
            "\n",
            "[epoch:   9/100000, batch:   732/  840, ite: 621] train loss: 2.473293, tar: 0.331872 \n",
            "l0: 0.304033, l1: 0.311492, l2: 0.293441, l3: 0.297117, l4: 0.325406, l5: 0.320928, l6: 0.358831\n",
            "\n",
            "[epoch:   9/100000, batch:   744/  840, ite: 622] train loss: 2.472871, tar: 0.331827 \n",
            "l0: 0.314910, l1: 0.336666, l2: 0.323216, l3: 0.330354, l4: 0.332028, l5: 0.360831, l6: 0.371190\n",
            "\n",
            "[epoch:   9/100000, batch:   756/  840, ite: 623] train loss: 2.472705, tar: 0.331800 \n",
            "l0: 0.338368, l1: 0.345241, l2: 0.333695, l3: 0.387579, l4: 0.377128, l5: 0.404314, l6: 0.406352\n",
            "\n",
            "[epoch:   9/100000, batch:   768/  840, ite: 624] train loss: 2.472897, tar: 0.331811 \n",
            "l0: 0.200061, l1: 0.214488, l2: 0.208890, l3: 0.212651, l4: 0.215295, l5: 0.222307, l6: 0.248549\n",
            "\n",
            "[epoch:   9/100000, batch:   780/  840, ite: 625] train loss: 2.471376, tar: 0.331600 \n",
            "l0: 0.252569, l1: 0.269025, l2: 0.269073, l3: 0.285086, l4: 0.267786, l5: 0.279065, l6: 0.301072\n",
            "\n",
            "[epoch:   9/100000, batch:   792/  840, ite: 626] train loss: 2.470501, tar: 0.331473 \n",
            "l0: 0.433555, l1: 0.433556, l2: 0.465475, l3: 0.477729, l4: 0.473333, l5: 0.469417, l6: 0.456517\n",
            "\n",
            "[epoch:   9/100000, batch:   804/  840, ite: 627] train loss: 2.471680, tar: 0.331636 \n",
            "l0: 0.386569, l1: 0.415543, l2: 0.412627, l3: 0.370934, l4: 0.379293, l5: 0.376847, l6: 0.392475\n",
            "\n",
            "[epoch:   9/100000, batch:   816/  840, ite: 628] train loss: 2.472098, tar: 0.331724 \n",
            "l0: 0.292418, l1: 0.281564, l2: 0.268257, l3: 0.339678, l4: 0.332569, l5: 0.385074, l6: 0.429770\n",
            "\n",
            "[epoch:   9/100000, batch:   828/  840, ite: 629] train loss: 2.471871, tar: 0.331661 \n",
            "l0: 0.209971, l1: 0.211325, l2: 0.215985, l3: 0.231181, l4: 0.236694, l5: 0.253235, l6: 0.293735\n",
            "\n",
            "[epoch:   9/100000, batch:   840/  840, ite: 630] train loss: 2.470570, tar: 0.331468 \n",
            "l0: 0.289896, l1: 0.274920, l2: 0.299859, l3: 0.321508, l4: 0.317694, l5: 0.352317, l6: 0.369411\n",
            "\n",
            "[epoch:  10/100000, batch:    12/  840, ite: 631] train loss: 2.470182, tar: 0.331402 \n",
            "l0: 0.305322, l1: 0.307474, l2: 0.331804, l3: 0.314265, l4: 0.309801, l5: 0.304289, l6: 0.328570\n",
            "\n",
            "[epoch:  10/100000, batch:    24/  840, ite: 632] train loss: 2.469757, tar: 0.331361 \n",
            "l0: 0.298979, l1: 0.301803, l2: 0.311339, l3: 0.310384, l4: 0.297894, l5: 0.294627, l6: 0.325309\n",
            "\n",
            "[epoch:  10/100000, batch:    36/  840, ite: 633] train loss: 2.469236, tar: 0.331310 \n",
            "l0: 0.299151, l1: 0.294276, l2: 0.305901, l3: 0.339916, l4: 0.336959, l5: 0.322986, l6: 0.331277\n",
            "\n",
            "[epoch:  10/100000, batch:    48/  840, ite: 634] train loss: 2.468860, tar: 0.331259 \n",
            "l0: 0.233765, l1: 0.243745, l2: 0.242914, l3: 0.253880, l4: 0.259887, l5: 0.278877, l6: 0.303342\n",
            "\n",
            "[epoch:  10/100000, batch:    60/  840, ite: 635] train loss: 2.467832, tar: 0.331106 \n",
            "l0: 0.420866, l1: 0.448315, l2: 0.411060, l3: 0.434467, l4: 0.447836, l5: 0.494398, l6: 0.506889\n",
            "\n",
            "[epoch:  10/100000, batch:    72/  840, ite: 636] train loss: 2.468926, tar: 0.331247 \n",
            "l0: 0.262249, l1: 0.261655, l2: 0.259772, l3: 0.281811, l4: 0.294109, l5: 0.303465, l6: 0.344408\n",
            "\n",
            "[epoch:  10/100000, batch:    84/  840, ite: 637] train loss: 2.468202, tar: 0.331138 \n",
            "l0: 0.228266, l1: 0.241795, l2: 0.216144, l3: 0.265881, l4: 0.263353, l5: 0.263948, l6: 0.297766\n",
            "\n",
            "[epoch:  10/100000, batch:    96/  840, ite: 638] train loss: 2.467119, tar: 0.330977 \n",
            "l0: 0.213555, l1: 0.222904, l2: 0.208104, l3: 0.236330, l4: 0.239426, l5: 0.261540, l6: 0.294928\n",
            "\n",
            "[epoch:  10/100000, batch:   108/  840, ite: 639] train loss: 2.465882, tar: 0.330793 \n",
            "l0: 0.361968, l1: 0.345077, l2: 0.350588, l3: 0.415374, l4: 0.411192, l5: 0.423096, l6: 0.445430\n",
            "\n",
            "[epoch:  10/100000, batch:   120/  840, ite: 640] train loss: 2.466330, tar: 0.330842 \n",
            "l0: 0.348506, l1: 0.339662, l2: 0.357790, l3: 0.369634, l4: 0.371968, l5: 0.387664, l6: 0.419219\n",
            "\n",
            "[epoch:  10/100000, batch:   132/  840, ite: 641] train loss: 2.466530, tar: 0.330870 \n",
            "l0: 0.211040, l1: 0.218076, l2: 0.216027, l3: 0.221988, l4: 0.226432, l5: 0.251149, l6: 0.277559\n",
            "\n",
            "[epoch:  10/100000, batch:   144/  840, ite: 642] train loss: 2.465215, tar: 0.330683 \n",
            "l0: 0.329959, l1: 0.314200, l2: 0.337998, l3: 0.338494, l4: 0.346422, l5: 0.346592, l6: 0.378108\n",
            "\n",
            "[epoch:  10/100000, batch:   156/  840, ite: 643] train loss: 2.465101, tar: 0.330682 \n",
            "l0: 0.358320, l1: 0.351924, l2: 0.377295, l3: 0.379401, l4: 0.379324, l5: 0.373885, l6: 0.391504\n",
            "\n",
            "[epoch:  10/100000, batch:   168/  840, ite: 644] train loss: 2.465328, tar: 0.330725 \n",
            "l0: 0.366665, l1: 0.375114, l2: 0.374091, l3: 0.414209, l4: 0.405762, l5: 0.375902, l6: 0.406825\n",
            "\n",
            "[epoch:  10/100000, batch:   180/  840, ite: 645] train loss: 2.465721, tar: 0.330781 \n",
            "l0: 0.315250, l1: 0.326251, l2: 0.311780, l3: 0.342187, l4: 0.345255, l5: 0.348959, l6: 0.380784\n",
            "\n",
            "[epoch:  10/100000, batch:   192/  840, ite: 646] train loss: 2.465574, tar: 0.330756 \n",
            "l0: 0.312413, l1: 0.333092, l2: 0.322097, l3: 0.340633, l4: 0.333490, l5: 0.340003, l6: 0.369699\n",
            "\n",
            "[epoch:  10/100000, batch:   204/  840, ite: 647] train loss: 2.465397, tar: 0.330728 \n",
            "l0: 0.216746, l1: 0.211666, l2: 0.228954, l3: 0.245792, l4: 0.241033, l5: 0.250930, l6: 0.295323\n",
            "\n",
            "[epoch:  10/100000, batch:   216/  840, ite: 648] train loss: 2.464201, tar: 0.330552 \n",
            "l0: 0.254522, l1: 0.245242, l2: 0.257396, l3: 0.315976, l4: 0.309009, l5: 0.296654, l6: 0.301380\n",
            "\n",
            "[epoch:  10/100000, batch:   228/  840, ite: 649] train loss: 2.463455, tar: 0.330435 \n",
            "l0: 0.232054, l1: 0.225371, l2: 0.241951, l3: 0.258807, l4: 0.255896, l5: 0.273390, l6: 0.285827\n",
            "\n",
            "[epoch:  10/100000, batch:   240/  840, ite: 650] train loss: 2.462394, tar: 0.330284 \n",
            "l0: 0.381387, l1: 0.389385, l2: 0.370437, l3: 0.390805, l4: 0.400042, l5: 0.440724, l6: 0.466625\n",
            "\n",
            "[epoch:  10/100000, batch:   252/  840, ite: 651] train loss: 2.462973, tar: 0.330362 \n",
            "l0: 0.322319, l1: 0.349011, l2: 0.358040, l3: 0.326265, l4: 0.328730, l5: 0.325832, l6: 0.352897\n",
            "\n",
            "[epoch:  10/100000, batch:   264/  840, ite: 652] train loss: 2.462819, tar: 0.330350 \n",
            "l0: 0.369368, l1: 0.411190, l2: 0.425485, l3: 0.376321, l4: 0.366980, l5: 0.349472, l6: 0.357096\n",
            "\n",
            "[epoch:  10/100000, batch:   276/  840, ite: 653] train loss: 2.463115, tar: 0.330410 \n",
            "l0: 0.201734, l1: 0.203912, l2: 0.218945, l3: 0.212342, l4: 0.211738, l5: 0.211438, l6: 0.246074\n",
            "\n",
            "[epoch:  10/100000, batch:   288/  840, ite: 654] train loss: 2.461652, tar: 0.330213 \n",
            "l0: 0.176062, l1: 0.179021, l2: 0.201576, l3: 0.206624, l4: 0.200953, l5: 0.195870, l6: 0.232728\n",
            "\n",
            "[epoch:  10/100000, batch:   300/  840, ite: 655] train loss: 2.460020, tar: 0.329978 \n",
            "l0: 0.260195, l1: 0.247030, l2: 0.261463, l3: 0.278701, l4: 0.274508, l5: 0.304028, l6: 0.377257\n",
            "\n",
            "[epoch:  10/100000, batch:   312/  840, ite: 656] train loss: 2.459324, tar: 0.329871 \n",
            "l0: 0.372518, l1: 0.352174, l2: 0.356752, l3: 0.389941, l4: 0.397024, l5: 0.455302, l6: 0.533344\n",
            "\n",
            "[epoch:  10/100000, batch:   324/  840, ite: 657] train loss: 2.459929, tar: 0.329936 \n",
            "l0: 0.267635, l1: 0.260289, l2: 0.270255, l3: 0.282943, l4: 0.289095, l5: 0.322359, l6: 0.329802\n",
            "\n",
            "[epoch:  10/100000, batch:   336/  840, ite: 658] train loss: 2.459264, tar: 0.329841 \n",
            "l0: 0.247430, l1: 0.252603, l2: 0.250034, l3: 0.264795, l4: 0.264754, l5: 0.291456, l6: 0.325446\n",
            "\n",
            "[epoch:  10/100000, batch:   348/  840, ite: 659] train loss: 2.458410, tar: 0.329716 \n",
            "l0: 0.248094, l1: 0.260250, l2: 0.273089, l3: 0.266582, l4: 0.257568, l5: 0.273694, l6: 0.301717\n",
            "\n",
            "[epoch:  10/100000, batch:   360/  840, ite: 660] train loss: 2.457535, tar: 0.329593 \n",
            "l0: 0.325234, l1: 0.309491, l2: 0.347531, l3: 0.357004, l4: 0.354023, l5: 0.389256, l6: 0.439919\n",
            "\n",
            "[epoch:  10/100000, batch:   372/  840, ite: 661] train loss: 2.457633, tar: 0.329586 \n",
            "l0: 0.284016, l1: 0.256434, l2: 0.266255, l3: 0.280580, l4: 0.309340, l5: 0.356815, l6: 0.456479\n",
            "\n",
            "[epoch:  10/100000, batch:   384/  840, ite: 662] train loss: 2.457259, tar: 0.329517 \n",
            "l0: 0.314822, l1: 0.295448, l2: 0.307467, l3: 0.322073, l4: 0.340109, l5: 0.367934, l6: 0.423056\n",
            "\n",
            "[epoch:  10/100000, batch:   396/  840, ite: 663] train loss: 2.457129, tar: 0.329495 \n",
            "l0: 0.406244, l1: 0.419902, l2: 0.389161, l3: 0.421047, l4: 0.459519, l5: 0.461932, l6: 0.496780\n",
            "\n",
            "[epoch:  10/100000, batch:   408/  840, ite: 664] train loss: 2.458029, tar: 0.329611 \n",
            "l0: 0.261177, l1: 0.270007, l2: 0.268757, l3: 0.291387, l4: 0.288148, l5: 0.302211, l6: 0.337271\n",
            "\n",
            "[epoch:  10/100000, batch:   420/  840, ite: 665] train loss: 2.457369, tar: 0.329508 \n",
            "l0: 0.249719, l1: 0.243325, l2: 0.265877, l3: 0.288257, l4: 0.291509, l5: 0.312427, l6: 0.333950\n",
            "\n",
            "[epoch:  10/100000, batch:   432/  840, ite: 666] train loss: 2.456660, tar: 0.329388 \n",
            "l0: 0.244563, l1: 0.245096, l2: 0.248506, l3: 0.261065, l4: 0.262155, l5: 0.281357, l6: 0.325172\n",
            "\n",
            "[epoch:  10/100000, batch:   444/  840, ite: 667] train loss: 2.455777, tar: 0.329261 \n",
            "l0: 0.223017, l1: 0.228833, l2: 0.243347, l3: 0.252572, l4: 0.249438, l5: 0.258921, l6: 0.281409\n",
            "\n",
            "[epoch:  10/100000, batch:   456/  840, ite: 668] train loss: 2.454702, tar: 0.329102 \n",
            "l0: 0.357832, l1: 0.321931, l2: 0.365561, l3: 0.384781, l4: 0.386040, l5: 0.399881, l6: 0.464870\n",
            "\n",
            "[epoch:  10/100000, batch:   468/  840, ite: 669] train loss: 2.455040, tar: 0.329145 \n",
            "l0: 0.243804, l1: 0.226438, l2: 0.255525, l3: 0.268129, l4: 0.277590, l5: 0.288216, l6: 0.318464\n",
            "\n",
            "[epoch:  10/100000, batch:   480/  840, ite: 670] train loss: 2.454179, tar: 0.329017 \n",
            "l0: 0.232146, l1: 0.241778, l2: 0.246949, l3: 0.244375, l4: 0.250818, l5: 0.262250, l6: 0.276127\n",
            "\n",
            "[epoch:  10/100000, batch:   492/  840, ite: 671] train loss: 2.453136, tar: 0.328873 \n",
            "l0: 0.293372, l1: 0.286368, l2: 0.298699, l3: 0.331050, l4: 0.346655, l5: 0.360431, l6: 0.372380\n",
            "\n",
            "[epoch:  10/100000, batch:   504/  840, ite: 672] train loss: 2.452892, tar: 0.328820 \n",
            "l0: 0.160137, l1: 0.152056, l2: 0.151202, l3: 0.178833, l4: 0.199051, l5: 0.201215, l6: 0.246412\n",
            "\n",
            "[epoch:  10/100000, batch:   516/  840, ite: 673] train loss: 2.451162, tar: 0.328570 \n",
            "l0: 0.210504, l1: 0.210254, l2: 0.204010, l3: 0.244352, l4: 0.252988, l5: 0.253336, l6: 0.288961\n",
            "\n",
            "[epoch:  10/100000, batch:   528/  840, ite: 674] train loss: 2.449995, tar: 0.328394 \n",
            "l0: 0.228144, l1: 0.238820, l2: 0.216017, l3: 0.271513, l4: 0.282170, l5: 0.277446, l6: 0.287538\n",
            "\n",
            "[epoch:  10/100000, batch:   540/  840, ite: 675] train loss: 2.449034, tar: 0.328246 \n",
            "l0: 0.165006, l1: 0.173368, l2: 0.162401, l3: 0.194345, l4: 0.204802, l5: 0.204190, l6: 0.254188\n",
            "\n",
            "[epoch:  10/100000, batch:   552/  840, ite: 676] train loss: 2.447421, tar: 0.328004 \n",
            "l0: 0.197691, l1: 0.199036, l2: 0.212152, l3: 0.214657, l4: 0.221208, l5: 0.227028, l6: 0.272912\n",
            "\n",
            "[epoch:  10/100000, batch:   564/  840, ite: 677] train loss: 2.446087, tar: 0.327812 \n",
            "l0: 0.288206, l1: 0.288676, l2: 0.302431, l3: 0.309892, l4: 0.308280, l5: 0.331123, l6: 0.343120\n",
            "\n",
            "[epoch:  10/100000, batch:   576/  840, ite: 678] train loss: 2.445683, tar: 0.327753 \n",
            "l0: 0.232387, l1: 0.227807, l2: 0.232144, l3: 0.270239, l4: 0.276888, l5: 0.288249, l6: 0.314683\n",
            "\n",
            "[epoch:  10/100000, batch:   588/  840, ite: 679] train loss: 2.444794, tar: 0.327613 \n",
            "l0: 0.248764, l1: 0.259606, l2: 0.255921, l3: 0.287015, l4: 0.270951, l5: 0.294852, l6: 0.316867\n",
            "\n",
            "[epoch:  10/100000, batch:   600/  840, ite: 680] train loss: 2.444043, tar: 0.327497 \n",
            "l0: 0.273731, l1: 0.276072, l2: 0.281552, l3: 0.303865, l4: 0.301080, l5: 0.305630, l6: 0.311978\n",
            "\n",
            "[epoch:  10/100000, batch:   612/  840, ite: 681] train loss: 2.443470, tar: 0.327418 \n",
            "l0: 0.314891, l1: 0.319584, l2: 0.300786, l3: 0.370001, l4: 0.374232, l5: 0.444128, l6: 0.441335\n",
            "\n",
            "[epoch:  10/100000, batch:   624/  840, ite: 682] train loss: 2.443648, tar: 0.327400 \n",
            "l0: 0.210198, l1: 0.195324, l2: 0.209143, l3: 0.219527, l4: 0.229986, l5: 0.239004, l6: 0.279981\n",
            "\n",
            "[epoch:  10/100000, batch:   636/  840, ite: 683] train loss: 2.442388, tar: 0.327228 \n",
            "l0: 0.136501, l1: 0.129886, l2: 0.139014, l3: 0.156797, l4: 0.155061, l5: 0.176618, l6: 0.206761\n",
            "\n",
            "[epoch:  10/100000, batch:   648/  840, ite: 684] train loss: 2.440427, tar: 0.326949 \n",
            "l0: 0.218407, l1: 0.222625, l2: 0.216803, l3: 0.240235, l4: 0.234909, l5: 0.251006, l6: 0.278193\n",
            "\n",
            "[epoch:  10/100000, batch:   660/  840, ite: 685] train loss: 2.439291, tar: 0.326791 \n",
            "l0: 0.289661, l1: 0.282211, l2: 0.307151, l3: 0.260632, l4: 0.278667, l5: 0.333171, l6: 0.360726\n",
            "\n",
            "[epoch:  10/100000, batch:   672/  840, ite: 686] train loss: 2.438814, tar: 0.326737 \n",
            "l0: 0.308099, l1: 0.297435, l2: 0.317365, l3: 0.323884, l4: 0.324162, l5: 0.358578, l6: 0.407060\n",
            "\n",
            "[epoch:  10/100000, batch:   684/  840, ite: 687] train loss: 2.438665, tar: 0.326710 \n",
            "l0: 0.230656, l1: 0.221805, l2: 0.237386, l3: 0.260341, l4: 0.269588, l5: 0.288300, l6: 0.312292\n",
            "\n",
            "[epoch:  10/100000, batch:   696/  840, ite: 688] train loss: 2.437766, tar: 0.326570 \n",
            "l0: 0.349248, l1: 0.340010, l2: 0.383252, l3: 0.347513, l4: 0.343306, l5: 0.332586, l6: 0.379607\n",
            "\n",
            "[epoch:  10/100000, batch:   708/  840, ite: 689] train loss: 2.437821, tar: 0.326603 \n",
            "l0: 0.213972, l1: 0.194173, l2: 0.237214, l3: 0.221518, l4: 0.228726, l5: 0.257942, l6: 0.281041\n",
            "\n",
            "[epoch:  10/100000, batch:   720/  840, ite: 690] train loss: 2.436657, tar: 0.326440 \n",
            "l0: 0.270390, l1: 0.269939, l2: 0.261011, l3: 0.276133, l4: 0.288741, l5: 0.341227, l6: 0.369214\n",
            "\n",
            "[epoch:  10/100000, batch:   732/  840, ite: 691] train loss: 2.436136, tar: 0.326359 \n",
            "l0: 0.407203, l1: 0.426734, l2: 0.421952, l3: 0.448313, l4: 0.431955, l5: 0.431399, l6: 0.463205\n",
            "\n",
            "[epoch:  10/100000, batch:   744/  840, ite: 692] train loss: 2.436995, tar: 0.326475 \n",
            "l0: 0.244232, l1: 0.295441, l2: 0.249198, l3: 0.275629, l4: 0.260572, l5: 0.256661, l6: 0.286323\n",
            "\n",
            "[epoch:  10/100000, batch:   756/  840, ite: 693] train loss: 2.436174, tar: 0.326357 \n",
            "l0: 0.125165, l1: 0.135109, l2: 0.135559, l3: 0.153545, l4: 0.147999, l5: 0.160880, l6: 0.182088\n",
            "\n",
            "[epoch:  10/100000, batch:   768/  840, ite: 694] train loss: 2.434163, tar: 0.326067 \n",
            "l0: 0.251278, l1: 0.260438, l2: 0.255242, l3: 0.279345, l4: 0.276210, l5: 0.289251, l6: 0.325903\n",
            "\n",
            "[epoch:  10/100000, batch:   780/  840, ite: 695] train loss: 2.433449, tar: 0.325959 \n",
            "l0: 0.454447, l1: 0.468097, l2: 0.460111, l3: 0.468616, l4: 0.470225, l5: 0.449680, l6: 0.469911\n",
            "\n",
            "[epoch:  10/100000, batch:   792/  840, ite: 696] train loss: 2.434609, tar: 0.326144 \n",
            "l0: 0.248786, l1: 0.238112, l2: 0.256104, l3: 0.273405, l4: 0.275108, l5: 0.278286, l6: 0.298653\n",
            "\n",
            "[epoch:  10/100000, batch:   804/  840, ite: 697] train loss: 2.433797, tar: 0.326033 \n",
            "l0: 0.201970, l1: 0.196774, l2: 0.209559, l3: 0.237928, l4: 0.234769, l5: 0.238186, l6: 0.256441\n",
            "\n",
            "[epoch:  10/100000, batch:   816/  840, ite: 698] train loss: 2.432567, tar: 0.325855 \n",
            "l0: 0.218970, l1: 0.219618, l2: 0.227902, l3: 0.243699, l4: 0.236668, l5: 0.249039, l6: 0.287223\n",
            "\n",
            "[epoch:  10/100000, batch:   828/  840, ite: 699] train loss: 2.431495, tar: 0.325702 \n",
            "l0: 0.215541, l1: 0.230664, l2: 0.235992, l3: 0.245316, l4: 0.236669, l5: 0.224133, l6: 0.244639\n",
            "\n",
            "[epoch:  10/100000, batch:   840/  840, ite: 700] train loss: 2.430354, tar: 0.325545 \n",
            "l0: 0.315470, l1: 0.339237, l2: 0.314752, l3: 0.318397, l4: 0.327292, l5: 0.351786, l6: 0.368643\n",
            "\n",
            "[epoch:  11/100000, batch:    12/  840, ite: 701] train loss: 2.430219, tar: 0.325530 \n",
            "l0: 0.228824, l1: 0.241364, l2: 0.245468, l3: 0.250531, l4: 0.247532, l5: 0.267380, l6: 0.279889\n",
            "\n",
            "[epoch:  11/100000, batch:    24/  840, ite: 702] train loss: 2.429266, tar: 0.325393 \n",
            "l0: 0.296854, l1: 0.284624, l2: 0.307052, l3: 0.314998, l4: 0.312113, l5: 0.325655, l6: 0.359715\n",
            "\n",
            "[epoch:  11/100000, batch:    36/  840, ite: 703] train loss: 2.428941, tar: 0.325352 \n",
            "l0: 0.147721, l1: 0.156813, l2: 0.152072, l3: 0.179805, l4: 0.178406, l5: 0.181756, l6: 0.207422\n",
            "\n",
            "[epoch:  11/100000, batch:    48/  840, ite: 704] train loss: 2.427201, tar: 0.325100 \n",
            "l0: 0.286034, l1: 0.279323, l2: 0.287753, l3: 0.318151, l4: 0.315931, l5: 0.343074, l6: 0.389250\n",
            "\n",
            "[epoch:  11/100000, batch:    60/  840, ite: 705] train loss: 2.426906, tar: 0.325044 \n",
            "l0: 0.313532, l1: 0.304802, l2: 0.333495, l3: 0.330166, l4: 0.338686, l5: 0.315275, l6: 0.327180\n",
            "\n",
            "[epoch:  11/100000, batch:    72/  840, ite: 706] train loss: 2.426674, tar: 0.325028 \n",
            "l0: 0.266469, l1: 0.277340, l2: 0.270579, l3: 0.280458, l4: 0.286155, l5: 0.306122, l6: 0.341012\n",
            "\n",
            "[epoch:  11/100000, batch:    84/  840, ite: 707] train loss: 2.426111, tar: 0.324945 \n",
            "l0: 0.324339, l1: 0.337846, l2: 0.344029, l3: 0.354673, l4: 0.343929, l5: 0.340441, l6: 0.359560\n",
            "\n",
            "[epoch:  11/100000, batch:    96/  840, ite: 708] train loss: 2.426080, tar: 0.324944 \n",
            "l0: 0.266935, l1: 0.254877, l2: 0.273915, l3: 0.304946, l4: 0.312283, l5: 0.319337, l6: 0.337714\n",
            "\n",
            "[epoch:  11/100000, batch:   108/  840, ite: 709] train loss: 2.425579, tar: 0.324863 \n",
            "l0: 0.176538, l1: 0.173531, l2: 0.173368, l3: 0.203787, l4: 0.216366, l5: 0.236880, l6: 0.270720\n",
            "\n",
            "[epoch:  11/100000, batch:   120/  840, ite: 710] train loss: 2.424206, tar: 0.324654 \n",
            "l0: 0.425765, l1: 0.403503, l2: 0.422796, l3: 0.463105, l4: 0.474465, l5: 0.500834, l6: 0.479329\n",
            "\n",
            "[epoch:  11/100000, batch:   132/  840, ite: 711] train loss: 2.425255, tar: 0.324796 \n",
            "l0: 0.347842, l1: 0.319639, l2: 0.398091, l3: 0.380834, l4: 0.367699, l5: 0.375143, l6: 0.391789\n",
            "\n",
            "[epoch:  11/100000, batch:   144/  840, ite: 712] train loss: 2.425473, tar: 0.324828 \n",
            "l0: 0.179665, l1: 0.169586, l2: 0.173071, l3: 0.215378, l4: 0.228655, l5: 0.225545, l6: 0.250933\n",
            "\n",
            "[epoch:  11/100000, batch:   156/  840, ite: 713] train loss: 2.424095, tar: 0.324625 \n",
            "l0: 0.270413, l1: 0.250860, l2: 0.269965, l3: 0.322779, l4: 0.329827, l5: 0.335517, l6: 0.349639\n",
            "\n",
            "[epoch:  11/100000, batch:   168/  840, ite: 714] train loss: 2.423682, tar: 0.324549 \n",
            "l0: 0.319609, l1: 0.319476, l2: 0.347635, l3: 0.346705, l4: 0.333588, l5: 0.344751, l6: 0.375759\n",
            "\n",
            "[epoch:  11/100000, batch:   180/  840, ite: 715] train loss: 2.423632, tar: 0.324542 \n",
            "l0: 0.322175, l1: 0.331014, l2: 0.347425, l3: 0.347923, l4: 0.346075, l5: 0.326570, l6: 0.343223\n",
            "\n",
            "[epoch:  11/100000, batch:   192/  840, ite: 716] train loss: 2.423549, tar: 0.324539 \n",
            "l0: 0.258546, l1: 0.280538, l2: 0.245478, l3: 0.271465, l4: 0.285721, l5: 0.312504, l6: 0.336578\n",
            "\n",
            "[epoch:  11/100000, batch:   204/  840, ite: 717] train loss: 2.422945, tar: 0.324446 \n",
            "l0: 0.294095, l1: 0.314943, l2: 0.319576, l3: 0.351249, l4: 0.339243, l5: 0.346860, l6: 0.346368\n",
            "\n",
            "[epoch:  11/100000, batch:   216/  840, ite: 718] train loss: 2.422791, tar: 0.324404 \n",
            "l0: 0.192369, l1: 0.183722, l2: 0.198031, l3: 0.223038, l4: 0.226203, l5: 0.236771, l6: 0.257467\n",
            "\n",
            "[epoch:  11/100000, batch:   228/  840, ite: 719] train loss: 2.421532, tar: 0.324221 \n",
            "l0: 0.236240, l1: 0.241774, l2: 0.243440, l3: 0.244446, l4: 0.247587, l5: 0.261735, l6: 0.321273\n",
            "\n",
            "[epoch:  11/100000, batch:   240/  840, ite: 720] train loss: 2.420664, tar: 0.324098 \n",
            "l0: 0.226075, l1: 0.222101, l2: 0.219263, l3: 0.257832, l4: 0.268640, l5: 0.273386, l6: 0.296151\n",
            "\n",
            "[epoch:  11/100000, batch:   252/  840, ite: 721] train loss: 2.419753, tar: 0.323962 \n",
            "l0: 0.266227, l1: 0.256079, l2: 0.279142, l3: 0.281657, l4: 0.284617, l5: 0.294965, l6: 0.321851\n",
            "\n",
            "[epoch:  11/100000, batch:   264/  840, ite: 722] train loss: 2.419150, tar: 0.323882 \n",
            "l0: 0.217059, l1: 0.204667, l2: 0.234913, l3: 0.248041, l4: 0.242441, l5: 0.247188, l6: 0.269211\n",
            "\n",
            "[epoch:  11/100000, batch:   276/  840, ite: 723] train loss: 2.418105, tar: 0.323735 \n",
            "l0: 0.220812, l1: 0.200432, l2: 0.218975, l3: 0.244185, l4: 0.256645, l5: 0.273077, l6: 0.315647\n",
            "\n",
            "[epoch:  11/100000, batch:   288/  840, ite: 724] train loss: 2.417154, tar: 0.323593 \n",
            "l0: 0.266505, l1: 0.290415, l2: 0.282253, l3: 0.282735, l4: 0.277140, l5: 0.298907, l6: 0.315223\n",
            "\n",
            "[epoch:  11/100000, batch:   300/  840, ite: 725] train loss: 2.416597, tar: 0.323514 \n",
            "l0: 0.228474, l1: 0.209717, l2: 0.248074, l3: 0.260282, l4: 0.252923, l5: 0.272829, l6: 0.296353\n",
            "\n",
            "[epoch:  11/100000, batch:   312/  840, ite: 726] train loss: 2.415704, tar: 0.323383 \n",
            "l0: 0.204122, l1: 0.211410, l2: 0.226069, l3: 0.214088, l4: 0.217229, l5: 0.220503, l6: 0.276220\n",
            "\n",
            "[epoch:  11/100000, batch:   324/  840, ite: 727] train loss: 2.414541, tar: 0.323219 \n",
            "l0: 0.300947, l1: 0.273094, l2: 0.305366, l3: 0.320666, l4: 0.325161, l5: 0.380119, l6: 0.407194\n",
            "\n",
            "[epoch:  11/100000, batch:   336/  840, ite: 728] train loss: 2.414401, tar: 0.323188 \n",
            "l0: 0.165223, l1: 0.174679, l2: 0.175421, l3: 0.171005, l4: 0.176797, l5: 0.204302, l6: 0.248142\n",
            "\n",
            "[epoch:  11/100000, batch:   348/  840, ite: 729] train loss: 2.412893, tar: 0.322972 \n",
            "l0: 0.300398, l1: 0.324767, l2: 0.283534, l3: 0.340600, l4: 0.335186, l5: 0.370002, l6: 0.390371\n",
            "\n",
            "[epoch:  11/100000, batch:   360/  840, ite: 730] train loss: 2.412800, tar: 0.322941 \n",
            "l0: 0.285123, l1: 0.278774, l2: 0.287127, l3: 0.310149, l4: 0.307798, l5: 0.326307, l6: 0.350289\n",
            "\n",
            "[epoch:  11/100000, batch:   372/  840, ite: 731] train loss: 2.412434, tar: 0.322889 \n",
            "l0: 0.160745, l1: 0.169550, l2: 0.153812, l3: 0.185350, l4: 0.192793, l5: 0.200773, l6: 0.243887\n",
            "\n",
            "[epoch:  11/100000, batch:   384/  840, ite: 732] train loss: 2.410924, tar: 0.322667 \n",
            "l0: 0.234951, l1: 0.216285, l2: 0.252093, l3: 0.231685, l4: 0.235539, l5: 0.253148, l6: 0.303709\n",
            "\n",
            "[epoch:  11/100000, batch:   396/  840, ite: 733] train loss: 2.409992, tar: 0.322548 \n",
            "l0: 0.282235, l1: 0.290523, l2: 0.315425, l3: 0.313687, l4: 0.313860, l5: 0.301403, l6: 0.293971\n",
            "\n",
            "[epoch:  11/100000, batch:   408/  840, ite: 734] train loss: 2.409584, tar: 0.322493 \n",
            "l0: 0.226850, l1: 0.242793, l2: 0.228554, l3: 0.270183, l4: 0.274853, l5: 0.293000, l6: 0.281240\n",
            "\n",
            "[epoch:  11/100000, batch:   420/  840, ite: 735] train loss: 2.408779, tar: 0.322363 \n",
            "l0: 0.230566, l1: 0.228070, l2: 0.237834, l3: 0.259719, l4: 0.263031, l5: 0.271189, l6: 0.294331\n",
            "\n",
            "[epoch:  11/100000, batch:   432/  840, ite: 736] train loss: 2.407931, tar: 0.322238 \n",
            "l0: 0.163942, l1: 0.160355, l2: 0.180571, l3: 0.172650, l4: 0.180240, l5: 0.196727, l6: 0.231808\n",
            "\n",
            "[epoch:  11/100000, batch:   444/  840, ite: 737] train loss: 2.406409, tar: 0.322023 \n",
            "l0: 0.245296, l1: 0.251701, l2: 0.259370, l3: 0.261006, l4: 0.268060, l5: 0.284413, l6: 0.304859\n",
            "\n",
            "[epoch:  11/100000, batch:   456/  840, ite: 738] train loss: 2.405689, tar: 0.321919 \n",
            "l0: 0.266118, l1: 0.286968, l2: 0.280171, l3: 0.299129, l4: 0.283458, l5: 0.305634, l6: 0.335610\n",
            "\n",
            "[epoch:  11/100000, batch:   468/  840, ite: 739] train loss: 2.405217, tar: 0.321844 \n",
            "l0: 0.318646, l1: 0.304465, l2: 0.329625, l3: 0.367940, l4: 0.369200, l5: 0.392851, l6: 0.386337\n",
            "\n",
            "[epoch:  11/100000, batch:   480/  840, ite: 740] train loss: 2.405303, tar: 0.321839 \n",
            "l0: 0.141220, l1: 0.151849, l2: 0.151529, l3: 0.155374, l4: 0.161663, l5: 0.176986, l6: 0.215161\n",
            "\n",
            "[epoch:  11/100000, batch:   492/  840, ite: 741] train loss: 2.403614, tar: 0.321596 \n",
            "l0: 0.346567, l1: 0.351148, l2: 0.359951, l3: 0.374679, l4: 0.380264, l5: 0.397699, l6: 0.390312\n",
            "\n",
            "[epoch:  11/100000, batch:   504/  840, ite: 742] train loss: 2.403880, tar: 0.321629 \n",
            "l0: 0.257665, l1: 0.262816, l2: 0.264777, l3: 0.294028, l4: 0.282862, l5: 0.289496, l6: 0.313217\n",
            "\n",
            "[epoch:  11/100000, batch:   516/  840, ite: 743] train loss: 2.403289, tar: 0.321543 \n",
            "l0: 0.249364, l1: 0.242217, l2: 0.260114, l3: 0.267524, l4: 0.277894, l5: 0.282812, l6: 0.307633\n",
            "\n",
            "[epoch:  11/100000, batch:   528/  840, ite: 744] train loss: 2.402596, tar: 0.321446 \n",
            "l0: 0.239707, l1: 0.252156, l2: 0.239118, l3: 0.247809, l4: 0.269245, l5: 0.290987, l6: 0.287759\n",
            "\n",
            "[epoch:  11/100000, batch:   540/  840, ite: 745] train loss: 2.401823, tar: 0.321336 \n",
            "l0: 0.182980, l1: 0.187291, l2: 0.184800, l3: 0.214262, l4: 0.208705, l5: 0.215934, l6: 0.245380\n",
            "\n",
            "[epoch:  11/100000, batch:   552/  840, ite: 746] train loss: 2.400533, tar: 0.321151 \n",
            "l0: 0.361509, l1: 0.370584, l2: 0.371231, l3: 0.410732, l4: 0.407043, l5: 0.410015, l6: 0.439293\n",
            "\n",
            "[epoch:  11/100000, batch:   564/  840, ite: 747] train loss: 2.401028, tar: 0.321205 \n",
            "l0: 0.159455, l1: 0.176282, l2: 0.159180, l3: 0.177311, l4: 0.172943, l5: 0.180393, l6: 0.214626\n",
            "\n",
            "[epoch:  11/100000, batch:   576/  840, ite: 748] train loss: 2.399476, tar: 0.320989 \n",
            "l0: 0.259946, l1: 0.248082, l2: 0.260184, l3: 0.288467, l4: 0.289978, l5: 0.303586, l6: 0.339694\n",
            "\n",
            "[epoch:  11/100000, batch:   588/  840, ite: 749] train loss: 2.398929, tar: 0.320907 \n",
            "l0: 0.246178, l1: 0.251262, l2: 0.256095, l3: 0.267405, l4: 0.267143, l5: 0.293010, l6: 0.314619\n",
            "\n",
            "[epoch:  11/100000, batch:   600/  840, ite: 750] train loss: 2.398258, tar: 0.320808 \n",
            "l0: 0.360577, l1: 0.349190, l2: 0.362232, l3: 0.361958, l4: 0.377011, l5: 0.413105, l6: 0.446642\n",
            "\n",
            "[epoch:  11/100000, batch:   612/  840, ite: 751] train loss: 2.398621, tar: 0.320861 \n",
            "l0: 0.217804, l1: 0.226465, l2: 0.224693, l3: 0.223057, l4: 0.228724, l5: 0.240501, l6: 0.262823\n",
            "\n",
            "[epoch:  11/100000, batch:   624/  840, ite: 752] train loss: 2.397591, tar: 0.320724 \n",
            "l0: 0.201855, l1: 0.204500, l2: 0.209946, l3: 0.216586, l4: 0.226177, l5: 0.242421, l6: 0.272521\n",
            "\n",
            "[epoch:  11/100000, batch:   636/  840, ite: 753] train loss: 2.396497, tar: 0.320566 \n",
            "l0: 0.311675, l1: 0.324707, l2: 0.308753, l3: 0.355834, l4: 0.347304, l5: 0.352869, l6: 0.366673\n",
            "\n",
            "[epoch:  11/100000, batch:   648/  840, ite: 754] train loss: 2.396459, tar: 0.320554 \n",
            "l0: 0.165794, l1: 0.163190, l2: 0.168879, l3: 0.173277, l4: 0.177818, l5: 0.208910, l6: 0.236781\n",
            "\n",
            "[epoch:  11/100000, batch:   660/  840, ite: 755] train loss: 2.395000, tar: 0.320349 \n",
            "l0: 0.197246, l1: 0.200204, l2: 0.198473, l3: 0.259676, l4: 0.233404, l5: 0.274920, l6: 0.273989\n",
            "\n",
            "[epoch:  11/100000, batch:   672/  840, ite: 756] train loss: 2.393998, tar: 0.320186 \n",
            "l0: 0.366704, l1: 0.336026, l2: 0.378151, l3: 0.394036, l4: 0.401787, l5: 0.393298, l6: 0.405919\n",
            "\n",
            "[epoch:  11/100000, batch:   684/  840, ite: 757] train loss: 2.394371, tar: 0.320248 \n",
            "l0: 0.250009, l1: 0.258750, l2: 0.265360, l3: 0.279659, l4: 0.271276, l5: 0.275368, l6: 0.286986\n",
            "\n",
            "[epoch:  11/100000, batch:   696/  840, ite: 758] train loss: 2.393702, tar: 0.320155 \n",
            "l0: 0.285427, l1: 0.293798, l2: 0.297318, l3: 0.304333, l4: 0.300991, l5: 0.311552, l6: 0.335812\n",
            "\n",
            "[epoch:  11/100000, batch:   708/  840, ite: 759] train loss: 2.393354, tar: 0.320109 \n",
            "l0: 0.220269, l1: 0.215069, l2: 0.212219, l3: 0.258397, l4: 0.261380, l5: 0.299731, l6: 0.332378\n",
            "\n",
            "[epoch:  11/100000, batch:   720/  840, ite: 760] train loss: 2.392572, tar: 0.319978 \n",
            "l0: 0.328152, l1: 0.334683, l2: 0.312204, l3: 0.389883, l4: 0.383027, l5: 0.401938, l6: 0.403255\n",
            "\n",
            "[epoch:  11/100000, batch:   732/  840, ite: 761] train loss: 2.392783, tar: 0.319989 \n",
            "l0: 0.310064, l1: 0.302525, l2: 0.318097, l3: 0.321886, l4: 0.335680, l5: 0.323367, l6: 0.353893\n",
            "\n",
            "[epoch:  11/100000, batch:   744/  840, ite: 762] train loss: 2.392616, tar: 0.319975 \n",
            "l0: 0.227588, l1: 0.255957, l2: 0.240195, l3: 0.249718, l4: 0.237311, l5: 0.247313, l6: 0.264109\n",
            "\n",
            "[epoch:  11/100000, batch:   756/  840, ite: 763] train loss: 2.391738, tar: 0.319854 \n",
            "l0: 0.225956, l1: 0.230321, l2: 0.242495, l3: 0.252418, l4: 0.244985, l5: 0.253626, l6: 0.276150\n",
            "\n",
            "[epoch:  11/100000, batch:   768/  840, ite: 764] train loss: 2.390866, tar: 0.319732 \n",
            "l0: 0.230062, l1: 0.233167, l2: 0.251882, l3: 0.252245, l4: 0.252380, l5: 0.241874, l6: 0.258194\n",
            "\n",
            "[epoch:  11/100000, batch:   780/  840, ite: 765] train loss: 2.389989, tar: 0.319614 \n",
            "l0: 0.269374, l1: 0.290733, l2: 0.298437, l3: 0.275481, l4: 0.278081, l5: 0.290345, l6: 0.298108\n",
            "\n",
            "[epoch:  11/100000, batch:   792/  840, ite: 766] train loss: 2.389481, tar: 0.319549 \n",
            "l0: 0.244318, l1: 0.237275, l2: 0.255003, l3: 0.265019, l4: 0.273460, l5: 0.293459, l6: 0.303543\n",
            "\n",
            "[epoch:  11/100000, batch:   804/  840, ite: 767] train loss: 2.388806, tar: 0.319451 \n",
            "l0: 0.247595, l1: 0.244495, l2: 0.248229, l3: 0.292467, l4: 0.301243, l5: 0.301503, l6: 0.319887\n",
            "\n",
            "[epoch:  11/100000, batch:   816/  840, ite: 768] train loss: 2.388242, tar: 0.319357 \n",
            "l0: 0.190742, l1: 0.177630, l2: 0.205196, l3: 0.213621, l4: 0.210586, l5: 0.221084, l6: 0.243513\n",
            "\n",
            "[epoch:  11/100000, batch:   828/  840, ite: 769] train loss: 2.387038, tar: 0.319190 \n",
            "l0: 0.441070, l1: 0.445207, l2: 0.445763, l3: 0.460715, l4: 0.465289, l5: 0.447759, l6: 0.471175\n",
            "\n",
            "[epoch:  11/100000, batch:   840/  840, ite: 770] train loss: 2.388064, tar: 0.319348 \n",
            "l0: 0.223140, l1: 0.215614, l2: 0.216544, l3: 0.255536, l4: 0.255918, l5: 0.268839, l6: 0.299434\n",
            "\n",
            "[epoch:  12/100000, batch:    12/  840, ite: 771] train loss: 2.387217, tar: 0.319223 \n",
            "l0: 0.190916, l1: 0.188685, l2: 0.202434, l3: 0.229102, l4: 0.230730, l5: 0.228171, l6: 0.238543\n",
            "\n",
            "[epoch:  12/100000, batch:    24/  840, ite: 772] train loss: 2.386078, tar: 0.319057 \n",
            "l0: 0.271930, l1: 0.270455, l2: 0.268385, l3: 0.293959, l4: 0.297768, l5: 0.309108, l6: 0.337573\n",
            "\n",
            "[epoch:  12/100000, batch:    36/  840, ite: 773] train loss: 2.385643, tar: 0.318996 \n",
            "l0: 0.283997, l1: 0.302625, l2: 0.283691, l3: 0.297078, l4: 0.304500, l5: 0.335983, l6: 0.334341\n",
            "\n",
            "[epoch:  12/100000, batch:    48/  840, ite: 774] train loss: 2.385328, tar: 0.318951 \n",
            "l0: 0.259887, l1: 0.259979, l2: 0.249072, l3: 0.271627, l4: 0.284092, l5: 0.292582, l6: 0.324946\n",
            "\n",
            "[epoch:  12/100000, batch:    60/  840, ite: 775] train loss: 2.384756, tar: 0.318875 \n",
            "l0: 0.193378, l1: 0.209439, l2: 0.201172, l3: 0.205274, l4: 0.201500, l5: 0.183614, l6: 0.228340\n",
            "\n",
            "[epoch:  12/100000, batch:    72/  840, ite: 776] train loss: 2.383517, tar: 0.318713 \n",
            "l0: 0.336387, l1: 0.345763, l2: 0.322749, l3: 0.365085, l4: 0.369901, l5: 0.367900, l6: 0.386779\n",
            "\n",
            "[epoch:  12/100000, batch:    84/  840, ite: 777] train loss: 2.383659, tar: 0.318736 \n",
            "l0: 0.381704, l1: 0.343667, l2: 0.376274, l3: 0.385010, l4: 0.404461, l5: 0.572116, l6: 0.560312\n",
            "\n",
            "[epoch:  12/100000, batch:    96/  840, ite: 778] train loss: 2.384482, tar: 0.318817 \n",
            "l0: 0.217357, l1: 0.202442, l2: 0.225771, l3: 0.253818, l4: 0.250742, l5: 0.301230, l6: 0.335666\n",
            "\n",
            "[epoch:  12/100000, batch:   108/  840, ite: 779] train loss: 2.383715, tar: 0.318686 \n",
            "l0: 0.233224, l1: 0.222655, l2: 0.250919, l3: 0.248558, l4: 0.254504, l5: 0.285276, l6: 0.304359\n",
            "\n",
            "[epoch:  12/100000, batch:   120/  840, ite: 780] train loss: 2.382966, tar: 0.318577 \n",
            "l0: 0.309509, l1: 0.310550, l2: 0.330399, l3: 0.330741, l4: 0.321579, l5: 0.327464, l6: 0.348479\n",
            "\n",
            "[epoch:  12/100000, batch:   132/  840, ite: 781] train loss: 2.382832, tar: 0.318565 \n",
            "l0: 0.225312, l1: 0.212909, l2: 0.235362, l3: 0.256030, l4: 0.239548, l5: 0.254784, l6: 0.299107\n",
            "\n",
            "[epoch:  12/100000, batch:   144/  840, ite: 782] train loss: 2.381989, tar: 0.318446 \n",
            "l0: 0.238283, l1: 0.229068, l2: 0.260999, l3: 0.274642, l4: 0.280717, l5: 0.269031, l6: 0.278281\n",
            "\n",
            "[epoch:  12/100000, batch:   156/  840, ite: 783] train loss: 2.381285, tar: 0.318344 \n",
            "l0: 0.221717, l1: 0.244425, l2: 0.233908, l3: 0.264076, l4: 0.261865, l5: 0.268720, l6: 0.278527\n",
            "\n",
            "[epoch:  12/100000, batch:   168/  840, ite: 784] train loss: 2.380509, tar: 0.318220 \n",
            "l0: 0.298657, l1: 0.297302, l2: 0.308976, l3: 0.286784, l4: 0.294726, l5: 0.343886, l6: 0.387644\n",
            "\n",
            "[epoch:  12/100000, batch:   180/  840, ite: 785] train loss: 2.380302, tar: 0.318195 \n",
            "l0: 0.283701, l1: 0.273315, l2: 0.285815, l3: 0.314944, l4: 0.314830, l5: 0.360680, l6: 0.399425\n",
            "\n",
            "[epoch:  12/100000, batch:   192/  840, ite: 786] train loss: 2.380115, tar: 0.318152 \n",
            "l0: 0.176817, l1: 0.188503, l2: 0.192743, l3: 0.189310, l4: 0.188806, l5: 0.198123, l6: 0.224354\n",
            "\n",
            "[epoch:  12/100000, batch:   204/  840, ite: 787] train loss: 2.378817, tar: 0.317972 \n",
            "l0: 0.324890, l1: 0.303945, l2: 0.321964, l3: 0.356430, l4: 0.360896, l5: 0.408771, l6: 0.398499\n",
            "\n",
            "[epoch:  12/100000, batch:   216/  840, ite: 788] train loss: 2.378939, tar: 0.317981 \n",
            "l0: 0.281000, l1: 0.316627, l2: 0.286843, l3: 0.297325, l4: 0.296596, l5: 0.292425, l6: 0.302524\n",
            "\n",
            "[epoch:  12/100000, batch:   228/  840, ite: 789] train loss: 2.378552, tar: 0.317934 \n",
            "l0: 0.228252, l1: 0.248269, l2: 0.229294, l3: 0.251282, l4: 0.254047, l5: 0.283259, l6: 0.343536\n",
            "\n",
            "[epoch:  12/100000, batch:   240/  840, ite: 790] train loss: 2.377867, tar: 0.317820 \n",
            "l0: 0.284722, l1: 0.299148, l2: 0.283363, l3: 0.265786, l4: 0.280435, l5: 0.350678, l6: 0.423641\n",
            "\n",
            "[epoch:  12/100000, batch:   252/  840, ite: 791] train loss: 2.377627, tar: 0.317779 \n",
            "l0: 0.302488, l1: 0.288737, l2: 0.312575, l3: 0.359979, l4: 0.354579, l5: 0.379195, l6: 0.427296\n",
            "\n",
            "[epoch:  12/100000, batch:   264/  840, ite: 792] train loss: 2.377687, tar: 0.317759 \n",
            "l0: 0.212918, l1: 0.233355, l2: 0.230634, l3: 0.222199, l4: 0.223674, l5: 0.255104, l6: 0.285397\n",
            "\n",
            "[epoch:  12/100000, batch:   276/  840, ite: 793] train loss: 2.376786, tar: 0.317627 \n",
            "l0: 0.242366, l1: 0.222126, l2: 0.264015, l3: 0.254204, l4: 0.264583, l5: 0.254320, l6: 0.278853\n",
            "\n",
            "[epoch:  12/100000, batch:   288/  840, ite: 794] train loss: 2.376035, tar: 0.317532 \n",
            "l0: 0.257225, l1: 0.249866, l2: 0.259856, l3: 0.277508, l4: 0.280451, l5: 0.307705, l6: 0.336372\n",
            "\n",
            "[epoch:  12/100000, batch:   300/  840, ite: 795] train loss: 2.375523, tar: 0.317456 \n",
            "l0: 0.231875, l1: 0.259543, l2: 0.229096, l3: 0.216181, l4: 0.234420, l5: 0.252847, l6: 0.295430\n",
            "\n",
            "[epoch:  12/100000, batch:   312/  840, ite: 796] train loss: 2.374698, tar: 0.317349 \n",
            "l0: 0.226303, l1: 0.234064, l2: 0.212295, l3: 0.244028, l4: 0.245238, l5: 0.272356, l6: 0.318208\n",
            "\n",
            "[epoch:  12/100000, batch:   324/  840, ite: 797] train loss: 2.373918, tar: 0.317235 \n",
            "l0: 0.212853, l1: 0.205285, l2: 0.218115, l3: 0.229493, l4: 0.229965, l5: 0.267749, l6: 0.302462\n",
            "\n",
            "[epoch:  12/100000, batch:   336/  840, ite: 798] train loss: 2.373030, tar: 0.317104 \n",
            "l0: 0.257535, l1: 0.285273, l2: 0.239649, l3: 0.265795, l4: 0.273756, l5: 0.302399, l6: 0.354645\n",
            "\n",
            "[epoch:  12/100000, batch:   348/  840, ite: 799] train loss: 2.372537, tar: 0.317029 \n",
            "l0: 0.214779, l1: 0.216599, l2: 0.220301, l3: 0.225914, l4: 0.229130, l5: 0.250488, l6: 0.278724\n",
            "\n",
            "[epoch:  12/100000, batch:   360/  840, ite: 800] train loss: 2.371617, tar: 0.316901 \n",
            "l0: 0.176113, l1: 0.179436, l2: 0.186100, l3: 0.187827, l4: 0.192938, l5: 0.213307, l6: 0.254104\n",
            "\n",
            "[epoch:  12/100000, batch:   372/  840, ite: 801] train loss: 2.370391, tar: 0.316726 \n",
            "l0: 0.190841, l1: 0.176548, l2: 0.217498, l3: 0.200360, l4: 0.200790, l5: 0.214077, l6: 0.238480\n",
            "\n",
            "[epoch:  12/100000, batch:   384/  840, ite: 802] train loss: 2.369229, tar: 0.316569 \n",
            "l0: 0.161866, l1: 0.158357, l2: 0.163460, l3: 0.180237, l4: 0.187547, l5: 0.205574, l6: 0.241624\n",
            "\n",
            "[epoch:  12/100000, batch:   396/  840, ite: 803] train loss: 2.367896, tar: 0.316376 \n",
            "l0: 0.222868, l1: 0.225526, l2: 0.231195, l3: 0.233465, l4: 0.236107, l5: 0.267043, l6: 0.292842\n",
            "\n",
            "[epoch:  12/100000, batch:   408/  840, ite: 804] train loss: 2.367076, tar: 0.316260 \n",
            "l0: 0.188989, l1: 0.190001, l2: 0.199428, l3: 0.211737, l4: 0.214281, l5: 0.216491, l6: 0.232251\n",
            "\n",
            "[epoch:  12/100000, batch:   420/  840, ite: 805] train loss: 2.365941, tar: 0.316102 \n",
            "l0: 0.317420, l1: 0.337530, l2: 0.338942, l3: 0.298297, l4: 0.307846, l5: 0.327445, l6: 0.344955\n",
            "\n",
            "[epoch:  12/100000, batch:   432/  840, ite: 806] train loss: 2.365825, tar: 0.316103 \n",
            "l0: 0.149239, l1: 0.158098, l2: 0.156019, l3: 0.161110, l4: 0.167388, l5: 0.183625, l6: 0.209968\n",
            "\n",
            "[epoch:  12/100000, batch:   444/  840, ite: 807] train loss: 2.364362, tar: 0.315897 \n",
            "l0: 0.232266, l1: 0.241107, l2: 0.237192, l3: 0.241471, l4: 0.251060, l5: 0.270239, l6: 0.307124\n",
            "\n",
            "[epoch:  12/100000, batch:   456/  840, ite: 808] train loss: 2.363640, tar: 0.315793 \n",
            "l0: 0.222558, l1: 0.211132, l2: 0.212983, l3: 0.260824, l4: 0.265250, l5: 0.294933, l6: 0.342203\n",
            "\n",
            "[epoch:  12/100000, batch:   468/  840, ite: 809] train loss: 2.362955, tar: 0.315678 \n",
            "l0: 0.218517, l1: 0.215587, l2: 0.237843, l3: 0.229592, l4: 0.217621, l5: 0.233742, l6: 0.286777\n",
            "\n",
            "[epoch:  12/100000, batch:   480/  840, ite: 810] train loss: 2.362063, tar: 0.315558 \n",
            "l0: 0.204191, l1: 0.202730, l2: 0.207340, l3: 0.228194, l4: 0.242172, l5: 0.255151, l6: 0.275078\n",
            "\n",
            "[epoch:  12/100000, batch:   492/  840, ite: 811] train loss: 2.361141, tar: 0.315421 \n",
            "l0: 0.409300, l1: 0.398230, l2: 0.435545, l3: 0.439762, l4: 0.419428, l5: 0.410164, l6: 0.407534\n",
            "\n",
            "[epoch:  12/100000, batch:   504/  840, ite: 812] train loss: 2.361829, tar: 0.315536 \n",
            "l0: 0.141568, l1: 0.148421, l2: 0.154472, l3: 0.150276, l4: 0.144625, l5: 0.158680, l6: 0.198127\n",
            "\n",
            "[epoch:  12/100000, batch:   516/  840, ite: 813] train loss: 2.360273, tar: 0.315322 \n",
            "l0: 0.404869, l1: 0.401380, l2: 0.393875, l3: 0.484500, l4: 0.467633, l5: 0.465374, l6: 0.468922\n",
            "\n",
            "[epoch:  12/100000, batch:   528/  840, ite: 814] train loss: 2.361165, tar: 0.315432 \n",
            "l0: 0.187451, l1: 0.185070, l2: 0.179252, l3: 0.205396, l4: 0.212551, l5: 0.232091, l6: 0.287181\n",
            "\n",
            "[epoch:  12/100000, batch:   540/  840, ite: 815] train loss: 2.360095, tar: 0.315275 \n",
            "l0: 0.292913, l1: 0.273815, l2: 0.285833, l3: 0.313181, l4: 0.327406, l5: 0.340942, l6: 0.388722\n",
            "\n",
            "[epoch:  12/100000, batch:   552/  840, ite: 816] train loss: 2.359926, tar: 0.315248 \n",
            "l0: 0.343360, l1: 0.323781, l2: 0.345856, l3: 0.376963, l4: 0.386187, l5: 0.441990, l6: 0.444053\n",
            "\n",
            "[epoch:  12/100000, batch:   564/  840, ite: 817] train loss: 2.360296, tar: 0.315282 \n",
            "l0: 0.474514, l1: 0.466619, l2: 0.479242, l3: 0.483717, l4: 0.498394, l5: 0.496274, l6: 0.497314\n",
            "\n",
            "[epoch:  12/100000, batch:   576/  840, ite: 818] train loss: 2.361563, tar: 0.315477 \n",
            "l0: 0.295922, l1: 0.279022, l2: 0.275217, l3: 0.341783, l4: 0.350071, l5: 0.365923, l6: 0.421632\n",
            "\n",
            "[epoch:  12/100000, batch:   588/  840, ite: 819] train loss: 2.361524, tar: 0.315453 \n",
            "l0: 0.247155, l1: 0.256061, l2: 0.252600, l3: 0.291619, l4: 0.278639, l5: 0.279863, l6: 0.303172\n",
            "\n",
            "[epoch:  12/100000, batch:   600/  840, ite: 820] train loss: 2.360972, tar: 0.315370 \n",
            "l0: 0.216881, l1: 0.221995, l2: 0.217422, l3: 0.260563, l4: 0.263403, l5: 0.286480, l6: 0.302553\n",
            "\n",
            "[epoch:  12/100000, batch:   612/  840, ite: 821] train loss: 2.360251, tar: 0.315250 \n",
            "l0: 0.327441, l1: 0.305439, l2: 0.316409, l3: 0.358413, l4: 0.364890, l5: 0.381819, l6: 0.479865\n",
            "\n",
            "[epoch:  12/100000, batch:   624/  840, ite: 822] train loss: 2.360463, tar: 0.315265 \n",
            "l0: 0.383273, l1: 0.392307, l2: 0.387898, l3: 0.376261, l4: 0.372274, l5: 0.386530, l6: 0.445928\n",
            "\n",
            "[epoch:  12/100000, batch:   636/  840, ite: 823] train loss: 2.360930, tar: 0.315347 \n",
            "l0: 0.330380, l1: 0.300392, l2: 0.347670, l3: 0.371062, l4: 0.365653, l5: 0.360125, l6: 0.375471\n",
            "\n",
            "[epoch:  12/100000, batch:   648/  840, ite: 824] train loss: 2.361039, tar: 0.315365 \n",
            "l0: 0.244530, l1: 0.244466, l2: 0.243623, l3: 0.276559, l4: 0.287924, l5: 0.291455, l6: 0.318831\n",
            "\n",
            "[epoch:  12/100000, batch:   660/  840, ite: 825] train loss: 2.360489, tar: 0.315280 \n",
            "l0: 0.381725, l1: 0.401878, l2: 0.387766, l3: 0.424307, l4: 0.432093, l5: 0.405127, l6: 0.424134\n",
            "\n",
            "[epoch:  12/100000, batch:   672/  840, ite: 826] train loss: 2.361090, tar: 0.315360 \n",
            "l0: 0.273636, l1: 0.285545, l2: 0.271737, l3: 0.304065, l4: 0.311376, l5: 0.311849, l6: 0.336592\n",
            "\n",
            "[epoch:  12/100000, batch:   684/  840, ite: 827] train loss: 2.360768, tar: 0.315310 \n",
            "l0: 0.443176, l1: 0.428407, l2: 0.466085, l3: 0.469445, l4: 0.450321, l5: 0.486574, l6: 0.551480\n",
            "\n",
            "[epoch:  12/100000, batch:   696/  840, ite: 828] train loss: 2.361897, tar: 0.315464 \n",
            "l0: 0.338026, l1: 0.340543, l2: 0.341110, l3: 0.346207, l4: 0.354790, l5: 0.378551, l6: 0.435731\n",
            "\n",
            "[epoch:  12/100000, batch:   708/  840, ite: 829] train loss: 2.362106, tar: 0.315491 \n",
            "l0: 0.167222, l1: 0.168427, l2: 0.172420, l3: 0.197613, l4: 0.205275, l5: 0.197200, l6: 0.207214\n",
            "\n",
            "[epoch:  12/100000, batch:   720/  840, ite: 830] train loss: 2.360845, tar: 0.315313 \n",
            "l0: 0.251412, l1: 0.248095, l2: 0.252244, l3: 0.255355, l4: 0.264554, l5: 0.292891, l6: 0.336176\n",
            "\n",
            "[epoch:  12/100000, batch:   732/  840, ite: 831] train loss: 2.360291, tar: 0.315236 \n",
            "l0: 0.255147, l1: 0.248164, l2: 0.258884, l3: 0.271579, l4: 0.279508, l5: 0.296034, l6: 0.328760\n",
            "\n",
            "[epoch:  12/100000, batch:   744/  840, ite: 832] train loss: 2.359784, tar: 0.315163 \n",
            "l0: 0.281785, l1: 0.286317, l2: 0.294731, l3: 0.307304, l4: 0.307023, l5: 0.320953, l6: 0.352029\n",
            "\n",
            "[epoch:  12/100000, batch:   756/  840, ite: 833] train loss: 2.359532, tar: 0.315123 \n",
            "l0: 0.194033, l1: 0.192806, l2: 0.202768, l3: 0.228199, l4: 0.235581, l5: 0.249141, l6: 0.266947\n",
            "\n",
            "[epoch:  12/100000, batch:   768/  840, ite: 834] train loss: 2.358585, tar: 0.314978 \n",
            "l0: 0.198482, l1: 0.203284, l2: 0.213265, l3: 0.192744, l4: 0.196184, l5: 0.216975, l6: 0.239610\n",
            "\n",
            "[epoch:  12/100000, batch:   780/  840, ite: 835] train loss: 2.357509, tar: 0.314839 \n",
            "l0: 0.204163, l1: 0.200035, l2: 0.205243, l3: 0.236332, l4: 0.239188, l5: 0.255306, l6: 0.277768\n",
            "\n",
            "[epoch:  12/100000, batch:   792/  840, ite: 836] train loss: 2.356624, tar: 0.314706 \n",
            "l0: 0.147521, l1: 0.150216, l2: 0.158085, l3: 0.164130, l4: 0.166511, l5: 0.186716, l6: 0.210925\n",
            "\n",
            "[epoch:  12/100000, batch:   804/  840, ite: 837] train loss: 2.355223, tar: 0.314507 \n",
            "l0: 0.321648, l1: 0.299761, l2: 0.317644, l3: 0.354801, l4: 0.378310, l5: 0.430944, l6: 0.450204\n",
            "\n",
            "[epoch:  12/100000, batch:   816/  840, ite: 838] train loss: 2.355460, tar: 0.314515 \n",
            "l0: 0.187431, l1: 0.184599, l2: 0.188808, l3: 0.215239, l4: 0.221367, l5: 0.249450, l6: 0.288608\n",
            "\n",
            "[epoch:  12/100000, batch:   828/  840, ite: 839] train loss: 2.354483, tar: 0.314364 \n",
            "l0: 0.225114, l1: 0.232119, l2: 0.242442, l3: 0.250815, l4: 0.229948, l5: 0.248574, l6: 0.282620\n",
            "\n",
            "[epoch:  12/100000, batch:   840/  840, ite: 840] train loss: 2.353718, tar: 0.314257 \n",
            "l0: 0.401152, l1: 0.421103, l2: 0.422257, l3: 0.394132, l4: 0.397253, l5: 0.415576, l6: 0.439292\n",
            "\n",
            "[epoch:  13/100000, batch:    12/  840, ite: 841] train loss: 2.354356, tar: 0.314361 \n",
            "l0: 0.111374, l1: 0.103303, l2: 0.112818, l3: 0.135303, l4: 0.135001, l5: 0.163510, l6: 0.197132\n",
            "\n",
            "[epoch:  13/100000, batch:    24/  840, ite: 842] train loss: 2.352698, tar: 0.314120 \n",
            "l0: 0.157383, l1: 0.171080, l2: 0.168967, l3: 0.160458, l4: 0.160657, l5: 0.170153, l6: 0.209636\n",
            "\n",
            "[epoch:  13/100000, batch:    36/  840, ite: 843] train loss: 2.351329, tar: 0.313934 \n",
            "l0: 0.308190, l1: 0.291981, l2: 0.305399, l3: 0.363355, l4: 0.368025, l5: 0.361524, l6: 0.390035\n",
            "\n",
            "[epoch:  13/100000, batch:    48/  840, ite: 844] train loss: 2.351373, tar: 0.313927 \n",
            "l0: 0.200291, l1: 0.221057, l2: 0.222274, l3: 0.218346, l4: 0.215265, l5: 0.216812, l6: 0.236436\n",
            "\n",
            "[epoch:  13/100000, batch:    60/  840, ite: 845] train loss: 2.350402, tar: 0.313792 \n",
            "l0: 0.277748, l1: 0.271233, l2: 0.311622, l3: 0.298089, l4: 0.289619, l5: 0.288398, l6: 0.315048\n",
            "\n",
            "[epoch:  13/100000, batch:    72/  840, ite: 846] train loss: 2.350049, tar: 0.313750 \n",
            "l0: 0.173209, l1: 0.179163, l2: 0.185187, l3: 0.185937, l4: 0.188863, l5: 0.196709, l6: 0.235072\n",
            "\n",
            "[epoch:  13/100000, batch:    84/  840, ite: 847] train loss: 2.348861, tar: 0.313584 \n",
            "l0: 0.146200, l1: 0.157945, l2: 0.153912, l3: 0.155567, l4: 0.152545, l5: 0.169321, l6: 0.216950\n",
            "\n",
            "[epoch:  13/100000, batch:    96/  840, ite: 848] train loss: 2.347450, tar: 0.313387 \n",
            "l0: 0.144890, l1: 0.156862, l2: 0.161330, l3: 0.150530, l4: 0.149960, l5: 0.167057, l6: 0.196604\n",
            "\n",
            "[epoch:  13/100000, batch:   108/  840, ite: 849] train loss: 2.346013, tar: 0.313188 \n",
            "l0: 0.313905, l1: 0.295244, l2: 0.330111, l3: 0.365985, l4: 0.363142, l5: 0.342349, l6: 0.365859\n",
            "\n",
            "[epoch:  13/100000, batch:   120/  840, ite: 850] train loss: 2.346049, tar: 0.313189 \n",
            "l0: 0.244517, l1: 0.237628, l2: 0.244217, l3: 0.268652, l4: 0.275615, l5: 0.292251, l6: 0.297630\n",
            "\n",
            "[epoch:  13/100000, batch:   132/  840, ite: 851] train loss: 2.345478, tar: 0.313108 \n",
            "l0: 0.239850, l1: 0.249064, l2: 0.243495, l3: 0.253238, l4: 0.262537, l5: 0.276279, l6: 0.315318\n",
            "\n",
            "[epoch:  13/100000, batch:   144/  840, ite: 852] train loss: 2.344885, tar: 0.313022 \n",
            "l0: 0.178253, l1: 0.184984, l2: 0.186227, l3: 0.199112, l4: 0.198690, l5: 0.209021, l6: 0.239942\n",
            "\n",
            "[epoch:  13/100000, batch:   156/  840, ite: 853] train loss: 2.343772, tar: 0.312864 \n",
            "l0: 0.342470, l1: 0.365207, l2: 0.336002, l3: 0.341130, l4: 0.351094, l5: 0.356548, l6: 0.403850\n",
            "\n",
            "[epoch:  13/100000, batch:   168/  840, ite: 854] train loss: 2.343951, tar: 0.312899 \n",
            "l0: 0.215824, l1: 0.228910, l2: 0.218905, l3: 0.223910, l4: 0.228952, l5: 0.239764, l6: 0.273721\n",
            "\n",
            "[epoch:  13/100000, batch:   180/  840, ite: 855] train loss: 2.343116, tar: 0.312785 \n",
            "l0: 0.275044, l1: 0.256341, l2: 0.276270, l3: 0.350827, l4: 0.362826, l5: 0.332444, l6: 0.341978\n",
            "\n",
            "[epoch:  13/100000, batch:   192/  840, ite: 856] train loss: 2.342944, tar: 0.312741 \n",
            "l0: 0.250722, l1: 0.255274, l2: 0.270662, l3: 0.267533, l4: 0.259937, l5: 0.267119, l6: 0.290317\n",
            "\n",
            "[epoch:  13/100000, batch:   204/  840, ite: 857] train loss: 2.342382, tar: 0.312669 \n",
            "l0: 0.288556, l1: 0.260674, l2: 0.304952, l3: 0.349085, l4: 0.336163, l5: 0.343766, l6: 0.377286\n",
            "\n",
            "[epoch:  13/100000, batch:   216/  840, ite: 858] train loss: 2.342287, tar: 0.312641 \n",
            "l0: 0.226160, l1: 0.229631, l2: 0.233260, l3: 0.260746, l4: 0.265738, l5: 0.272543, l6: 0.287981\n",
            "\n",
            "[epoch:  13/100000, batch:   228/  840, ite: 859] train loss: 2.341628, tar: 0.312540 \n",
            "l0: 0.326457, l1: 0.338270, l2: 0.363069, l3: 0.347273, l4: 0.340701, l5: 0.343060, l6: 0.350443\n",
            "\n",
            "[epoch:  13/100000, batch:   240/  840, ite: 860] train loss: 2.341706, tar: 0.312556 \n",
            "l0: 0.198687, l1: 0.211190, l2: 0.205465, l3: 0.238437, l4: 0.236733, l5: 0.249452, l6: 0.261939\n",
            "\n",
            "[epoch:  13/100000, batch:   252/  840, ite: 861] train loss: 2.340847, tar: 0.312424 \n",
            "l0: 0.297355, l1: 0.279309, l2: 0.310015, l3: 0.339235, l4: 0.350587, l5: 0.365692, l6: 0.368289\n",
            "\n",
            "[epoch:  13/100000, batch:   264/  840, ite: 862] train loss: 2.340811, tar: 0.312407 \n",
            "l0: 0.323583, l1: 0.310719, l2: 0.321857, l3: 0.345722, l4: 0.365048, l5: 0.392104, l6: 0.431842\n",
            "\n",
            "[epoch:  13/100000, batch:   276/  840, ite: 863] train loss: 2.340986, tar: 0.312420 \n",
            "l0: 0.132327, l1: 0.134500, l2: 0.134320, l3: 0.165039, l4: 0.166611, l5: 0.187197, l6: 0.203373\n",
            "\n",
            "[epoch:  13/100000, batch:   288/  840, ite: 864] train loss: 2.339576, tar: 0.312211 \n",
            "l0: 0.285154, l1: 0.294503, l2: 0.292494, l3: 0.304695, l4: 0.302997, l5: 0.300815, l6: 0.337348\n",
            "\n",
            "[epoch:  13/100000, batch:   300/  840, ite: 865] train loss: 2.339320, tar: 0.312180 \n",
            "l0: 0.151972, l1: 0.152189, l2: 0.153735, l3: 0.184024, l4: 0.188386, l5: 0.203582, l6: 0.219993\n",
            "\n",
            "[epoch:  13/100000, batch:   312/  840, ite: 866] train loss: 2.338067, tar: 0.311995 \n",
            "l0: 0.281760, l1: 0.295183, l2: 0.261377, l3: 0.314002, l4: 0.318210, l5: 0.340373, l6: 0.394297\n",
            "\n",
            "[epoch:  13/100000, batch:   324/  840, ite: 867] train loss: 2.337914, tar: 0.311960 \n",
            "l0: 0.462539, l1: 0.463094, l2: 0.470799, l3: 0.494478, l4: 0.489365, l5: 0.484650, l6: 0.498257\n",
            "\n",
            "[epoch:  13/100000, batch:   336/  840, ite: 868] train loss: 2.339095, tar: 0.312133 \n",
            "l0: 0.234204, l1: 0.224464, l2: 0.236446, l3: 0.255494, l4: 0.262206, l5: 0.272327, l6: 0.312488\n",
            "\n",
            "[epoch:  13/100000, batch:   348/  840, ite: 869] train loss: 2.338472, tar: 0.312044 \n",
            "l0: 0.258867, l1: 0.259496, l2: 0.259059, l3: 0.267096, l4: 0.273885, l5: 0.279389, l6: 0.327594\n",
            "\n",
            "[epoch:  13/100000, batch:   360/  840, ite: 870] train loss: 2.337997, tar: 0.311983 \n",
            "l0: 0.350121, l1: 0.349577, l2: 0.358952, l3: 0.385271, l4: 0.382418, l5: 0.410133, l6: 0.433880\n",
            "\n",
            "[epoch:  13/100000, batch:   372/  840, ite: 871] train loss: 2.338378, tar: 0.312026 \n",
            "l0: 0.323913, l1: 0.316705, l2: 0.325463, l3: 0.363928, l4: 0.363320, l5: 0.373225, l6: 0.391004\n",
            "\n",
            "[epoch:  13/100000, batch:   384/  840, ite: 872] train loss: 2.338515, tar: 0.312040 \n",
            "l0: 0.274709, l1: 0.298077, l2: 0.282567, l3: 0.300163, l4: 0.302887, l5: 0.319801, l6: 0.324679\n",
            "\n",
            "[epoch:  13/100000, batch:   396/  840, ite: 873] train loss: 2.338245, tar: 0.311997 \n",
            "l0: 0.204807, l1: 0.212475, l2: 0.209736, l3: 0.243016, l4: 0.237738, l5: 0.253985, l6: 0.270534\n",
            "\n",
            "[epoch:  13/100000, batch:   408/  840, ite: 874] train loss: 2.337437, tar: 0.311875 \n",
            "l0: 0.276686, l1: 0.309042, l2: 0.289562, l3: 0.277946, l4: 0.288605, l5: 0.277848, l6: 0.303933\n",
            "\n",
            "[epoch:  13/100000, batch:   420/  840, ite: 875] train loss: 2.337079, tar: 0.311834 \n",
            "l0: 0.174245, l1: 0.189422, l2: 0.170892, l3: 0.193602, l4: 0.199847, l5: 0.214541, l6: 0.248375\n",
            "\n",
            "[epoch:  13/100000, batch:   432/  840, ite: 876] train loss: 2.335999, tar: 0.311677 \n",
            "l0: 0.212518, l1: 0.238624, l2: 0.224989, l3: 0.230516, l4: 0.229691, l5: 0.230750, l6: 0.247782\n",
            "\n",
            "[epoch:  13/100000, batch:   444/  840, ite: 877] train loss: 2.335176, tar: 0.311564 \n",
            "l0: 0.235885, l1: 0.260527, l2: 0.244325, l3: 0.254224, l4: 0.247955, l5: 0.265763, l6: 0.303823\n",
            "\n",
            "[epoch:  13/100000, batch:   456/  840, ite: 878] train loss: 2.334581, tar: 0.311478 \n",
            "l0: 0.280599, l1: 0.284213, l2: 0.282691, l3: 0.313600, l4: 0.315319, l5: 0.328603, l6: 0.365118\n",
            "\n",
            "[epoch:  13/100000, batch:   468/  840, ite: 879] train loss: 2.334394, tar: 0.311443 \n",
            "l0: 0.369281, l1: 0.353697, l2: 0.383111, l3: 0.372967, l4: 0.380767, l5: 0.397368, l6: 0.456991\n",
            "\n",
            "[epoch:  13/100000, batch:   480/  840, ite: 880] train loss: 2.334826, tar: 0.311509 \n",
            "l0: 0.287922, l1: 0.270534, l2: 0.298311, l3: 0.299855, l4: 0.307036, l5: 0.341786, l6: 0.335546\n",
            "\n",
            "[epoch:  13/100000, batch:   492/  840, ite: 881] train loss: 2.334606, tar: 0.311482 \n",
            "l0: 0.236616, l1: 0.234299, l2: 0.231054, l3: 0.266094, l4: 0.275221, l5: 0.306428, l6: 0.309375\n",
            "\n",
            "[epoch:  13/100000, batch:   504/  840, ite: 882] train loss: 2.334067, tar: 0.311397 \n",
            "l0: 0.319739, l1: 0.308087, l2: 0.322426, l3: 0.345873, l4: 0.356980, l5: 0.391364, l6: 0.422471\n",
            "\n",
            "[epoch:  13/100000, batch:   516/  840, ite: 883] train loss: 2.334217, tar: 0.311407 \n",
            "l0: 0.180740, l1: 0.188232, l2: 0.182353, l3: 0.198957, l4: 0.199367, l5: 0.224186, l6: 0.269386\n",
            "\n",
            "[epoch:  13/100000, batch:   528/  840, ite: 884] train loss: 2.333209, tar: 0.311259 \n",
            "l0: 0.253743, l1: 0.266052, l2: 0.263853, l3: 0.262683, l4: 0.274108, l5: 0.274886, l6: 0.298248\n",
            "\n",
            "[epoch:  13/100000, batch:   540/  840, ite: 885] train loss: 2.332712, tar: 0.311194 \n",
            "l0: 0.289085, l1: 0.295659, l2: 0.276563, l3: 0.343600, l4: 0.368586, l5: 0.365210, l6: 0.396964\n",
            "\n",
            "[epoch:  13/100000, batch:   552/  840, ite: 886] train loss: 2.332716, tar: 0.311169 \n",
            "l0: 0.404292, l1: 0.398697, l2: 0.414746, l3: 0.454270, l4: 0.457817, l5: 0.450986, l6: 0.487289\n",
            "\n",
            "[epoch:  13/100000, batch:   564/  840, ite: 887] train loss: 2.333545, tar: 0.311274 \n",
            "l0: 0.219101, l1: 0.219027, l2: 0.218143, l3: 0.229445, l4: 0.239584, l5: 0.248249, l6: 0.282997\n",
            "\n",
            "[epoch:  13/100000, batch:   576/  840, ite: 888] train loss: 2.332783, tar: 0.311170 \n",
            "l0: 0.244269, l1: 0.257798, l2: 0.247699, l3: 0.269841, l4: 0.272125, l5: 0.281482, l6: 0.301964\n",
            "\n",
            "[epoch:  13/100000, batch:   588/  840, ite: 889] train loss: 2.332268, tar: 0.311095 \n",
            "l0: 0.282961, l1: 0.291775, l2: 0.299104, l3: 0.301058, l4: 0.300832, l5: 0.329846, l6: 0.342426\n",
            "\n",
            "[epoch:  13/100000, batch:   600/  840, ite: 890] train loss: 2.332061, tar: 0.311063 \n",
            "l0: 0.243638, l1: 0.237136, l2: 0.249407, l3: 0.272697, l4: 0.272633, l5: 0.294368, l6: 0.317531\n",
            "\n",
            "[epoch:  13/100000, batch:   612/  840, ite: 891] train loss: 2.331562, tar: 0.310987 \n",
            "l0: 0.186800, l1: 0.186772, l2: 0.191427, l3: 0.192346, l4: 0.196969, l5: 0.230220, l6: 0.257831\n",
            "\n",
            "[epoch:  13/100000, batch:   624/  840, ite: 892] train loss: 2.330565, tar: 0.310848 \n",
            "l0: 0.369165, l1: 0.351247, l2: 0.390282, l3: 0.401225, l4: 0.397005, l5: 0.400831, l6: 0.426480\n",
            "\n",
            "[epoch:  13/100000, batch:   636/  840, ite: 893] train loss: 2.331019, tar: 0.310914 \n",
            "l0: 0.168168, l1: 0.176895, l2: 0.180330, l3: 0.178423, l4: 0.178497, l5: 0.198198, l6: 0.229729\n",
            "\n",
            "[epoch:  13/100000, batch:   648/  840, ite: 894] train loss: 2.329877, tar: 0.310754 \n",
            "l0: 0.285894, l1: 0.285065, l2: 0.282179, l3: 0.312923, l4: 0.315178, l5: 0.344512, l6: 0.392085\n",
            "\n",
            "[epoch:  13/100000, batch:   660/  840, ite: 895] train loss: 2.329752, tar: 0.310726 \n",
            "l0: 0.268091, l1: 0.261309, l2: 0.288648, l3: 0.283980, l4: 0.286291, l5: 0.291557, l6: 0.336099\n",
            "\n",
            "[epoch:  13/100000, batch:   672/  840, ite: 896] train loss: 2.329402, tar: 0.310678 \n",
            "l0: 0.264409, l1: 0.260967, l2: 0.264440, l3: 0.291288, l4: 0.298218, l5: 0.317760, l6: 0.350964\n",
            "\n",
            "[epoch:  13/100000, batch:   684/  840, ite: 897] train loss: 2.329088, tar: 0.310627 \n",
            "l0: 0.220053, l1: 0.210906, l2: 0.228216, l3: 0.237119, l4: 0.248120, l5: 0.257460, l6: 0.272470\n",
            "\n",
            "[epoch:  13/100000, batch:   696/  840, ite: 898] train loss: 2.328359, tar: 0.310526 \n",
            "l0: 0.245197, l1: 0.242909, l2: 0.243434, l3: 0.248988, l4: 0.258478, l5: 0.292413, l6: 0.347411\n",
            "\n",
            "[epoch:  13/100000, batch:   708/  840, ite: 899] train loss: 2.327859, tar: 0.310453 \n",
            "l0: 0.251036, l1: 0.253098, l2: 0.252368, l3: 0.270855, l4: 0.276916, l5: 0.288002, l6: 0.323848\n",
            "\n",
            "[epoch:  13/100000, batch:   720/  840, ite: 900] train loss: 2.327402, tar: 0.310387 \n",
            "l0: 0.164259, l1: 0.168144, l2: 0.164348, l3: 0.173995, l4: 0.180895, l5: 0.190896, l6: 0.244085\n",
            "\n",
            "[epoch:  13/100000, batch:   732/  840, ite: 901] train loss: 2.326247, tar: 0.310225 \n",
            "l0: 0.314624, l1: 0.324044, l2: 0.329945, l3: 0.326792, l4: 0.328541, l5: 0.348426, l6: 0.369584\n",
            "\n",
            "[epoch:  13/100000, batch:   744/  840, ite: 902] train loss: 2.326264, tar: 0.310230 \n",
            "l0: 0.265967, l1: 0.255333, l2: 0.276135, l3: 0.291572, l4: 0.298627, l5: 0.308411, l6: 0.316389\n",
            "\n",
            "[epoch:  13/100000, batch:   756/  840, ite: 903] train loss: 2.325916, tar: 0.310181 \n",
            "l0: 0.176229, l1: 0.192423, l2: 0.184729, l3: 0.193325, l4: 0.202080, l5: 0.198435, l6: 0.226382\n",
            "\n",
            "[epoch:  13/100000, batch:   768/  840, ite: 904] train loss: 2.324863, tar: 0.310033 \n",
            "l0: 0.271038, l1: 0.288566, l2: 0.278294, l3: 0.285890, l4: 0.283705, l5: 0.289649, l6: 0.326713\n",
            "\n",
            "[epoch:  13/100000, batch:   780/  840, ite: 905] train loss: 2.324530, tar: 0.309990 \n",
            "l0: 0.247181, l1: 0.250717, l2: 0.245078, l3: 0.263655, l4: 0.275196, l5: 0.305523, l6: 0.343490\n",
            "\n",
            "[epoch:  13/100000, batch:   792/  840, ite: 906] train loss: 2.324096, tar: 0.309920 \n",
            "l0: 0.229913, l1: 0.240272, l2: 0.228795, l3: 0.247221, l4: 0.252239, l5: 0.264437, l6: 0.305931\n",
            "\n",
            "[epoch:  13/100000, batch:   804/  840, ite: 907] train loss: 2.323484, tar: 0.309832 \n",
            "l0: 0.187520, l1: 0.181504, l2: 0.199743, l3: 0.204482, l4: 0.188386, l5: 0.211952, l6: 0.266490\n",
            "\n",
            "[epoch:  13/100000, batch:   816/  840, ite: 908] train loss: 2.322511, tar: 0.309698 \n",
            "l0: 0.154805, l1: 0.151067, l2: 0.166191, l3: 0.169671, l4: 0.170921, l5: 0.179013, l6: 0.220169\n",
            "\n",
            "[epoch:  13/100000, batch:   828/  840, ite: 909] train loss: 2.321289, tar: 0.309527 \n",
            "l0: 0.238988, l1: 0.243628, l2: 0.247295, l3: 0.262138, l4: 0.256309, l5: 0.274146, l6: 0.312396\n",
            "\n",
            "[epoch:  13/100000, batch:   840/  840, ite: 910] train loss: 2.320755, tar: 0.309450 \n",
            "l0: 0.215843, l1: 0.210192, l2: 0.228892, l3: 0.235354, l4: 0.228117, l5: 0.231521, l6: 0.289439\n",
            "\n",
            "[epoch:  14/100000, batch:    12/  840, ite: 911] train loss: 2.320007, tar: 0.309347 \n",
            "l0: 0.322499, l1: 0.321600, l2: 0.322824, l3: 0.337924, l4: 0.335109, l5: 0.379265, l6: 0.400017\n",
            "\n",
            "[epoch:  14/100000, batch:    24/  840, ite: 912] train loss: 2.320116, tar: 0.309361 \n",
            "l0: 0.280195, l1: 0.274291, l2: 0.297495, l3: 0.281144, l4: 0.274935, l5: 0.315821, l6: 0.340560\n",
            "\n",
            "[epoch:  14/100000, batch:    36/  840, ite: 913] train loss: 2.319835, tar: 0.309329 \n",
            "l0: 0.195079, l1: 0.199822, l2: 0.186549, l3: 0.207536, l4: 0.223808, l5: 0.255859, l6: 0.317086\n",
            "\n",
            "[epoch:  14/100000, batch:    48/  840, ite: 914] train loss: 2.319032, tar: 0.309204 \n",
            "l0: 0.208090, l1: 0.217484, l2: 0.215166, l3: 0.201684, l4: 0.206864, l5: 0.238544, l6: 0.282448\n",
            "\n",
            "[epoch:  14/100000, batch:    60/  840, ite: 915] train loss: 2.318214, tar: 0.309094 \n",
            "l0: 0.250786, l1: 0.239379, l2: 0.253061, l3: 0.254736, l4: 0.269812, l5: 0.291412, l6: 0.338981\n",
            "\n",
            "[epoch:  14/100000, batch:    72/  840, ite: 916] train loss: 2.317755, tar: 0.309030 \n",
            "l0: 0.199658, l1: 0.183396, l2: 0.207952, l3: 0.226100, l4: 0.233072, l5: 0.256572, l6: 0.287981\n",
            "\n",
            "[epoch:  14/100000, batch:    84/  840, ite: 917] train loss: 2.316967, tar: 0.308911 \n",
            "l0: 0.246616, l1: 0.239957, l2: 0.247542, l3: 0.259191, l4: 0.267929, l5: 0.271551, l6: 0.316972\n",
            "\n",
            "[epoch:  14/100000, batch:    96/  840, ite: 918] train loss: 2.316458, tar: 0.308843 \n",
            "l0: 0.225231, l1: 0.238293, l2: 0.239619, l3: 0.220853, l4: 0.236815, l5: 0.253451, l6: 0.302532\n",
            "\n",
            "[epoch:  14/100000, batch:   108/  840, ite: 919] train loss: 2.315805, tar: 0.308752 \n",
            "l0: 0.204453, l1: 0.206164, l2: 0.213680, l3: 0.203793, l4: 0.213910, l5: 0.216526, l6: 0.256989\n",
            "\n",
            "[epoch:  14/100000, batch:   120/  840, ite: 920] train loss: 2.314936, tar: 0.308639 \n",
            "l0: 0.144866, l1: 0.138200, l2: 0.163541, l3: 0.147764, l4: 0.158496, l5: 0.174325, l6: 0.204030\n",
            "\n",
            "[epoch:  14/100000, batch:   132/  840, ite: 921] train loss: 2.313651, tar: 0.308461 \n",
            "l0: 0.172843, l1: 0.172254, l2: 0.183245, l3: 0.175526, l4: 0.180466, l5: 0.201839, l6: 0.255456\n",
            "\n",
            "[epoch:  14/100000, batch:   144/  840, ite: 922] train loss: 2.312596, tar: 0.308314 \n",
            "l0: 0.221523, l1: 0.240091, l2: 0.220678, l3: 0.236018, l4: 0.244101, l5: 0.262943, l6: 0.296594\n",
            "\n",
            "[epoch:  14/100000, batch:   156/  840, ite: 923] train loss: 2.311956, tar: 0.308220 \n",
            "l0: 0.389777, l1: 0.376364, l2: 0.398515, l3: 0.459181, l4: 0.451398, l5: 0.419443, l6: 0.442166\n",
            "\n",
            "[epoch:  14/100000, batch:   168/  840, ite: 924] train loss: 2.312632, tar: 0.308308 \n",
            "l0: 0.178266, l1: 0.171406, l2: 0.192375, l3: 0.191043, l4: 0.201094, l5: 0.207027, l6: 0.233451\n",
            "\n",
            "[epoch:  14/100000, batch:   180/  840, ite: 925] train loss: 2.311619, tar: 0.308167 \n",
            "l0: 0.505876, l1: 0.467707, l2: 0.520619, l3: 0.559706, l4: 0.570298, l5: 0.528848, l6: 0.542995\n",
            "\n",
            "[epoch:  14/100000, batch:   192/  840, ite: 926] train loss: 2.313113, tar: 0.308381 \n",
            "l0: 0.206734, l1: 0.194707, l2: 0.218091, l3: 0.224411, l4: 0.230938, l5: 0.258207, l6: 0.275427\n",
            "\n",
            "[epoch:  14/100000, batch:   204/  840, ite: 927] train loss: 2.312353, tar: 0.308271 \n",
            "l0: 0.285777, l1: 0.286894, l2: 0.297969, l3: 0.327274, l4: 0.315737, l5: 0.321932, l6: 0.344874\n",
            "\n",
            "[epoch:  14/100000, batch:   216/  840, ite: 928] train loss: 2.312211, tar: 0.308247 \n",
            "l0: 0.344078, l1: 0.358393, l2: 0.368866, l3: 0.386447, l4: 0.361298, l5: 0.359914, l6: 0.376265\n",
            "\n",
            "[epoch:  14/100000, batch:   228/  840, ite: 929] train loss: 2.312473, tar: 0.308286 \n",
            "l0: 0.229395, l1: 0.215519, l2: 0.249219, l3: 0.244364, l4: 0.254882, l5: 0.275646, l6: 0.321393\n",
            "\n",
            "[epoch:  14/100000, batch:   240/  840, ite: 930] train loss: 2.311912, tar: 0.308201 \n",
            "l0: 0.248144, l1: 0.221158, l2: 0.244662, l3: 0.256548, l4: 0.276772, l5: 0.299676, l6: 0.365476\n",
            "\n",
            "[epoch:  14/100000, batch:   252/  840, ite: 931] train loss: 2.311482, tar: 0.308136 \n",
            "l0: 0.254367, l1: 0.269012, l2: 0.281812, l3: 0.286824, l4: 0.275439, l5: 0.271136, l6: 0.298659\n",
            "\n",
            "[epoch:  14/100000, batch:   264/  840, ite: 932] train loss: 2.311081, tar: 0.308079 \n",
            "l0: 0.159005, l1: 0.180919, l2: 0.175479, l3: 0.179305, l4: 0.168965, l5: 0.166710, l6: 0.186056\n",
            "\n",
            "[epoch:  14/100000, batch:   276/  840, ite: 933] train loss: 2.309907, tar: 0.307919 \n",
            "l0: 0.230161, l1: 0.217749, l2: 0.239729, l3: 0.257793, l4: 0.259334, l5: 0.268998, l6: 0.298400\n",
            "\n",
            "[epoch:  14/100000, batch:   288/  840, ite: 934] train loss: 2.309332, tar: 0.307836 \n",
            "l0: 0.256928, l1: 0.254158, l2: 0.263587, l3: 0.304301, l4: 0.302727, l5: 0.286577, l6: 0.296788\n",
            "\n",
            "[epoch:  14/100000, batch:   300/  840, ite: 935] train loss: 2.308964, tar: 0.307781 \n",
            "l0: 0.217589, l1: 0.213879, l2: 0.228954, l3: 0.255870, l4: 0.257707, l5: 0.242525, l6: 0.260032\n",
            "\n",
            "[epoch:  14/100000, batch:   312/  840, ite: 936] train loss: 2.308288, tar: 0.307685 \n",
            "l0: 0.226866, l1: 0.248899, l2: 0.219136, l3: 0.230709, l4: 0.232843, l5: 0.259482, l6: 0.314142\n",
            "\n",
            "[epoch:  14/100000, batch:   324/  840, ite: 937] train loss: 2.307673, tar: 0.307599 \n",
            "l0: 0.195852, l1: 0.215142, l2: 0.204709, l3: 0.211947, l4: 0.209667, l5: 0.200961, l6: 0.233165\n",
            "\n",
            "[epoch:  14/100000, batch:   336/  840, ite: 938] train loss: 2.306782, tar: 0.307479 \n",
            "l0: 0.227347, l1: 0.240540, l2: 0.221706, l3: 0.258262, l4: 0.255847, l5: 0.265912, l6: 0.311047\n",
            "\n",
            "[epoch:  14/100000, batch:   348/  840, ite: 939] train loss: 2.306221, tar: 0.307394 \n",
            "l0: 0.222882, l1: 0.219575, l2: 0.218710, l3: 0.259369, l4: 0.259709, l5: 0.280328, l6: 0.298868\n",
            "\n",
            "[epoch:  14/100000, batch:   360/  840, ite: 940] train loss: 2.305640, tar: 0.307304 \n",
            "l0: 0.234233, l1: 0.220631, l2: 0.238549, l3: 0.279891, l4: 0.285457, l5: 0.303843, l6: 0.304397\n",
            "\n",
            "[epoch:  14/100000, batch:   372/  840, ite: 941] train loss: 2.305174, tar: 0.307226 \n",
            "l0: 0.345814, l1: 0.354011, l2: 0.341792, l3: 0.366702, l4: 0.358607, l5: 0.376971, l6: 0.424482\n",
            "\n",
            "[epoch:  14/100000, batch:   384/  840, ite: 942] train loss: 2.305453, tar: 0.307267 \n",
            "l0: 0.294159, l1: 0.293514, l2: 0.289674, l3: 0.307476, l4: 0.318771, l5: 0.336173, l6: 0.367572\n",
            "\n",
            "[epoch:  14/100000, batch:   396/  840, ite: 943] train loss: 2.305349, tar: 0.307254 \n",
            "l0: 0.310902, l1: 0.306024, l2: 0.311341, l3: 0.307728, l4: 0.302960, l5: 0.337037, l6: 0.414758\n",
            "\n",
            "[epoch:  14/100000, batch:   408/  840, ite: 944] train loss: 2.305334, tar: 0.307257 \n",
            "l0: 0.368597, l1: 0.368412, l2: 0.372268, l3: 0.392065, l4: 0.368971, l5: 0.407146, l6: 0.431292\n",
            "\n",
            "[epoch:  14/100000, batch:   420/  840, ite: 945] train loss: 2.305760, tar: 0.307322 \n",
            "l0: 0.286798, l1: 0.300675, l2: 0.288301, l3: 0.327459, l4: 0.316588, l5: 0.331601, l6: 0.355012\n",
            "\n",
            "[epoch:  14/100000, batch:   432/  840, ite: 946] train loss: 2.305655, tar: 0.307301 \n",
            "l0: 0.215595, l1: 0.195842, l2: 0.228376, l3: 0.226378, l4: 0.235693, l5: 0.252265, l6: 0.305210\n",
            "\n",
            "[epoch:  14/100000, batch:   444/  840, ite: 947] train loss: 2.304973, tar: 0.307204 \n",
            "l0: 0.219608, l1: 0.214489, l2: 0.223106, l3: 0.224099, l4: 0.234275, l5: 0.273782, l6: 0.364881\n",
            "\n",
            "[epoch:  14/100000, batch:   456/  840, ite: 948] train loss: 2.304392, tar: 0.307111 \n",
            "l0: 0.217088, l1: 0.215871, l2: 0.209189, l3: 0.229108, l4: 0.233784, l5: 0.268719, l6: 0.370762\n",
            "\n",
            "[epoch:  14/100000, batch:   468/  840, ite: 949] train loss: 2.303802, tar: 0.307017 \n",
            "l0: 0.234767, l1: 0.229592, l2: 0.228244, l3: 0.259639, l4: 0.272115, l5: 0.304819, l6: 0.363006\n",
            "\n",
            "[epoch:  14/100000, batch:   480/  840, ite: 950] train loss: 2.303369, tar: 0.306941 \n",
            "l0: 0.224156, l1: 0.221307, l2: 0.228682, l3: 0.243543, l4: 0.253171, l5: 0.261412, l6: 0.285698\n",
            "\n",
            "[epoch:  14/100000, batch:   492/  840, ite: 951] train loss: 2.302753, tar: 0.306853 \n",
            "l0: 0.256852, l1: 0.264785, l2: 0.260503, l3: 0.274352, l4: 0.281480, l5: 0.290470, l6: 0.305675\n",
            "\n",
            "[epoch:  14/100000, batch:   504/  840, ite: 952] train loss: 2.302366, tar: 0.306801 \n",
            "l0: 0.196828, l1: 0.205658, l2: 0.196763, l3: 0.224959, l4: 0.230069, l5: 0.243024, l6: 0.280677\n",
            "\n",
            "[epoch:  14/100000, batch:   516/  840, ite: 953] train loss: 2.301606, tar: 0.306686 \n",
            "l0: 0.077435, l1: 0.091943, l2: 0.081720, l3: 0.094887, l4: 0.100804, l5: 0.101375, l6: 0.148720\n",
            "\n",
            "[epoch:  14/100000, batch:   528/  840, ite: 954] train loss: 2.299924, tar: 0.306445 \n",
            "l0: 0.147150, l1: 0.160119, l2: 0.146051, l3: 0.162191, l4: 0.163859, l5: 0.184119, l6: 0.240732\n",
            "\n",
            "[epoch:  14/100000, batch:   540/  840, ite: 955] train loss: 2.298776, tar: 0.306278 \n",
            "l0: 0.296462, l1: 0.317264, l2: 0.297367, l3: 0.343336, l4: 0.346775, l5: 0.353812, l6: 0.383205\n",
            "\n",
            "[epoch:  14/100000, batch:   552/  840, ite: 956] train loss: 2.298817, tar: 0.306268 \n",
            "l0: 0.246652, l1: 0.243026, l2: 0.243026, l3: 0.263370, l4: 0.272447, l5: 0.320228, l6: 0.376510\n",
            "\n",
            "[epoch:  14/100000, batch:   564/  840, ite: 957] train loss: 2.298469, tar: 0.306206 \n",
            "l0: 0.228264, l1: 0.231522, l2: 0.225782, l3: 0.223915, l4: 0.234500, l5: 0.276629, l6: 0.315928\n",
            "\n",
            "[epoch:  14/100000, batch:   576/  840, ite: 958] train loss: 2.297883, tar: 0.306125 \n",
            "l0: 0.273530, l1: 0.262898, l2: 0.277714, l3: 0.306101, l4: 0.312218, l5: 0.338843, l6: 0.366024\n",
            "\n",
            "[epoch:  14/100000, batch:   588/  840, ite: 959] train loss: 2.297715, tar: 0.306091 \n",
            "l0: 0.210050, l1: 0.215493, l2: 0.208862, l3: 0.259851, l4: 0.257492, l5: 0.281093, l6: 0.297932\n",
            "\n",
            "[epoch:  14/100000, batch:   600/  840, ite: 960] train loss: 2.297124, tar: 0.305991 \n",
            "l0: 0.324679, l1: 0.311885, l2: 0.329590, l3: 0.374292, l4: 0.375548, l5: 0.387538, l6: 0.457152\n",
            "\n",
            "[epoch:  14/100000, batch:   612/  840, ite: 961] train loss: 2.297399, tar: 0.306010 \n",
            "l0: 0.207254, l1: 0.219500, l2: 0.211476, l3: 0.207329, l4: 0.207792, l5: 0.251328, l6: 0.342470\n",
            "\n",
            "[epoch:  14/100000, batch:   624/  840, ite: 962] train loss: 2.296723, tar: 0.305907 \n",
            "l0: 0.334918, l1: 0.340542, l2: 0.343547, l3: 0.327795, l4: 0.328485, l5: 0.376170, l6: 0.461589\n",
            "\n",
            "[epoch:  14/100000, batch:   636/  840, ite: 963] train loss: 2.296947, tar: 0.305937 \n",
            "l0: 0.230448, l1: 0.225780, l2: 0.249602, l3: 0.228088, l4: 0.231037, l5: 0.230825, l6: 0.289904\n",
            "\n",
            "[epoch:  14/100000, batch:   648/  840, ite: 964] train loss: 2.296313, tar: 0.305859 \n",
            "l0: 0.266488, l1: 0.270437, l2: 0.274653, l3: 0.289203, l4: 0.292167, l5: 0.289357, l6: 0.318826\n",
            "\n",
            "[epoch:  14/100000, batch:   660/  840, ite: 965] train loss: 2.296008, tar: 0.305818 \n",
            "l0: 0.197850, l1: 0.195954, l2: 0.197388, l3: 0.249628, l4: 0.244088, l5: 0.247801, l6: 0.329052\n",
            "\n",
            "[epoch:  14/100000, batch:   672/  840, ite: 966] train loss: 2.295351, tar: 0.305707 \n",
            "l0: 0.148507, l1: 0.156223, l2: 0.159944, l3: 0.171790, l4: 0.164126, l5: 0.175010, l6: 0.205784\n",
            "\n",
            "[epoch:  14/100000, batch:   684/  840, ite: 967] train loss: 2.294199, tar: 0.305544 \n",
            "l0: 0.253100, l1: 0.263995, l2: 0.245560, l3: 0.274008, l4: 0.283468, l5: 0.312838, l6: 0.337500\n",
            "\n",
            "[epoch:  14/100000, batch:   696/  840, ite: 968] train loss: 2.293864, tar: 0.305490 \n",
            "l0: 0.302981, l1: 0.330248, l2: 0.342051, l3: 0.357602, l4: 0.331007, l5: 0.335456, l6: 0.295429\n",
            "\n",
            "[epoch:  14/100000, batch:   708/  840, ite: 969] train loss: 2.293865, tar: 0.305487 \n",
            "l0: 0.254225, l1: 0.264233, l2: 0.250816, l3: 0.289347, l4: 0.293114, l5: 0.321353, l6: 0.368084\n",
            "\n",
            "[epoch:  14/100000, batch:   720/  840, ite: 970] train loss: 2.293605, tar: 0.305434 \n",
            "l0: 0.236932, l1: 0.233919, l2: 0.246014, l3: 0.272051, l4: 0.268058, l5: 0.274942, l6: 0.311297\n",
            "\n",
            "[epoch:  14/100000, batch:   732/  840, ite: 971] train loss: 2.293141, tar: 0.305364 \n",
            "l0: 0.194393, l1: 0.195277, l2: 0.193148, l3: 0.237848, l4: 0.239906, l5: 0.247863, l6: 0.279010\n",
            "\n",
            "[epoch:  14/100000, batch:   744/  840, ite: 972] train loss: 2.292415, tar: 0.305250 \n",
            "l0: 0.229021, l1: 0.235783, l2: 0.243504, l3: 0.240475, l4: 0.241292, l5: 0.238833, l6: 0.264729\n",
            "\n",
            "[epoch:  14/100000, batch:   756/  840, ite: 973] train loss: 2.291800, tar: 0.305171 \n",
            "l0: 0.311604, l1: 0.320642, l2: 0.337332, l3: 0.314803, l4: 0.321330, l5: 0.324739, l6: 0.355704\n",
            "\n",
            "[epoch:  14/100000, batch:   768/  840, ite: 974] train loss: 2.291794, tar: 0.305178 \n",
            "l0: 0.328572, l1: 0.329002, l2: 0.337831, l3: 0.357679, l4: 0.365805, l5: 0.366514, l6: 0.382881\n",
            "\n",
            "[epoch:  14/100000, batch:   780/  840, ite: 975] train loss: 2.291975, tar: 0.305202 \n",
            "l0: 0.316766, l1: 0.317363, l2: 0.316501, l3: 0.358961, l4: 0.363813, l5: 0.362325, l6: 0.373573\n",
            "\n",
            "[epoch:  14/100000, batch:   792/  840, ite: 976] train loss: 2.292095, tar: 0.305214 \n",
            "l0: 0.274236, l1: 0.268947, l2: 0.299239, l3: 0.315040, l4: 0.299551, l5: 0.283895, l6: 0.311276\n",
            "\n",
            "[epoch:  14/100000, batch:   804/  840, ite: 977] train loss: 2.291849, tar: 0.305182 \n",
            "l0: 0.196995, l1: 0.196684, l2: 0.205005, l3: 0.234900, l4: 0.238520, l5: 0.247902, l6: 0.294273\n",
            "\n",
            "[epoch:  14/100000, batch:   816/  840, ite: 978] train loss: 2.291157, tar: 0.305071 \n",
            "l0: 0.204406, l1: 0.201967, l2: 0.208059, l3: 0.215909, l4: 0.231201, l5: 0.244905, l6: 0.287877\n",
            "\n",
            "[epoch:  14/100000, batch:   828/  840, ite: 979] train loss: 2.290445, tar: 0.304969 \n",
            "l0: 0.162099, l1: 0.158184, l2: 0.177467, l3: 0.179330, l4: 0.178316, l5: 0.190695, l6: 0.227459\n",
            "\n",
            "[epoch:  14/100000, batch:   840/  840, ite: 980] train loss: 2.289407, tar: 0.304823 \n",
            "l0: 0.272105, l1: 0.290228, l2: 0.275075, l3: 0.287375, l4: 0.283539, l5: 0.287641, l6: 0.313349\n",
            "\n",
            "[epoch:  15/100000, batch:    12/  840, ite: 981] train loss: 2.289121, tar: 0.304789 \n",
            "l0: 0.274890, l1: 0.264489, l2: 0.262602, l3: 0.306899, l4: 0.314836, l5: 0.316413, l6: 0.364388\n",
            "\n",
            "[epoch:  15/100000, batch:    24/  840, ite: 982] train loss: 2.288934, tar: 0.304759 \n",
            "l0: 0.214871, l1: 0.214548, l2: 0.216222, l3: 0.235484, l4: 0.234081, l5: 0.261410, l6: 0.290856\n",
            "\n",
            "[epoch:  15/100000, batch:    36/  840, ite: 983] train loss: 2.288301, tar: 0.304668 \n",
            "l0: 0.164404, l1: 0.170034, l2: 0.164566, l3: 0.174870, l4: 0.178185, l5: 0.194879, l6: 0.235010\n",
            "\n",
            "[epoch:  15/100000, batch:    48/  840, ite: 984] train loss: 2.287278, tar: 0.304525 \n",
            "l0: 0.314521, l1: 0.328212, l2: 0.349294, l3: 0.385135, l4: 0.364682, l5: 0.345509, l6: 0.355032\n",
            "\n",
            "[epoch:  15/100000, batch:    60/  840, ite: 985] train loss: 2.287436, tar: 0.304535 \n",
            "l0: 0.312123, l1: 0.328652, l2: 0.315142, l3: 0.346751, l4: 0.340138, l5: 0.351737, l6: 0.404438\n",
            "\n",
            "[epoch:  15/100000, batch:    72/  840, ite: 986] train loss: 2.287549, tar: 0.304543 \n",
            "l0: 0.213867, l1: 0.207633, l2: 0.197540, l3: 0.210832, l4: 0.224877, l5: 0.294178, l6: 0.376562\n",
            "\n",
            "[epoch:  15/100000, batch:    84/  840, ite: 987] train loss: 2.286980, tar: 0.304451 \n",
            "l0: 0.196937, l1: 0.208520, l2: 0.196706, l3: 0.215970, l4: 0.214551, l5: 0.244843, l6: 0.292941\n",
            "\n",
            "[epoch:  15/100000, batch:    96/  840, ite: 988] train loss: 2.286255, tar: 0.304342 \n",
            "l0: 0.241774, l1: 0.237659, l2: 0.248038, l3: 0.271242, l4: 0.285984, l5: 0.286388, l6: 0.309818\n",
            "\n",
            "[epoch:  15/100000, batch:   108/  840, ite: 989] train loss: 2.285845, tar: 0.304279 \n",
            "l0: 0.111034, l1: 0.106451, l2: 0.114284, l3: 0.124535, l4: 0.125956, l5: 0.139154, l6: 0.184180\n",
            "\n",
            "[epoch:  15/100000, batch:   120/  840, ite: 990] train loss: 2.284451, tar: 0.304084 \n",
            "l0: 0.254441, l1: 0.227673, l2: 0.275624, l3: 0.302157, l4: 0.290608, l5: 0.305935, l6: 0.358110\n",
            "\n",
            "[epoch:  15/100000, batch:   132/  840, ite: 991] train loss: 2.284178, tar: 0.304034 \n",
            "l0: 0.280091, l1: 0.273840, l2: 0.286130, l3: 0.316980, l4: 0.314710, l5: 0.332059, l6: 0.376987\n",
            "\n",
            "[epoch:  15/100000, batch:   144/  840, ite: 992] train loss: 2.284074, tar: 0.304009 \n",
            "l0: 0.166372, l1: 0.173497, l2: 0.166906, l3: 0.185549, l4: 0.190314, l5: 0.203461, l6: 0.239037\n",
            "\n",
            "[epoch:  15/100000, batch:   156/  840, ite: 993] train loss: 2.283108, tar: 0.303871 \n",
            "l0: 0.201455, l1: 0.194581, l2: 0.233738, l3: 0.207867, l4: 0.202022, l5: 0.204753, l6: 0.242269\n",
            "\n",
            "[epoch:  15/100000, batch:   168/  840, ite: 994] train loss: 2.282307, tar: 0.303768 \n",
            "l0: 0.298471, l1: 0.292964, l2: 0.310531, l3: 0.319008, l4: 0.317043, l5: 0.325187, l6: 0.345980\n",
            "\n",
            "[epoch:  15/100000, batch:   180/  840, ite: 995] train loss: 2.282234, tar: 0.303763 \n",
            "l0: 0.465255, l1: 0.496045, l2: 0.475635, l3: 0.475644, l4: 0.469910, l5: 0.495496, l6: 0.498061\n",
            "\n",
            "[epoch:  15/100000, batch:   192/  840, ite: 996] train loss: 2.283332, tar: 0.303925 \n",
            "l0: 0.209271, l1: 0.224538, l2: 0.220523, l3: 0.214728, l4: 0.210441, l5: 0.217683, l6: 0.251025\n",
            "\n",
            "[epoch:  15/100000, batch:   204/  840, ite: 997] train loss: 2.282594, tar: 0.303830 \n",
            "l0: 0.239201, l1: 0.231952, l2: 0.241845, l3: 0.273374, l4: 0.276360, l5: 0.297276, l6: 0.307226\n",
            "\n",
            "[epoch:  15/100000, batch:   216/  840, ite: 998] train loss: 2.282178, tar: 0.303765 \n",
            "l0: 0.391611, l1: 0.391935, l2: 0.407516, l3: 0.399068, l4: 0.401512, l5: 0.435440, l6: 0.426126\n",
            "\n",
            "[epoch:  15/100000, batch:   228/  840, ite: 999] train loss: 2.282750, tar: 0.303853 \n",
            "l0: 0.384741, l1: 0.400719, l2: 0.362523, l3: 0.373694, l4: 0.380328, l5: 0.395595, l6: 0.487586\n",
            "\n",
            "[epoch:  15/100000, batch:   240/  840, ite: 1000] train loss: 2.283252, tar: 0.303934 \n",
            "l0: 0.206576, l1: 0.195855, l2: 0.223029, l3: 0.231098, l4: 0.235746, l5: 0.244843, l6: 0.271425\n",
            "\n",
            "[epoch:  15/100000, batch:   252/  840, ite: 1001] train loss: 2.282578, tar: 0.303836 \n",
            "l0: 0.301587, l1: 0.291508, l2: 0.332147, l3: 0.325076, l4: 0.327200, l5: 0.320229, l6: 0.326168\n",
            "\n",
            "[epoch:  15/100000, batch:   264/  840, ite: 1002] train loss: 2.282520, tar: 0.303834 \n",
            "l0: 0.138088, l1: 0.139353, l2: 0.153593, l3: 0.164785, l4: 0.159430, l5: 0.176723, l6: 0.191264\n",
            "\n",
            "[epoch:  15/100000, batch:   276/  840, ite: 1003] train loss: 2.281364, tar: 0.303669 \n",
            "l0: 0.142148, l1: 0.147178, l2: 0.151115, l3: 0.151118, l4: 0.152056, l5: 0.159298, l6: 0.192680\n",
            "\n",
            "[epoch:  15/100000, batch:   288/  840, ite: 1004] train loss: 2.280183, tar: 0.303508 \n",
            "l0: 0.301249, l1: 0.305712, l2: 0.314216, l3: 0.348349, l4: 0.344484, l5: 0.350099, l6: 0.364786\n",
            "\n",
            "[epoch:  15/100000, batch:   300/  840, ite: 1005] train loss: 2.280231, tar: 0.303506 \n",
            "l0: 0.323440, l1: 0.320371, l2: 0.331834, l3: 0.330170, l4: 0.334792, l5: 0.333579, l6: 0.381980\n",
            "\n",
            "[epoch:  15/100000, batch:   312/  840, ite: 1006] train loss: 2.280307, tar: 0.303526 \n",
            "l0: 0.246533, l1: 0.256791, l2: 0.267847, l3: 0.258815, l4: 0.256710, l5: 0.274760, l6: 0.285745\n",
            "\n",
            "[epoch:  15/100000, batch:   324/  840, ite: 1007] train loss: 2.279877, tar: 0.303469 \n",
            "l0: 0.227054, l1: 0.215518, l2: 0.231184, l3: 0.244800, l4: 0.263438, l5: 0.278377, l6: 0.285873\n",
            "\n",
            "[epoch:  15/100000, batch:   336/  840, ite: 1008] train loss: 2.279348, tar: 0.303393 \n",
            "l0: 0.164089, l1: 0.169430, l2: 0.171882, l3: 0.182352, l4: 0.186150, l5: 0.193312, l6: 0.214375\n",
            "\n",
            "[epoch:  15/100000, batch:   348/  840, ite: 1009] train loss: 2.278359, tar: 0.303255 \n",
            "l0: 0.174819, l1: 0.169515, l2: 0.182704, l3: 0.189264, l4: 0.200051, l5: 0.209127, l6: 0.247380\n",
            "\n",
            "[epoch:  15/100000, batch:   360/  840, ite: 1010] train loss: 2.277462, tar: 0.303128 \n",
            "l0: 0.509312, l1: 0.464748, l2: 0.560278, l3: 0.588153, l4: 0.610260, l5: 0.601260, l6: 0.567944\n",
            "\n",
            "[epoch:  15/100000, batch:   372/  840, ite: 1011] train loss: 2.279068, tar: 0.303332 \n",
            "l0: 0.236943, l1: 0.223175, l2: 0.249308, l3: 0.270276, l4: 0.272505, l5: 0.296436, l6: 0.329822\n",
            "\n",
            "[epoch:  15/100000, batch:   384/  840, ite: 1012] train loss: 2.278673, tar: 0.303266 \n",
            "l0: 0.218999, l1: 0.226326, l2: 0.238102, l3: 0.246545, l4: 0.250896, l5: 0.261492, l6: 0.295308\n",
            "\n",
            "[epoch:  15/100000, batch:   396/  840, ite: 1013] train loss: 2.278139, tar: 0.303183 \n",
            "l0: 0.180892, l1: 0.180479, l2: 0.190637, l3: 0.216609, l4: 0.212119, l5: 0.239256, l6: 0.307669\n",
            "\n",
            "[epoch:  15/100000, batch:   408/  840, ite: 1014] train loss: 2.277398, tar: 0.303063 \n",
            "l0: 0.232720, l1: 0.239219, l2: 0.251422, l3: 0.240626, l4: 0.242738, l5: 0.253847, l6: 0.276364\n",
            "\n",
            "[epoch:  15/100000, batch:   420/  840, ite: 1015] train loss: 2.276866, tar: 0.302993 \n",
            "l0: 0.297683, l1: 0.307342, l2: 0.308859, l3: 0.297838, l4: 0.312267, l5: 0.324954, l6: 0.337977\n",
            "\n",
            "[epoch:  15/100000, batch:   432/  840, ite: 1016] train loss: 2.276777, tar: 0.302988 \n",
            "l0: 0.184406, l1: 0.180178, l2: 0.210197, l3: 0.196347, l4: 0.213464, l5: 0.205887, l6: 0.226264\n",
            "\n",
            "[epoch:  15/100000, batch:   444/  840, ite: 1017] train loss: 2.275932, tar: 0.302871 \n",
            "l0: 0.163557, l1: 0.156885, l2: 0.172362, l3: 0.190667, l4: 0.204425, l5: 0.202624, l6: 0.229066\n",
            "\n",
            "[epoch:  15/100000, batch:   456/  840, ite: 1018] train loss: 2.274992, tar: 0.302735 \n",
            "l0: 0.196699, l1: 0.193780, l2: 0.205790, l3: 0.232602, l4: 0.242587, l5: 0.228149, l6: 0.253413\n",
            "\n",
            "[epoch:  15/100000, batch:   468/  840, ite: 1019] train loss: 2.274284, tar: 0.302630 \n",
            "l0: 0.274531, l1: 0.274144, l2: 0.295766, l3: 0.294359, l4: 0.303209, l5: 0.319402, l6: 0.370646\n",
            "\n",
            "[epoch:  15/100000, batch:   480/  840, ite: 1020] train loss: 2.274144, tar: 0.302603 \n",
            "l0: 0.242398, l1: 0.244426, l2: 0.242237, l3: 0.289009, l4: 0.296537, l5: 0.284244, l6: 0.320285\n",
            "\n",
            "[epoch:  15/100000, batch:   492/  840, ite: 1021] train loss: 2.273797, tar: 0.302544 \n",
            "l0: 0.181345, l1: 0.179312, l2: 0.180738, l3: 0.203203, l4: 0.211760, l5: 0.238044, l6: 0.268832\n",
            "\n",
            "[epoch:  15/100000, batch:   504/  840, ite: 1022] train loss: 2.273003, tar: 0.302425 \n",
            "l0: 0.262595, l1: 0.256745, l2: 0.266874, l3: 0.291193, l4: 0.291050, l5: 0.332971, l6: 0.363914\n",
            "\n",
            "[epoch:  15/100000, batch:   516/  840, ite: 1023] train loss: 2.272800, tar: 0.302386 \n",
            "l0: 0.234151, l1: 0.225434, l2: 0.252007, l3: 0.257028, l4: 0.253264, l5: 0.257722, l6: 0.281025\n",
            "\n",
            "[epoch:  15/100000, batch:   528/  840, ite: 1024] train loss: 2.272300, tar: 0.302320 \n",
            "l0: 0.120158, l1: 0.124951, l2: 0.122402, l3: 0.132764, l4: 0.134973, l5: 0.157594, l6: 0.200022\n",
            "\n",
            "[epoch:  15/100000, batch:   540/  840, ite: 1025] train loss: 2.271052, tar: 0.302142 \n",
            "l0: 0.189096, l1: 0.190923, l2: 0.203669, l3: 0.199857, l4: 0.205713, l5: 0.218065, l6: 0.249605\n",
            "\n",
            "[epoch:  15/100000, batch:   552/  840, ite: 1026] train loss: 2.270259, tar: 0.302032 \n",
            "l0: 0.166476, l1: 0.168939, l2: 0.175502, l3: 0.166348, l4: 0.173881, l5: 0.191227, l6: 0.229585\n",
            "\n",
            "[epoch:  15/100000, batch:   564/  840, ite: 1027] train loss: 2.269287, tar: 0.301900 \n",
            "l0: 0.360211, l1: 0.386424, l2: 0.378451, l3: 0.370644, l4: 0.367556, l5: 0.377933, l6: 0.399466\n",
            "\n",
            "[epoch:  15/100000, batch:   576/  840, ite: 1028] train loss: 2.269648, tar: 0.301957 \n",
            "l0: 0.168823, l1: 0.185658, l2: 0.175002, l3: 0.179873, l4: 0.189728, l5: 0.203946, l6: 0.246864\n",
            "\n",
            "[epoch:  15/100000, batch:   588/  840, ite: 1029] train loss: 2.268754, tar: 0.301827 \n",
            "l0: 0.098884, l1: 0.105305, l2: 0.109725, l3: 0.107863, l4: 0.106835, l5: 0.117795, l6: 0.154605\n",
            "\n",
            "[epoch:  15/100000, batch:   600/  840, ite: 1030] train loss: 2.267329, tar: 0.301630 \n",
            "l0: 0.149059, l1: 0.158447, l2: 0.151120, l3: 0.147914, l4: 0.159871, l5: 0.171145, l6: 0.217448\n",
            "\n",
            "[epoch:  15/100000, batch:   612/  840, ite: 1031] train loss: 2.266250, tar: 0.301482 \n",
            "l0: 0.220608, l1: 0.219432, l2: 0.229715, l3: 0.236377, l4: 0.237702, l5: 0.264434, l6: 0.289856\n",
            "\n",
            "[epoch:  15/100000, batch:   624/  840, ite: 1032] train loss: 2.265700, tar: 0.301404 \n",
            "l0: 0.280506, l1: 0.289280, l2: 0.282985, l3: 0.300057, l4: 0.296731, l5: 0.299332, l6: 0.323741\n",
            "\n",
            "[epoch:  15/100000, batch:   636/  840, ite: 1033] train loss: 2.265513, tar: 0.301384 \n",
            "l0: 0.246469, l1: 0.242973, l2: 0.249654, l3: 0.249173, l4: 0.250926, l5: 0.275684, l6: 0.290151\n",
            "\n",
            "[epoch:  15/100000, batch:   648/  840, ite: 1034] train loss: 2.265068, tar: 0.301330 \n",
            "l0: 0.283466, l1: 0.259509, l2: 0.291742, l3: 0.305508, l4: 0.298247, l5: 0.328022, l6: 0.382876\n",
            "\n",
            "[epoch:  15/100000, batch:   660/  840, ite: 1035] train loss: 2.264956, tar: 0.301313 \n",
            "l0: 0.151530, l1: 0.163858, l2: 0.148659, l3: 0.144458, l4: 0.156671, l5: 0.172458, l6: 0.211121\n",
            "\n",
            "[epoch:  15/100000, batch:   672/  840, ite: 1036] train loss: 2.263878, tar: 0.301169 \n",
            "l0: 0.298174, l1: 0.289328, l2: 0.297709, l3: 0.322232, l4: 0.321513, l5: 0.335645, l6: 0.367576\n",
            "\n",
            "[epoch:  15/100000, batch:   684/  840, ite: 1037] train loss: 2.263848, tar: 0.301166 \n",
            "l0: 0.197679, l1: 0.188922, l2: 0.201226, l3: 0.206096, l4: 0.225526, l5: 0.240120, l6: 0.260296\n",
            "\n",
            "[epoch:  15/100000, batch:   696/  840, ite: 1038] train loss: 2.263131, tar: 0.301066 \n",
            "l0: 0.151849, l1: 0.143239, l2: 0.160385, l3: 0.153225, l4: 0.169513, l5: 0.194220, l6: 0.250363\n",
            "\n",
            "[epoch:  15/100000, batch:   708/  840, ite: 1039] train loss: 2.262130, tar: 0.300922 \n",
            "l0: 0.366236, l1: 0.360052, l2: 0.360381, l3: 0.373563, l4: 0.392924, l5: 0.415560, l6: 0.454580\n",
            "\n",
            "[epoch:  15/100000, batch:   720/  840, ite: 1040] train loss: 2.262573, tar: 0.300985 \n",
            "l0: 0.409182, l1: 0.411241, l2: 0.398976, l3: 0.432272, l4: 0.440677, l5: 0.468842, l6: 0.458700\n",
            "\n",
            "[epoch:  15/100000, batch:   732/  840, ite: 1041] train loss: 2.263301, tar: 0.301089 \n",
            "l0: 0.229495, l1: 0.246055, l2: 0.238560, l3: 0.251028, l4: 0.250846, l5: 0.253440, l6: 0.273768\n",
            "\n",
            "[epoch:  15/100000, batch:   744/  840, ite: 1042] train loss: 2.262801, tar: 0.301021 \n",
            "l0: 0.267531, l1: 0.291881, l2: 0.292364, l3: 0.252688, l4: 0.260217, l5: 0.263562, l6: 0.283872\n",
            "\n",
            "[epoch:  15/100000, batch:   756/  840, ite: 1043] train loss: 2.262465, tar: 0.300988 \n",
            "l0: 0.433197, l1: 0.434090, l2: 0.420019, l3: 0.476558, l4: 0.474693, l5: 0.535084, l6: 0.559285\n",
            "\n",
            "[epoch:  15/100000, batch:   768/  840, ite: 1044] train loss: 2.263491, tar: 0.301115 \n",
            "l0: 0.265910, l1: 0.288938, l2: 0.281721, l3: 0.266186, l4: 0.262969, l5: 0.269229, l6: 0.290000\n",
            "\n",
            "[epoch:  15/100000, batch:   780/  840, ite: 1045] train loss: 2.263167, tar: 0.301081 \n",
            "l0: 0.262085, l1: 0.258333, l2: 0.268068, l3: 0.281677, l4: 0.287365, l5: 0.305774, l6: 0.362799\n",
            "\n",
            "[epoch:  15/100000, batch:   792/  840, ite: 1046] train loss: 2.262940, tar: 0.301044 \n",
            "l0: 0.136470, l1: 0.143420, l2: 0.149832, l3: 0.146448, l4: 0.150478, l5: 0.168544, l6: 0.183571\n",
            "\n",
            "[epoch:  15/100000, batch:   804/  840, ite: 1047] train loss: 2.261809, tar: 0.300887 \n",
            "l0: 0.301306, l1: 0.301029, l2: 0.326034, l3: 0.338406, l4: 0.319252, l5: 0.348156, l6: 0.351155\n",
            "\n",
            "[epoch:  15/100000, batch:   816/  840, ite: 1048] train loss: 2.261832, tar: 0.300887 \n",
            "l0: 0.321077, l1: 0.322702, l2: 0.313751, l3: 0.363475, l4: 0.369312, l5: 0.431650, l6: 0.444353\n",
            "\n",
            "[epoch:  15/100000, batch:   828/  840, ite: 1049] train loss: 2.262122, tar: 0.300907 \n",
            "l0: 0.250819, l1: 0.268765, l2: 0.248171, l3: 0.266837, l4: 0.272408, l5: 0.283696, l6: 0.316340\n",
            "\n",
            "[epoch:  15/100000, batch:   840/  840, ite: 1050] train loss: 2.261784, tar: 0.300859 \n",
            "l0: 0.216047, l1: 0.235683, l2: 0.228185, l3: 0.219034, l4: 0.223877, l5: 0.233599, l6: 0.251485\n",
            "\n",
            "[epoch:  16/100000, batch:    12/  840, ite: 1051] train loss: 2.261162, tar: 0.300778 \n",
            "l0: 0.236950, l1: 0.228711, l2: 0.259489, l3: 0.238787, l4: 0.240684, l5: 0.257366, l6: 0.290339\n",
            "\n",
            "[epoch:  16/100000, batch:    24/  840, ite: 1052] train loss: 2.260678, tar: 0.300717 \n",
            "l0: 0.281461, l1: 0.295378, l2: 0.290642, l3: 0.293475, l4: 0.287346, l5: 0.323192, l6: 0.351349\n",
            "\n",
            "[epoch:  16/100000, batch:    36/  840, ite: 1053] train loss: 2.260547, tar: 0.300699 \n",
            "l0: 0.167245, l1: 0.174068, l2: 0.173535, l3: 0.186193, l4: 0.185914, l5: 0.204370, l6: 0.239013\n",
            "\n",
            "[epoch:  16/100000, batch:    48/  840, ite: 1054] train loss: 2.259665, tar: 0.300572 \n",
            "l0: 0.214386, l1: 0.211449, l2: 0.223468, l3: 0.238046, l4: 0.247157, l5: 0.246846, l6: 0.270457\n",
            "\n",
            "[epoch:  16/100000, batch:    60/  840, ite: 1055] train loss: 2.259089, tar: 0.300491 \n",
            "l0: 0.243574, l1: 0.240685, l2: 0.238960, l3: 0.264137, l4: 0.290409, l5: 0.301383, l6: 0.332713\n",
            "\n",
            "[epoch:  16/100000, batch:    72/  840, ite: 1056] train loss: 2.258760, tar: 0.300437 \n",
            "l0: 0.154593, l1: 0.156858, l2: 0.156111, l3: 0.173515, l4: 0.186918, l5: 0.195344, l6: 0.224200\n",
            "\n",
            "[epoch:  16/100000, batch:    84/  840, ite: 1057] train loss: 2.257803, tar: 0.300299 \n",
            "l0: 0.147169, l1: 0.156597, l2: 0.153275, l3: 0.157201, l4: 0.167204, l5: 0.174711, l6: 0.210942\n",
            "\n",
            "[epoch:  16/100000, batch:    96/  840, ite: 1058] train loss: 2.256772, tar: 0.300154 \n",
            "l0: 0.279336, l1: 0.273244, l2: 0.293333, l3: 0.303688, l4: 0.313147, l5: 0.315747, l6: 0.331704\n",
            "\n",
            "[epoch:  16/100000, batch:   108/  840, ite: 1059] train loss: 2.256634, tar: 0.300135 \n",
            "l0: 0.222223, l1: 0.215081, l2: 0.227786, l3: 0.237111, l4: 0.246268, l5: 0.246684, l6: 0.271889\n",
            "\n",
            "[epoch:  16/100000, batch:   120/  840, ite: 1060] train loss: 2.256077, tar: 0.300061 \n",
            "l0: 0.267171, l1: 0.267192, l2: 0.283834, l3: 0.273810, l4: 0.285893, l5: 0.301660, l6: 0.301462\n",
            "\n",
            "[epoch:  16/100000, batch:   132/  840, ite: 1061] train loss: 2.255818, tar: 0.300030 \n",
            "l0: 0.231183, l1: 0.224777, l2: 0.237963, l3: 0.240118, l4: 0.245833, l5: 0.263836, l6: 0.318929\n",
            "\n",
            "[epoch:  16/100000, batch:   144/  840, ite: 1062] train loss: 2.255354, tar: 0.299965 \n",
            "l0: 0.192582, l1: 0.180349, l2: 0.207032, l3: 0.232245, l4: 0.215672, l5: 0.232941, l6: 0.251872\n",
            "\n",
            "[epoch:  16/100000, batch:   156/  840, ite: 1063] train loss: 2.254655, tar: 0.299864 \n",
            "l0: 0.221664, l1: 0.211173, l2: 0.223241, l3: 0.245144, l4: 0.255392, l5: 0.285989, l6: 0.316571\n",
            "\n",
            "[epoch:  16/100000, batch:   168/  840, ite: 1064] train loss: 2.254189, tar: 0.299791 \n",
            "l0: 0.240987, l1: 0.226977, l2: 0.233448, l3: 0.279296, l4: 0.288140, l5: 0.302667, l6: 0.342844\n",
            "\n",
            "[epoch:  16/100000, batch:   180/  840, ite: 1065] train loss: 2.253870, tar: 0.299735 \n",
            "l0: 0.228186, l1: 0.255060, l2: 0.242086, l3: 0.252175, l4: 0.238184, l5: 0.231518, l6: 0.269154\n",
            "\n",
            "[epoch:  16/100000, batch:   192/  840, ite: 1066] train loss: 2.253366, tar: 0.299668 \n",
            "l0: 0.224848, l1: 0.231418, l2: 0.242889, l3: 0.234251, l4: 0.228692, l5: 0.225143, l6: 0.263461\n",
            "\n",
            "[epoch:  16/100000, batch:   204/  840, ite: 1067] train loss: 2.252801, tar: 0.299598 \n",
            "l0: 0.290923, l1: 0.285744, l2: 0.293301, l3: 0.307745, l4: 0.331114, l5: 0.342912, l6: 0.344044\n",
            "\n",
            "[epoch:  16/100000, batch:   216/  840, ite: 1068] train loss: 2.252748, tar: 0.299590 \n",
            "l0: 0.358630, l1: 0.314706, l2: 0.375915, l3: 0.405908, l4: 0.402973, l5: 0.402066, l6: 0.422810\n",
            "\n",
            "[epoch:  16/100000, batch:   228/  840, ite: 1069] train loss: 2.253150, tar: 0.299645 \n",
            "l0: 0.267835, l1: 0.262825, l2: 0.274543, l3: 0.309949, l4: 0.307945, l5: 0.324757, l6: 0.346293\n",
            "\n",
            "[epoch:  16/100000, batch:   240/  840, ite: 1070] train loss: 2.253002, tar: 0.299616 \n",
            "l0: 0.226760, l1: 0.204565, l2: 0.236902, l3: 0.258428, l4: 0.272075, l5: 0.290766, l6: 0.312375\n",
            "\n",
            "[epoch:  16/100000, batch:   252/  840, ite: 1071] train loss: 2.252580, tar: 0.299548 \n",
            "l0: 0.257016, l1: 0.258298, l2: 0.255638, l3: 0.256071, l4: 0.260918, l5: 0.284807, l6: 0.321472\n",
            "\n",
            "[epoch:  16/100000, batch:   264/  840, ite: 1072] train loss: 2.252246, tar: 0.299508 \n",
            "l0: 0.208045, l1: 0.194129, l2: 0.219330, l3: 0.233968, l4: 0.241722, l5: 0.245107, l6: 0.269920\n",
            "\n",
            "[epoch:  16/100000, batch:   276/  840, ite: 1073] train loss: 2.251650, tar: 0.299423 \n",
            "l0: 0.249313, l1: 0.248701, l2: 0.255741, l3: 0.280824, l4: 0.279343, l5: 0.271398, l6: 0.314483\n",
            "\n",
            "[epoch:  16/100000, batch:   288/  840, ite: 1074] train loss: 2.251322, tar: 0.299376 \n",
            "l0: 0.346581, l1: 0.369897, l2: 0.364216, l3: 0.342647, l4: 0.344217, l5: 0.355589, l6: 0.391360\n",
            "\n",
            "[epoch:  16/100000, batch:   300/  840, ite: 1075] train loss: 2.251567, tar: 0.299420 \n",
            "l0: 0.167407, l1: 0.168507, l2: 0.182202, l3: 0.188421, l4: 0.190211, l5: 0.194228, l6: 0.208072\n",
            "\n",
            "[epoch:  16/100000, batch:   312/  840, ite: 1076] train loss: 2.250682, tar: 0.299297 \n",
            "l0: 0.240231, l1: 0.231419, l2: 0.261876, l3: 0.273754, l4: 0.273928, l5: 0.294709, l6: 0.311045\n",
            "\n",
            "[epoch:  16/100000, batch:   324/  840, ite: 1077] train loss: 2.250344, tar: 0.299242 \n",
            "l0: 0.343456, l1: 0.367947, l2: 0.349624, l3: 0.372142, l4: 0.362752, l5: 0.395008, l6: 0.410437\n",
            "\n",
            "[epoch:  16/100000, batch:   336/  840, ite: 1078] train loss: 2.250670, tar: 0.299283 \n",
            "l0: 0.301535, l1: 0.321097, l2: 0.309494, l3: 0.310408, l4: 0.321789, l5: 0.340215, l6: 0.374347\n",
            "\n",
            "[epoch:  16/100000, batch:   348/  840, ite: 1079] train loss: 2.250696, tar: 0.299286 \n",
            "l0: 0.205409, l1: 0.209970, l2: 0.212540, l3: 0.229638, l4: 0.237736, l5: 0.239176, l6: 0.269970\n",
            "\n",
            "[epoch:  16/100000, batch:   360/  840, ite: 1080] train loss: 2.250097, tar: 0.299199 \n",
            "l0: 0.230009, l1: 0.241760, l2: 0.222667, l3: 0.262919, l4: 0.282267, l5: 0.271064, l6: 0.285732\n",
            "\n",
            "[epoch:  16/100000, batch:   372/  840, ite: 1081] train loss: 2.249678, tar: 0.299135 \n",
            "l0: 0.114428, l1: 0.115449, l2: 0.118913, l3: 0.137586, l4: 0.146361, l5: 0.154902, l6: 0.174586\n",
            "\n",
            "[epoch:  16/100000, batch:   384/  840, ite: 1082] train loss: 2.248488, tar: 0.298964 \n",
            "l0: 0.111122, l1: 0.105565, l2: 0.119932, l3: 0.138219, l4: 0.153489, l5: 0.155457, l6: 0.186282\n",
            "\n",
            "[epoch:  16/100000, batch:   396/  840, ite: 1083] train loss: 2.247307, tar: 0.298790 \n",
            "l0: 0.218516, l1: 0.218722, l2: 0.226703, l3: 0.236123, l4: 0.241946, l5: 0.249869, l6: 0.292315\n",
            "\n",
            "[epoch:  16/100000, batch:   408/  840, ite: 1084] train loss: 2.246788, tar: 0.298716 \n",
            "l0: 0.177617, l1: 0.176886, l2: 0.179373, l3: 0.191073, l4: 0.199861, l5: 0.216717, l6: 0.264500\n",
            "\n",
            "[epoch:  16/100000, batch:   420/  840, ite: 1085] train loss: 2.246013, tar: 0.298605 \n",
            "l0: 0.284393, l1: 0.294186, l2: 0.282441, l3: 0.290157, l4: 0.282777, l5: 0.308421, l6: 0.359566\n",
            "\n",
            "[epoch:  16/100000, batch:   432/  840, ite: 1086] train loss: 2.245880, tar: 0.298592 \n",
            "l0: 0.296912, l1: 0.315982, l2: 0.317540, l3: 0.359116, l4: 0.341565, l5: 0.321228, l6: 0.317283\n",
            "\n",
            "[epoch:  16/100000, batch:   444/  840, ite: 1087] train loss: 2.245902, tar: 0.298590 \n",
            "l0: 0.288439, l1: 0.280304, l2: 0.291975, l3: 0.343785, l4: 0.335057, l5: 0.335976, l6: 0.368397\n",
            "\n",
            "[epoch:  16/100000, batch:   456/  840, ite: 1088] train loss: 2.245900, tar: 0.298581 \n",
            "l0: 0.133891, l1: 0.136538, l2: 0.137584, l3: 0.143196, l4: 0.152641, l5: 0.167751, l6: 0.196831\n",
            "\n",
            "[epoch:  16/100000, batch:   468/  840, ite: 1089] train loss: 2.244819, tar: 0.298430 \n",
            "l0: 0.257088, l1: 0.268177, l2: 0.262234, l3: 0.268929, l4: 0.281652, l5: 0.314429, l6: 0.312360\n",
            "\n",
            "[epoch:  16/100000, batch:   480/  840, ite: 1090] train loss: 2.244562, tar: 0.298392 \n",
            "l0: 0.157906, l1: 0.171082, l2: 0.157025, l3: 0.173328, l4: 0.179156, l5: 0.209406, l6: 0.232468\n",
            "\n",
            "[epoch:  16/100000, batch:   492/  840, ite: 1091] train loss: 2.243678, tar: 0.298263 \n",
            "l0: 0.242260, l1: 0.259671, l2: 0.243110, l3: 0.265869, l4: 0.265975, l5: 0.278793, l6: 0.309468\n",
            "\n",
            "[epoch:  16/100000, batch:   504/  840, ite: 1092] train loss: 2.243331, tar: 0.298212 \n",
            "l0: 0.176663, l1: 0.178500, l2: 0.184290, l3: 0.183718, l4: 0.184550, l5: 0.208237, l6: 0.232552\n",
            "\n",
            "[epoch:  16/100000, batch:   516/  840, ite: 1093] train loss: 2.242513, tar: 0.298100 \n",
            "l0: 0.241054, l1: 0.250925, l2: 0.253020, l3: 0.260617, l4: 0.241825, l5: 0.248911, l6: 0.284298\n",
            "\n",
            "[epoch:  16/100000, batch:   528/  840, ite: 1094] train loss: 2.242091, tar: 0.298048 \n",
            "l0: 0.376053, l1: 0.403203, l2: 0.378318, l3: 0.377530, l4: 0.361020, l5: 0.368343, l6: 0.399977\n",
            "\n",
            "[epoch:  16/100000, batch:   540/  840, ite: 1095] train loss: 2.242476, tar: 0.298119 \n",
            "l0: 0.194156, l1: 0.198053, l2: 0.217890, l3: 0.204599, l4: 0.203051, l5: 0.216330, l6: 0.246731\n",
            "\n",
            "[epoch:  16/100000, batch:   552/  840, ite: 1096] train loss: 2.241781, tar: 0.298025 \n",
            "l0: 0.209020, l1: 0.181850, l2: 0.191526, l3: 0.250167, l4: 0.434365, l5: 0.317976, l6: 0.326069\n",
            "\n",
            "[epoch:  16/100000, batch:   564/  840, ite: 1097] train loss: 2.241480, tar: 0.297943 \n",
            "l0: 0.244367, l1: 0.271763, l2: 0.243784, l3: 0.286434, l4: 0.272736, l5: 0.272357, l6: 0.294476\n",
            "\n",
            "[epoch:  16/100000, batch:   576/  840, ite: 1098] train loss: 2.241156, tar: 0.297895 \n",
            "l0: 0.289979, l1: 0.280924, l2: 0.275942, l3: 0.329252, l4: 0.347618, l5: 0.355218, l6: 0.399824\n",
            "\n",
            "[epoch:  16/100000, batch:   588/  840, ite: 1099] train loss: 2.241190, tar: 0.297887 \n",
            "l0: 0.229399, l1: 0.214937, l2: 0.242023, l3: 0.255979, l4: 0.273452, l5: 0.250609, l6: 0.274774\n",
            "\n",
            "[epoch:  16/100000, batch:   600/  840, ite: 1100] train loss: 2.240736, tar: 0.297825 \n",
            "l0: 0.140655, l1: 0.152641, l2: 0.139875, l3: 0.148322, l4: 0.159993, l5: 0.172714, l6: 0.204908\n",
            "\n",
            "[epoch:  16/100000, batch:   612/  840, ite: 1101] train loss: 2.239717, tar: 0.297682 \n",
            "l0: 0.189644, l1: 0.180417, l2: 0.191096, l3: 0.196712, l4: 0.216936, l5: 0.249136, l6: 0.295089\n",
            "\n",
            "[epoch:  16/100000, batch:   624/  840, ite: 1102] train loss: 2.239063, tar: 0.297584 \n",
            "l0: 0.240630, l1: 0.235959, l2: 0.264775, l3: 0.260869, l4: 0.258760, l5: 0.256749, l6: 0.304684\n",
            "\n",
            "[epoch:  16/100000, batch:   636/  840, ite: 1103] train loss: 2.238685, tar: 0.297533 \n",
            "l0: 0.266902, l1: 0.283485, l2: 0.281489, l3: 0.278851, l4: 0.275162, l5: 0.296993, l6: 0.352032\n",
            "\n",
            "[epoch:  16/100000, batch:   648/  840, ite: 1104] train loss: 2.238501, tar: 0.297505 \n",
            "l0: 0.266464, l1: 0.274443, l2: 0.284879, l3: 0.286491, l4: 0.289613, l5: 0.300878, l6: 0.321145\n",
            "\n",
            "[epoch:  16/100000, batch:   660/  840, ite: 1105] train loss: 2.238307, tar: 0.297477 \n",
            "l0: 0.364074, l1: 0.386874, l2: 0.384334, l3: 0.414765, l4: 0.397767, l5: 0.371532, l6: 0.390375\n",
            "\n",
            "[epoch:  16/100000, batch:   672/  840, ite: 1106] train loss: 2.238733, tar: 0.297537 \n",
            "l0: 0.271708, l1: 0.276881, l2: 0.283343, l3: 0.290491, l4: 0.294611, l5: 0.293976, l6: 0.313457\n",
            "\n",
            "[epoch:  16/100000, batch:   684/  840, ite: 1107] train loss: 2.238539, tar: 0.297514 \n",
            "l0: 0.198592, l1: 0.191162, l2: 0.203510, l3: 0.229365, l4: 0.235124, l5: 0.233973, l6: 0.261077\n",
            "\n",
            "[epoch:  16/100000, batch:   696/  840, ite: 1108] train loss: 2.237920, tar: 0.297424 \n",
            "l0: 0.201354, l1: 0.212900, l2: 0.202195, l3: 0.212845, l4: 0.215017, l5: 0.227960, l6: 0.281025\n",
            "\n",
            "[epoch:  16/100000, batch:   708/  840, ite: 1109] train loss: 2.237303, tar: 0.297338 \n",
            "l0: 0.235866, l1: 0.234171, l2: 0.245215, l3: 0.257161, l4: 0.259323, l5: 0.275590, l6: 0.302346\n",
            "\n",
            "[epoch:  16/100000, batch:   720/  840, ite: 1110] train loss: 2.236918, tar: 0.297283 \n",
            "l0: 0.201483, l1: 0.205644, l2: 0.201832, l3: 0.226541, l4: 0.228323, l5: 0.237936, l6: 0.275951\n",
            "\n",
            "[epoch:  16/100000, batch:   732/  840, ite: 1111] train loss: 2.236324, tar: 0.297196 \n",
            "l0: 0.245468, l1: 0.254844, l2: 0.249296, l3: 0.274490, l4: 0.277188, l5: 0.276546, l6: 0.304830\n",
            "\n",
            "[epoch:  16/100000, batch:   744/  840, ite: 1112] train loss: 2.236006, tar: 0.297150 \n",
            "l0: 0.193626, l1: 0.198486, l2: 0.213643, l3: 0.200576, l4: 0.197090, l5: 0.189821, l6: 0.227188\n",
            "\n",
            "[epoch:  16/100000, batch:   756/  840, ite: 1113] train loss: 2.235273, tar: 0.297057 \n",
            "l0: 0.263147, l1: 0.260600, l2: 0.277546, l3: 0.276022, l4: 0.278468, l5: 0.317809, l6: 0.343050\n",
            "\n",
            "[epoch:  16/100000, batch:   768/  840, ite: 1114] train loss: 2.235077, tar: 0.297026 \n",
            "l0: 0.145924, l1: 0.152521, l2: 0.143982, l3: 0.181182, l4: 0.182905, l5: 0.189654, l6: 0.229286\n",
            "\n",
            "[epoch:  16/100000, batch:   780/  840, ite: 1115] train loss: 2.234171, tar: 0.296891 \n",
            "l0: 0.150480, l1: 0.154692, l2: 0.160803, l3: 0.169639, l4: 0.172451, l5: 0.183971, l6: 0.213459\n",
            "\n",
            "[epoch:  16/100000, batch:   792/  840, ite: 1116] train loss: 2.233250, tar: 0.296760 \n",
            "l0: 0.316925, l1: 0.301994, l2: 0.315713, l3: 0.326981, l4: 0.341696, l5: 0.357889, l6: 0.389953\n",
            "\n",
            "[epoch:  16/100000, batch:   804/  840, ite: 1117] train loss: 2.233355, tar: 0.296778 \n",
            "l0: 0.254665, l1: 0.253031, l2: 0.251311, l3: 0.260189, l4: 0.266424, l5: 0.285468, l6: 0.334141\n",
            "\n",
            "[epoch:  16/100000, batch:   816/  840, ite: 1118] train loss: 2.233062, tar: 0.296740 \n",
            "l0: 0.207150, l1: 0.211430, l2: 0.207735, l3: 0.235694, l4: 0.236285, l5: 0.243811, l6: 0.284718\n",
            "\n",
            "[epoch:  16/100000, batch:   828/  840, ite: 1119] train loss: 2.232520, tar: 0.296660 \n",
            "l0: 0.281584, l1: 0.274010, l2: 0.295064, l3: 0.295178, l4: 0.301360, l5: 0.294320, l6: 0.317336\n",
            "\n",
            "[epoch:  16/100000, batch:   840/  840, ite: 1120] train loss: 2.232365, tar: 0.296646 \n",
            "l0: 0.221285, l1: 0.221022, l2: 0.222875, l3: 0.266325, l4: 0.265958, l5: 0.269458, l6: 0.301479\n",
            "\n",
            "[epoch:  17/100000, batch:    12/  840, ite: 1121] train loss: 2.231951, tar: 0.296579 \n",
            "l0: 0.248236, l1: 0.240123, l2: 0.254608, l3: 0.261639, l4: 0.282118, l5: 0.305241, l6: 0.338897\n",
            "\n",
            "[epoch:  17/100000, batch:    24/  840, ite: 1122] train loss: 2.231683, tar: 0.296536 \n",
            "l0: 0.187860, l1: 0.192927, l2: 0.202062, l3: 0.198777, l4: 0.197089, l5: 0.213832, l6: 0.239905\n",
            "\n",
            "[epoch:  17/100000, batch:    36/  840, ite: 1123] train loss: 2.230971, tar: 0.296439 \n",
            "l0: 0.280252, l1: 0.283238, l2: 0.292758, l3: 0.282110, l4: 0.298952, l5: 0.311877, l6: 0.321385\n",
            "\n",
            "[epoch:  17/100000, batch:    48/  840, ite: 1124] train loss: 2.230828, tar: 0.296425 \n",
            "l0: 0.220846, l1: 0.235901, l2: 0.216141, l3: 0.225134, l4: 0.238661, l5: 0.255612, l6: 0.293756\n",
            "\n",
            "[epoch:  17/100000, batch:    60/  840, ite: 1125] train loss: 2.230344, tar: 0.296358 \n",
            "l0: 0.185565, l1: 0.207739, l2: 0.184248, l3: 0.192845, l4: 0.198092, l5: 0.221336, l6: 0.275090\n",
            "\n",
            "[epoch:  17/100000, batch:    72/  840, ite: 1126] train loss: 2.229664, tar: 0.296259 \n",
            "l0: 0.159390, l1: 0.157148, l2: 0.161560, l3: 0.188814, l4: 0.196954, l5: 0.211235, l6: 0.238341\n",
            "\n",
            "[epoch:  17/100000, batch:    84/  840, ite: 1127] train loss: 2.228851, tar: 0.296138 \n",
            "l0: 0.173516, l1: 0.180595, l2: 0.178645, l3: 0.183325, l4: 0.194993, l5: 0.201144, l6: 0.230512\n",
            "\n",
            "[epoch:  17/100000, batch:    96/  840, ite: 1128] train loss: 2.228065, tar: 0.296029 \n",
            "l0: 0.250139, l1: 0.246955, l2: 0.256813, l3: 0.251573, l4: 0.247140, l5: 0.264397, l6: 0.310284\n",
            "\n",
            "[epoch:  17/100000, batch:   108/  840, ite: 1129] train loss: 2.227710, tar: 0.295989 \n",
            "l0: 0.213475, l1: 0.228401, l2: 0.215879, l3: 0.220502, l4: 0.221962, l5: 0.228766, l6: 0.258126\n",
            "\n",
            "[epoch:  17/100000, batch:   120/  840, ite: 1130] train loss: 2.227144, tar: 0.295916 \n",
            "l0: 0.235206, l1: 0.221271, l2: 0.254384, l3: 0.284397, l4: 0.278503, l5: 0.278593, l6: 0.298962\n",
            "\n",
            "[epoch:  17/100000, batch:   132/  840, ite: 1131] train loss: 2.226811, tar: 0.295862 \n",
            "l0: 0.188500, l1: 0.196274, l2: 0.193974, l3: 0.205548, l4: 0.208580, l5: 0.223570, l6: 0.256443\n",
            "\n",
            "[epoch:  17/100000, batch:   144/  840, ite: 1132] train loss: 2.226146, tar: 0.295767 \n",
            "l0: 0.228081, l1: 0.235484, l2: 0.230069, l3: 0.238799, l4: 0.249914, l5: 0.253249, l6: 0.265966\n",
            "\n",
            "[epoch:  17/100000, batch:   156/  840, ite: 1133] train loss: 2.225682, tar: 0.295707 \n",
            "l0: 0.148382, l1: 0.148105, l2: 0.157783, l3: 0.161204, l4: 0.156270, l5: 0.169357, l6: 0.210194\n",
            "\n",
            "[epoch:  17/100000, batch:   168/  840, ite: 1134] train loss: 2.224735, tar: 0.295577 \n",
            "l0: 0.294272, l1: 0.292853, l2: 0.311569, l3: 0.310649, l4: 0.303886, l5: 0.319628, l6: 0.347277\n",
            "\n",
            "[epoch:  17/100000, batch:   180/  840, ite: 1135] train loss: 2.224696, tar: 0.295576 \n",
            "l0: 0.233651, l1: 0.244691, l2: 0.241066, l3: 0.257042, l4: 0.260378, l5: 0.258193, l6: 0.293690\n",
            "\n",
            "[epoch:  17/100000, batch:   192/  840, ite: 1136] train loss: 2.224312, tar: 0.295522 \n",
            "l0: 0.120763, l1: 0.127595, l2: 0.125679, l3: 0.127860, l4: 0.131456, l5: 0.157872, l6: 0.192684\n",
            "\n",
            "[epoch:  17/100000, batch:   204/  840, ite: 1137] train loss: 2.223221, tar: 0.295368 \n",
            "l0: 0.284889, l1: 0.295848, l2: 0.293333, l3: 0.296542, l4: 0.284228, l5: 0.341180, l6: 0.379114\n",
            "\n",
            "[epoch:  17/100000, batch:   216/  840, ite: 1138] train loss: 2.223179, tar: 0.295359 \n",
            "l0: 0.246220, l1: 0.275213, l2: 0.251771, l3: 0.238586, l4: 0.236725, l5: 0.265363, l6: 0.288137\n",
            "\n",
            "[epoch:  17/100000, batch:   228/  840, ite: 1139] train loss: 2.222809, tar: 0.295316 \n",
            "l0: 0.188232, l1: 0.189784, l2: 0.191003, l3: 0.206997, l4: 0.210139, l5: 0.223682, l6: 0.262626\n",
            "\n",
            "[epoch:  17/100000, batch:   240/  840, ite: 1140] train loss: 2.222151, tar: 0.295222 \n",
            "l0: 0.138146, l1: 0.144049, l2: 0.147788, l3: 0.162689, l4: 0.162241, l5: 0.169071, l6: 0.189463\n",
            "\n",
            "[epoch:  17/100000, batch:   252/  840, ite: 1141] train loss: 2.221179, tar: 0.295084 \n",
            "l0: 0.153486, l1: 0.159070, l2: 0.159713, l3: 0.189542, l4: 0.190992, l5: 0.199267, l6: 0.243392\n",
            "\n",
            "[epoch:  17/100000, batch:   264/  840, ite: 1142] train loss: 2.220369, tar: 0.294960 \n",
            "l0: 0.190200, l1: 0.198626, l2: 0.193983, l3: 0.225214, l4: 0.222880, l5: 0.235390, l6: 0.281445\n",
            "\n",
            "[epoch:  17/100000, batch:   276/  840, ite: 1143] train loss: 2.219780, tar: 0.294868 \n",
            "l0: 0.240295, l1: 0.223705, l2: 0.227071, l3: 0.258945, l4: 0.272619, l5: 0.319388, l6: 0.393893\n",
            "\n",
            "[epoch:  17/100000, batch:   288/  840, ite: 1144] train loss: 2.219532, tar: 0.294821 \n",
            "l0: 0.196108, l1: 0.221293, l2: 0.206130, l3: 0.226618, l4: 0.222334, l5: 0.205791, l6: 0.227645\n",
            "\n",
            "[epoch:  17/100000, batch:   300/  840, ite: 1145] train loss: 2.218909, tar: 0.294735 \n",
            "l0: 0.174883, l1: 0.166968, l2: 0.182032, l3: 0.218605, l4: 0.214626, l5: 0.221781, l6: 0.242635\n",
            "\n",
            "[epoch:  17/100000, batch:   312/  840, ite: 1146] train loss: 2.218213, tar: 0.294630 \n",
            "l0: 0.258170, l1: 0.259953, l2: 0.260387, l3: 0.285144, l4: 0.298435, l5: 0.307832, l6: 0.333266\n",
            "\n",
            "[epoch:  17/100000, batch:   324/  840, ite: 1147] train loss: 2.218026, tar: 0.294598 \n",
            "l0: 0.238971, l1: 0.259192, l2: 0.249990, l3: 0.234356, l4: 0.235546, l5: 0.247340, l6: 0.307504\n",
            "\n",
            "[epoch:  17/100000, batch:   336/  840, ite: 1148] train loss: 2.217638, tar: 0.294550 \n",
            "l0: 0.308463, l1: 0.317442, l2: 0.310530, l3: 0.341264, l4: 0.353295, l5: 0.346865, l6: 0.356016\n",
            "\n",
            "[epoch:  17/100000, batch:   348/  840, ite: 1149] train loss: 2.217739, tar: 0.294562 \n",
            "l0: 0.220020, l1: 0.221835, l2: 0.225762, l3: 0.229405, l4: 0.242243, l5: 0.255917, l6: 0.294468\n",
            "\n",
            "[epoch:  17/100000, batch:   360/  840, ite: 1150] train loss: 2.217280, tar: 0.294497 \n",
            "l0: 0.210550, l1: 0.212348, l2: 0.226738, l3: 0.236596, l4: 0.225778, l5: 0.237464, l6: 0.273257\n",
            "\n",
            "[epoch:  17/100000, batch:   372/  840, ite: 1151] train loss: 2.216764, tar: 0.294424 \n",
            "l0: 0.292136, l1: 0.304743, l2: 0.289709, l3: 0.269864, l4: 0.296871, l5: 0.339547, l6: 0.372719\n",
            "\n",
            "[epoch:  17/100000, batch:   384/  840, ite: 1152] train loss: 2.216719, tar: 0.294422 \n",
            "l0: 0.187869, l1: 0.204716, l2: 0.197548, l3: 0.195207, l4: 0.188775, l5: 0.198107, l6: 0.224556\n",
            "\n",
            "[epoch:  17/100000, batch:   396/  840, ite: 1153] train loss: 2.216008, tar: 0.294330 \n",
            "l0: 0.168399, l1: 0.175492, l2: 0.173735, l3: 0.189751, l4: 0.177371, l5: 0.191050, l6: 0.232314\n",
            "\n",
            "[epoch:  17/100000, batch:   408/  840, ite: 1154] train loss: 2.215221, tar: 0.294221 \n",
            "l0: 0.246122, l1: 0.241319, l2: 0.248336, l3: 0.270724, l4: 0.269687, l5: 0.285442, l6: 0.320459\n",
            "\n",
            "[epoch:  17/100000, batch:   420/  840, ite: 1155] train loss: 2.214933, tar: 0.294179 \n",
            "l0: 0.192783, l1: 0.196068, l2: 0.194615, l3: 0.235137, l4: 0.236199, l5: 0.229785, l6: 0.253706\n",
            "\n",
            "[epoch:  17/100000, batch:   432/  840, ite: 1156] train loss: 2.214348, tar: 0.294091 \n",
            "l0: 0.215016, l1: 0.215058, l2: 0.224233, l3: 0.232001, l4: 0.227743, l5: 0.228889, l6: 0.266705\n",
            "\n",
            "[epoch:  17/100000, batch:   444/  840, ite: 1157] train loss: 2.213825, tar: 0.294023 \n",
            "l0: 0.187711, l1: 0.191247, l2: 0.195065, l3: 0.190468, l4: 0.192599, l5: 0.203876, l6: 0.237060\n",
            "\n",
            "[epoch:  17/100000, batch:   456/  840, ite: 1158] train loss: 2.213120, tar: 0.293931 \n",
            "l0: 0.436648, l1: 0.438476, l2: 0.459166, l3: 0.462269, l4: 0.448314, l5: 0.437374, l6: 0.447474\n",
            "\n",
            "[epoch:  17/100000, batch:   468/  840, ite: 1159] train loss: 2.213911, tar: 0.294054 \n",
            "l0: 0.245197, l1: 0.248150, l2: 0.236805, l3: 0.260455, l4: 0.270507, l5: 0.298395, l6: 0.336232\n",
            "\n",
            "[epoch:  17/100000, batch:   480/  840, ite: 1160] train loss: 2.213637, tar: 0.294012 \n",
            "l0: 0.163930, l1: 0.171009, l2: 0.162561, l3: 0.170146, l4: 0.175432, l5: 0.203901, l6: 0.236805\n",
            "\n",
            "[epoch:  17/100000, batch:   492/  840, ite: 1161] train loss: 2.212836, tar: 0.293900 \n",
            "l0: 0.184047, l1: 0.185408, l2: 0.190379, l3: 0.202811, l4: 0.200435, l5: 0.225970, l6: 0.254632\n",
            "\n",
            "[epoch:  17/100000, batch:   504/  840, ite: 1162] train loss: 2.212174, tar: 0.293806 \n",
            "l0: 0.185413, l1: 0.186405, l2: 0.192344, l3: 0.179559, l4: 0.183323, l5: 0.204055, l6: 0.244657\n",
            "\n",
            "[epoch:  17/100000, batch:   516/  840, ite: 1163] train loss: 2.211455, tar: 0.293712 \n",
            "l0: 0.234572, l1: 0.232650, l2: 0.242452, l3: 0.258902, l4: 0.255370, l5: 0.265733, l6: 0.295015\n",
            "\n",
            "[epoch:  17/100000, batch:   528/  840, ite: 1164] train loss: 2.211088, tar: 0.293662 \n",
            "l0: 0.193668, l1: 0.191825, l2: 0.197708, l3: 0.224392, l4: 0.223351, l5: 0.246656, l6: 0.260798\n",
            "\n",
            "[epoch:  17/100000, batch:   540/  840, ite: 1165] train loss: 2.210510, tar: 0.293576 \n",
            "l0: 0.315592, l1: 0.306283, l2: 0.318677, l3: 0.343107, l4: 0.350354, l5: 0.363398, l6: 0.400413\n",
            "\n",
            "[epoch:  17/100000, batch:   552/  840, ite: 1166] train loss: 2.210671, tar: 0.293595 \n",
            "l0: 0.194806, l1: 0.192882, l2: 0.204931, l3: 0.208156, l4: 0.213113, l5: 0.230550, l6: 0.265118\n",
            "\n",
            "[epoch:  17/100000, batch:   564/  840, ite: 1167] train loss: 2.210070, tar: 0.293510 \n",
            "l0: 0.231473, l1: 0.234083, l2: 0.250148, l3: 0.247649, l4: 0.243976, l5: 0.249768, l6: 0.283880\n",
            "\n",
            "[epoch:  17/100000, batch:   576/  840, ite: 1168] train loss: 2.209668, tar: 0.293457 \n",
            "l0: 0.204294, l1: 0.219893, l2: 0.219634, l3: 0.232335, l4: 0.218864, l5: 0.217559, l6: 0.257105\n",
            "\n",
            "[epoch:  17/100000, batch:   588/  840, ite: 1169] train loss: 2.209121, tar: 0.293381 \n",
            "l0: 0.130052, l1: 0.136562, l2: 0.134121, l3: 0.134726, l4: 0.145102, l5: 0.159638, l6: 0.202468\n",
            "\n",
            "[epoch:  17/100000, batch:   600/  840, ite: 1170] train loss: 2.208124, tar: 0.293241 \n",
            "l0: 0.200032, l1: 0.212275, l2: 0.207384, l3: 0.197459, l4: 0.204569, l5: 0.224225, l6: 0.261892\n",
            "\n",
            "[epoch:  17/100000, batch:   612/  840, ite: 1171] train loss: 2.207526, tar: 0.293161 \n",
            "l0: 0.585824, l1: 0.585940, l2: 0.562292, l3: 0.649925, l4: 0.687916, l5: 0.648278, l6: 0.582788\n",
            "\n",
            "[epoch:  17/100000, batch:   624/  840, ite: 1172] train loss: 2.209314, tar: 0.293411 \n",
            "l0: 0.211125, l1: 0.235168, l2: 0.220537, l3: 0.227225, l4: 0.217717, l5: 0.246419, l6: 0.283641\n",
            "\n",
            "[epoch:  17/100000, batch:   636/  840, ite: 1173] train loss: 2.208830, tar: 0.293341 \n",
            "l0: 0.172157, l1: 0.175627, l2: 0.180715, l3: 0.186831, l4: 0.193720, l5: 0.209173, l6: 0.244059\n",
            "\n",
            "[epoch:  17/100000, batch:   648/  840, ite: 1174] train loss: 2.208109, tar: 0.293238 \n",
            "l0: 0.216000, l1: 0.206403, l2: 0.244299, l3: 0.237691, l4: 0.235337, l5: 0.231614, l6: 0.251498\n",
            "\n",
            "[epoch:  17/100000, batch:   660/  840, ite: 1175] train loss: 2.207611, tar: 0.293172 \n",
            "l0: 0.298685, l1: 0.319252, l2: 0.299814, l3: 0.325984, l4: 0.327354, l5: 0.315943, l6: 0.332017\n",
            "\n",
            "[epoch:  17/100000, batch:   672/  840, ite: 1176] train loss: 2.207621, tar: 0.293177 \n",
            "l0: 0.279941, l1: 0.287732, l2: 0.309876, l3: 0.305219, l4: 0.297262, l5: 0.299822, l6: 0.301669\n",
            "\n",
            "[epoch:  17/100000, batch:   684/  840, ite: 1177] train loss: 2.207513, tar: 0.293165 \n",
            "l0: 0.325210, l1: 0.318374, l2: 0.330468, l3: 0.347386, l4: 0.361095, l5: 0.353385, l6: 0.414212\n",
            "\n",
            "[epoch:  17/100000, batch:   696/  840, ite: 1178] train loss: 2.207720, tar: 0.293193 \n",
            "l0: 0.305317, l1: 0.290753, l2: 0.311247, l3: 0.314645, l4: 0.327392, l5: 0.333250, l6: 0.416088\n",
            "\n",
            "[epoch:  17/100000, batch:   708/  840, ite: 1179] train loss: 2.207797, tar: 0.293203 \n",
            "l0: 0.224844, l1: 0.223449, l2: 0.240450, l3: 0.240547, l4: 0.242773, l5: 0.243664, l6: 0.295436\n",
            "\n",
            "[epoch:  17/100000, batch:   720/  840, ite: 1180] train loss: 2.207376, tar: 0.293145 \n",
            "l0: 0.332396, l1: 0.315195, l2: 0.364651, l3: 0.360680, l4: 0.351412, l5: 0.331718, l6: 0.345376\n",
            "\n",
            "[epoch:  17/100000, batch:   732/  840, ite: 1181] train loss: 2.207540, tar: 0.293178 \n",
            "l0: 0.184873, l1: 0.183995, l2: 0.190603, l3: 0.201901, l4: 0.211238, l5: 0.208835, l6: 0.249487\n",
            "\n",
            "[epoch:  17/100000, batch:   744/  840, ite: 1182] train loss: 2.206883, tar: 0.293087 \n",
            "l0: 0.217312, l1: 0.204699, l2: 0.222884, l3: 0.248925, l4: 0.252506, l5: 0.254224, l6: 0.284862\n",
            "\n",
            "[epoch:  17/100000, batch:   756/  840, ite: 1183] train loss: 2.206442, tar: 0.293022 \n",
            "l0: 0.189637, l1: 0.184451, l2: 0.209105, l3: 0.200026, l4: 0.203072, l5: 0.217705, l6: 0.235417\n",
            "\n",
            "[epoch:  17/100000, batch:   768/  840, ite: 1184] train loss: 2.205794, tar: 0.292935 \n",
            "l0: 0.403510, l1: 0.404436, l2: 0.414552, l3: 0.439614, l4: 0.434032, l5: 0.437537, l6: 0.454882\n",
            "\n",
            "[epoch:  17/100000, batch:   780/  840, ite: 1185] train loss: 2.206455, tar: 0.293028 \n",
            "l0: 0.215423, l1: 0.208234, l2: 0.234530, l3: 0.247673, l4: 0.252227, l5: 0.260163, l6: 0.284008\n",
            "\n",
            "[epoch:  17/100000, batch:   792/  840, ite: 1186] train loss: 2.206030, tar: 0.292963 \n",
            "l0: 0.215436, l1: 0.210107, l2: 0.226913, l3: 0.219450, l4: 0.224540, l5: 0.248490, l6: 0.312798\n",
            "\n",
            "[epoch:  17/100000, batch:   804/  840, ite: 1187] train loss: 2.205568, tar: 0.292898 \n",
            "l0: 0.126213, l1: 0.132858, l2: 0.138181, l3: 0.150640, l4: 0.151512, l5: 0.158402, l6: 0.203502\n",
            "\n",
            "[epoch:  17/100000, batch:   816/  840, ite: 1188] train loss: 2.204604, tar: 0.292757 \n",
            "l0: 0.402961, l1: 0.400986, l2: 0.426611, l3: 0.425731, l4: 0.411839, l5: 0.441181, l6: 0.454637\n",
            "\n",
            "[epoch:  17/100000, batch:   828/  840, ite: 1189] train loss: 2.205243, tar: 0.292850 \n",
            "l0: 0.190421, l1: 0.196785, l2: 0.199744, l3: 0.217052, l4: 0.218088, l5: 0.221197, l6: 0.250911\n",
            "\n",
            "[epoch:  17/100000, batch:   840/  840, ite: 1190] train loss: 2.204645, tar: 0.292764 \n",
            "l0: 0.145818, l1: 0.137540, l2: 0.152911, l3: 0.165900, l4: 0.176492, l5: 0.180762, l6: 0.220781\n",
            "\n",
            "[epoch:  18/100000, batch:    12/  840, ite: 1191] train loss: 2.203785, tar: 0.292641 \n",
            "l0: 0.218219, l1: 0.231254, l2: 0.226233, l3: 0.244715, l4: 0.242394, l5: 0.254393, l6: 0.273201\n",
            "\n",
            "[epoch:  18/100000, batch:    24/  840, ite: 1192] train loss: 2.203355, tar: 0.292578 \n",
            "l0: 0.257695, l1: 0.243555, l2: 0.268068, l3: 0.301539, l4: 0.307091, l5: 0.280048, l6: 0.308862\n",
            "\n",
            "[epoch:  18/100000, batch:    36/  840, ite: 1193] train loss: 2.203156, tar: 0.292549 \n",
            "l0: 0.188125, l1: 0.177797, l2: 0.191921, l3: 0.223552, l4: 0.244786, l5: 0.225588, l6: 0.258344\n",
            "\n",
            "[epoch:  18/100000, batch:    48/  840, ite: 1194] train loss: 2.202576, tar: 0.292462 \n",
            "l0: 0.200241, l1: 0.179031, l2: 0.220333, l3: 0.243638, l4: 0.256766, l5: 0.235530, l6: 0.266153\n",
            "\n",
            "[epoch:  18/100000, batch:    60/  840, ite: 1195] train loss: 2.202073, tar: 0.292384 \n",
            "l0: 0.177419, l1: 0.186278, l2: 0.197013, l3: 0.211683, l4: 0.220453, l5: 0.184525, l6: 0.187717\n",
            "\n",
            "[epoch:  18/100000, batch:    72/  840, ite: 1196] train loss: 2.201373, tar: 0.292288 \n",
            "l0: 0.164648, l1: 0.162642, l2: 0.176621, l3: 0.181453, l4: 0.185021, l5: 0.193694, l6: 0.225408\n",
            "\n",
            "[epoch:  18/100000, batch:    84/  840, ite: 1197] train loss: 2.200611, tar: 0.292182 \n",
            "l0: 0.291208, l1: 0.281523, l2: 0.288753, l3: 0.310969, l4: 0.329055, l5: 0.366944, l6: 0.446232\n",
            "\n",
            "[epoch:  18/100000, batch:    96/  840, ite: 1198] train loss: 2.200706, tar: 0.292181 \n",
            "l0: 0.215971, l1: 0.210912, l2: 0.237567, l3: 0.248794, l4: 0.243665, l5: 0.252060, l6: 0.280709\n",
            "\n",
            "[epoch:  18/100000, batch:   108/  840, ite: 1199] train loss: 2.200280, tar: 0.292117 \n",
            "l0: 0.311957, l1: 0.313636, l2: 0.334877, l3: 0.324065, l4: 0.321451, l5: 0.320927, l6: 0.336486\n",
            "\n",
            "[epoch:  18/100000, batch:   120/  840, ite: 1200] train loss: 2.200333, tar: 0.292134 \n",
            "l0: 0.178485, l1: 0.188013, l2: 0.176674, l3: 0.199081, l4: 0.208198, l5: 0.214078, l6: 0.252778\n",
            "\n",
            "[epoch:  18/100000, batch:   132/  840, ite: 1201] train loss: 2.199681, tar: 0.292039 \n",
            "l0: 0.216284, l1: 0.224863, l2: 0.222880, l3: 0.269173, l4: 0.269108, l5: 0.267842, l6: 0.292344\n",
            "\n",
            "[epoch:  18/100000, batch:   144/  840, ite: 1202] train loss: 2.199317, tar: 0.291976 \n",
            "l0: 0.348057, l1: 0.309346, l2: 0.345117, l3: 0.427621, l4: 0.427544, l5: 0.459895, l6: 0.497860\n",
            "\n",
            "[epoch:  18/100000, batch:   156/  840, ite: 1203] train loss: 2.199830, tar: 0.292023 \n",
            "l0: 0.210928, l1: 0.219331, l2: 0.214450, l3: 0.236315, l4: 0.244554, l5: 0.238258, l6: 0.279389\n",
            "\n",
            "[epoch:  18/100000, batch:   168/  840, ite: 1204] train loss: 2.199367, tar: 0.291955 \n",
            "l0: 0.268871, l1: 0.258666, l2: 0.277991, l3: 0.284324, l4: 0.301887, l5: 0.322728, l6: 0.336282\n",
            "\n",
            "[epoch:  18/100000, batch:   180/  840, ite: 1205] train loss: 2.199244, tar: 0.291936 \n",
            "l0: 0.212989, l1: 0.230126, l2: 0.227350, l3: 0.226759, l4: 0.233434, l5: 0.236630, l6: 0.266887\n",
            "\n",
            "[epoch:  18/100000, batch:   192/  840, ite: 1206] train loss: 2.198775, tar: 0.291871 \n",
            "l0: 0.200575, l1: 0.206823, l2: 0.200965, l3: 0.228748, l4: 0.235148, l5: 0.233900, l6: 0.276214\n",
            "\n",
            "[epoch:  18/100000, batch:   204/  840, ite: 1207] train loss: 2.198265, tar: 0.291795 \n",
            "l0: 0.235702, l1: 0.244109, l2: 0.244138, l3: 0.260666, l4: 0.258172, l5: 0.262618, l6: 0.313961\n",
            "\n",
            "[epoch:  18/100000, batch:   216/  840, ite: 1208] train loss: 2.197951, tar: 0.291749 \n",
            "l0: 0.187490, l1: 0.195151, l2: 0.205023, l3: 0.213188, l4: 0.210049, l5: 0.232859, l6: 0.254931\n",
            "\n",
            "[epoch:  18/100000, batch:   228/  840, ite: 1209] train loss: 2.197373, tar: 0.291662 \n",
            "l0: 0.168872, l1: 0.172258, l2: 0.177405, l3: 0.176857, l4: 0.178392, l5: 0.187429, l6: 0.231875\n",
            "\n",
            "[epoch:  18/100000, batch:   240/  840, ite: 1210] train loss: 2.196625, tar: 0.291561 \n",
            "l0: 0.166771, l1: 0.181028, l2: 0.177504, l3: 0.197962, l4: 0.202375, l5: 0.211329, l6: 0.236360\n",
            "\n",
            "[epoch:  18/100000, batch:   252/  840, ite: 1211] train loss: 2.195946, tar: 0.291458 \n",
            "l0: 0.244474, l1: 0.244802, l2: 0.248236, l3: 0.257212, l4: 0.264772, l5: 0.273314, l6: 0.297416\n",
            "\n",
            "[epoch:  18/100000, batch:   264/  840, ite: 1212] train loss: 2.195644, tar: 0.291419 \n",
            "l0: 0.312748, l1: 0.340808, l2: 0.333255, l3: 0.320422, l4: 0.310146, l5: 0.319454, l6: 0.338242\n",
            "\n",
            "[epoch:  18/100000, batch:   276/  840, ite: 1213] train loss: 2.195709, tar: 0.291437 \n",
            "l0: 0.143209, l1: 0.133572, l2: 0.151773, l3: 0.159459, l4: 0.163845, l5: 0.182417, l6: 0.229750\n",
            "\n",
            "[epoch:  18/100000, batch:   288/  840, ite: 1214] train loss: 2.194859, tar: 0.291315 \n",
            "l0: 0.185334, l1: 0.197385, l2: 0.189932, l3: 0.207247, l4: 0.211914, l5: 0.199100, l6: 0.234926\n",
            "\n",
            "[epoch:  18/100000, batch:   300/  840, ite: 1215] train loss: 2.194227, tar: 0.291227 \n",
            "l0: 0.170022, l1: 0.173455, l2: 0.165484, l3: 0.203638, l4: 0.206405, l5: 0.228586, l6: 0.263928\n",
            "\n",
            "[epoch:  18/100000, batch:   312/  840, ite: 1216] train loss: 2.193583, tar: 0.291128 \n",
            "l0: 0.187660, l1: 0.204139, l2: 0.190700, l3: 0.218146, l4: 0.221437, l5: 0.235116, l6: 0.277248\n",
            "\n",
            "[epoch:  18/100000, batch:   324/  840, ite: 1217] train loss: 2.193041, tar: 0.291043 \n",
            "l0: 0.223503, l1: 0.245398, l2: 0.238454, l3: 0.232647, l4: 0.237200, l5: 0.237207, l6: 0.267217\n",
            "\n",
            "[epoch:  18/100000, batch:   336/  840, ite: 1218] train loss: 2.192621, tar: 0.290987 \n",
            "l0: 0.252872, l1: 0.269289, l2: 0.260219, l3: 0.278363, l4: 0.277661, l5: 0.279897, l6: 0.307164\n",
            "\n",
            "[epoch:  18/100000, batch:   348/  840, ite: 1219] train loss: 2.192402, tar: 0.290956 \n",
            "l0: 0.241497, l1: 0.281533, l2: 0.251000, l3: 0.251056, l4: 0.252634, l5: 0.254487, l6: 0.282729\n",
            "\n",
            "[epoch:  18/100000, batch:   360/  840, ite: 1220] train loss: 2.192093, tar: 0.290915 \n",
            "l0: 0.134605, l1: 0.138751, l2: 0.134884, l3: 0.152578, l4: 0.158168, l5: 0.186162, l6: 0.209023\n",
            "\n",
            "[epoch:  18/100000, batch:   372/  840, ite: 1221] train loss: 2.191210, tar: 0.290787 \n",
            "l0: 0.270567, l1: 0.264765, l2: 0.278938, l3: 0.279464, l4: 0.288693, l5: 0.295509, l6: 0.344397\n",
            "\n",
            "[epoch:  18/100000, batch:   384/  840, ite: 1222] train loss: 2.191072, tar: 0.290771 \n",
            "l0: 0.217233, l1: 0.209208, l2: 0.217100, l3: 0.209431, l4: 0.227485, l5: 0.245681, l6: 0.300531\n",
            "\n",
            "[epoch:  18/100000, batch:   396/  840, ite: 1223] train loss: 2.190610, tar: 0.290711 \n",
            "l0: 0.223070, l1: 0.227189, l2: 0.237606, l3: 0.235067, l4: 0.240359, l5: 0.236630, l6: 0.276993\n",
            "\n",
            "[epoch:  18/100000, batch:   408/  840, ite: 1224] train loss: 2.190191, tar: 0.290655 \n",
            "l0: 0.169287, l1: 0.182926, l2: 0.181293, l3: 0.177876, l4: 0.179293, l5: 0.187836, l6: 0.225532\n",
            "\n",
            "[epoch:  18/100000, batch:   420/  840, ite: 1225] train loss: 2.189467, tar: 0.290556 \n",
            "l0: 0.226875, l1: 0.224093, l2: 0.241104, l3: 0.241769, l4: 0.231424, l5: 0.250965, l6: 0.299599\n",
            "\n",
            "[epoch:  18/100000, batch:   432/  840, ite: 1226] train loss: 2.189081, tar: 0.290504 \n",
            "l0: 0.165574, l1: 0.183546, l2: 0.176048, l3: 0.164293, l4: 0.158032, l5: 0.185390, l6: 0.258929\n",
            "\n",
            "[epoch:  18/100000, batch:   444/  840, ite: 1227] train loss: 2.188349, tar: 0.290403 \n",
            "l0: 0.230251, l1: 0.218098, l2: 0.229685, l3: 0.227322, l4: 0.232882, l5: 0.276985, l6: 0.343427\n",
            "\n",
            "[epoch:  18/100000, batch:   456/  840, ite: 1228] train loss: 2.187999, tar: 0.290354 \n",
            "l0: 0.262992, l1: 0.235133, l2: 0.260575, l3: 0.276341, l4: 0.291730, l5: 0.333224, l6: 0.358162\n",
            "\n",
            "[epoch:  18/100000, batch:   468/  840, ite: 1229] train loss: 2.187861, tar: 0.290331 \n",
            "l0: 0.234939, l1: 0.222136, l2: 0.239930, l3: 0.268627, l4: 0.264042, l5: 0.284465, l6: 0.326592\n",
            "\n",
            "[epoch:  18/100000, batch:   480/  840, ite: 1230] train loss: 2.187579, tar: 0.290286 \n",
            "l0: 0.158413, l1: 0.152242, l2: 0.158780, l3: 0.183184, l4: 0.196382, l5: 0.197596, l6: 0.227027\n",
            "\n",
            "[epoch:  18/100000, batch:   492/  840, ite: 1231] train loss: 2.186837, tar: 0.290179 \n",
            "l0: 0.167943, l1: 0.172850, l2: 0.167010, l3: 0.182042, l4: 0.181739, l5: 0.183236, l6: 0.235524\n",
            "\n",
            "[epoch:  18/100000, batch:   504/  840, ite: 1232] train loss: 2.186109, tar: 0.290080 \n",
            "l0: 0.209463, l1: 0.209866, l2: 0.199145, l3: 0.237782, l4: 0.234206, l5: 0.280385, l6: 0.339364\n",
            "\n",
            "[epoch:  18/100000, batch:   516/  840, ite: 1233] train loss: 2.185723, tar: 0.290015 \n",
            "l0: 0.209543, l1: 0.213846, l2: 0.203329, l3: 0.213244, l4: 0.216302, l5: 0.257959, l6: 0.338768\n",
            "\n",
            "[epoch:  18/100000, batch:   528/  840, ite: 1234] train loss: 2.185292, tar: 0.289949 \n",
            "l0: 0.114272, l1: 0.116452, l2: 0.118449, l3: 0.128347, l4: 0.130104, l5: 0.146893, l6: 0.179386\n",
            "\n",
            "[epoch:  18/100000, batch:   540/  840, ite: 1235] train loss: 2.184278, tar: 0.289807 \n",
            "l0: 0.162042, l1: 0.162878, l2: 0.159055, l3: 0.177918, l4: 0.181789, l5: 0.201655, l6: 0.251670\n",
            "\n",
            "[epoch:  18/100000, batch:   552/  840, ite: 1236] train loss: 2.183560, tar: 0.289704 \n",
            "l0: 0.217265, l1: 0.214327, l2: 0.225860, l3: 0.209338, l4: 0.209727, l5: 0.225792, l6: 0.262870\n",
            "\n",
            "[epoch:  18/100000, batch:   564/  840, ite: 1237] train loss: 2.183060, tar: 0.289645 \n",
            "l0: 0.216616, l1: 0.205343, l2: 0.227128, l3: 0.214583, l4: 0.217549, l5: 0.226703, l6: 0.260816\n",
            "\n",
            "[epoch:  18/100000, batch:   576/  840, ite: 1238] train loss: 2.182564, tar: 0.289586 \n",
            "l0: 0.186829, l1: 0.185296, l2: 0.190787, l3: 0.198078, l4: 0.210540, l5: 0.210889, l6: 0.234086\n",
            "\n",
            "[epoch:  18/100000, batch:   588/  840, ite: 1239] train loss: 2.181946, tar: 0.289503 \n",
            "l0: 0.282944, l1: 0.296097, l2: 0.282535, l3: 0.295423, l4: 0.286537, l5: 0.305102, l6: 0.356933\n",
            "\n",
            "[epoch:  18/100000, batch:   600/  840, ite: 1240] train loss: 2.181884, tar: 0.289498 \n",
            "l0: 0.137803, l1: 0.142008, l2: 0.136680, l3: 0.140972, l4: 0.145106, l5: 0.165990, l6: 0.227888\n",
            "\n",
            "[epoch:  18/100000, batch:   612/  840, ite: 1241] train loss: 2.181010, tar: 0.289376 \n",
            "l0: 0.266800, l1: 0.276679, l2: 0.286302, l3: 0.286452, l4: 0.281742, l5: 0.302466, l6: 0.312523\n",
            "\n",
            "[epoch:  18/100000, batch:   624/  840, ite: 1242] train loss: 2.180874, tar: 0.289358 \n",
            "l0: 0.267181, l1: 0.303941, l2: 0.270351, l3: 0.282927, l4: 0.291482, l5: 0.286069, l6: 0.315496\n",
            "\n",
            "[epoch:  18/100000, batch:   636/  840, ite: 1243] train loss: 2.180743, tar: 0.289340 \n",
            "l0: 0.208745, l1: 0.210692, l2: 0.208342, l3: 0.243076, l4: 0.243092, l5: 0.272669, l6: 0.320398\n",
            "\n",
            "[epoch:  18/100000, batch:   648/  840, ite: 1244] train loss: 2.180362, tar: 0.289275 \n",
            "l0: 0.263811, l1: 0.266150, l2: 0.266544, l3: 0.285645, l4: 0.295889, l5: 0.309304, l6: 0.334468\n",
            "\n",
            "[epoch:  18/100000, batch:   660/  840, ite: 1245] train loss: 2.180235, tar: 0.289255 \n",
            "l0: 0.264946, l1: 0.259597, l2: 0.280551, l3: 0.271296, l4: 0.280648, l5: 0.287626, l6: 0.322143\n",
            "\n",
            "[epoch:  18/100000, batch:   672/  840, ite: 1246] train loss: 2.180063, tar: 0.289235 \n",
            "l0: 0.260234, l1: 0.251239, l2: 0.271660, l3: 0.275704, l4: 0.273095, l5: 0.284042, l6: 0.310212\n",
            "\n",
            "[epoch:  18/100000, batch:   684/  840, ite: 1247] train loss: 2.179860, tar: 0.289212 \n",
            "l0: 0.182691, l1: 0.177053, l2: 0.193377, l3: 0.194151, l4: 0.197110, l5: 0.215936, l6: 0.258527\n",
            "\n",
            "[epoch:  18/100000, batch:   696/  840, ite: 1248] train loss: 2.179250, tar: 0.289126 \n",
            "l0: 0.311544, l1: 0.301449, l2: 0.322534, l3: 0.370896, l4: 0.359194, l5: 0.356344, l6: 0.376774\n",
            "\n",
            "[epoch:  18/100000, batch:   708/  840, ite: 1249] train loss: 2.179426, tar: 0.289144 \n",
            "l0: 0.245106, l1: 0.263211, l2: 0.252614, l3: 0.250956, l4: 0.248991, l5: 0.258583, l6: 0.283085\n",
            "\n",
            "[epoch:  18/100000, batch:   720/  840, ite: 1250] train loss: 2.179124, tar: 0.289109 \n",
            "l0: 0.198697, l1: 0.199654, l2: 0.201829, l3: 0.206229, l4: 0.220622, l5: 0.223442, l6: 0.263752\n",
            "\n",
            "[epoch:  18/100000, batch:   732/  840, ite: 1251] train loss: 2.178593, tar: 0.289037 \n",
            "l0: 0.253386, l1: 0.260777, l2: 0.249379, l3: 0.271992, l4: 0.283518, l5: 0.303444, l6: 0.328764\n",
            "\n",
            "[epoch:  18/100000, batch:   744/  840, ite: 1252] train loss: 2.178411, tar: 0.289008 \n",
            "l0: 0.253035, l1: 0.277073, l2: 0.238781, l3: 0.272769, l4: 0.275202, l5: 0.296854, l6: 0.332697\n",
            "\n",
            "[epoch:  18/100000, batch:   756/  840, ite: 1253] train loss: 2.178226, tar: 0.288980 \n",
            "l0: 0.149559, l1: 0.152359, l2: 0.154486, l3: 0.160293, l4: 0.165637, l5: 0.175445, l6: 0.217282\n",
            "\n",
            "[epoch:  18/100000, batch:   768/  840, ite: 1254] train loss: 2.177426, tar: 0.288869 \n",
            "l0: 0.272424, l1: 0.254490, l2: 0.296519, l3: 0.291284, l4: 0.293126, l5: 0.302384, l6: 0.322646\n",
            "\n",
            "[epoch:  18/100000, batch:   780/  840, ite: 1255] train loss: 2.177311, tar: 0.288855 \n",
            "l0: 0.230558, l1: 0.242633, l2: 0.240954, l3: 0.245489, l4: 0.240804, l5: 0.252536, l6: 0.284429\n",
            "\n",
            "[epoch:  18/100000, batch:   792/  840, ite: 1256] train loss: 2.176960, tar: 0.288809 \n",
            "l0: 0.298123, l1: 0.292380, l2: 0.304561, l3: 0.353154, l4: 0.349012, l5: 0.348552, l6: 0.370198\n",
            "\n",
            "[epoch:  18/100000, batch:   804/  840, ite: 1257] train loss: 2.177071, tar: 0.288816 \n",
            "l0: 0.163092, l1: 0.168790, l2: 0.162297, l3: 0.187811, l4: 0.191068, l5: 0.207489, l6: 0.238110\n",
            "\n",
            "[epoch:  18/100000, batch:   816/  840, ite: 1258] train loss: 2.176389, tar: 0.288716 \n",
            "l0: 0.299588, l1: 0.292056, l2: 0.327367, l3: 0.318956, l4: 0.324948, l5: 0.326297, l6: 0.341552\n",
            "\n",
            "[epoch:  18/100000, batch:   828/  840, ite: 1259] train loss: 2.176432, tar: 0.288725 \n",
            "l0: 0.162609, l1: 0.160242, l2: 0.176130, l3: 0.173497, l4: 0.177746, l5: 0.181800, l6: 0.210350\n",
            "\n",
            "[epoch:  18/100000, batch:   840/  840, ite: 1260] train loss: 2.175691, tar: 0.288625 \n",
            "l0: 0.268884, l1: 0.276116, l2: 0.271550, l3: 0.308110, l4: 0.298568, l5: 0.305802, l6: 0.323380\n",
            "\n",
            "[epoch:  19/100000, batch:    12/  840, ite: 1261] train loss: 2.175593, tar: 0.288609 \n",
            "l0: 0.211159, l1: 0.210264, l2: 0.215481, l3: 0.257481, l4: 0.256181, l5: 0.258975, l6: 0.282543\n",
            "\n",
            "[epoch:  19/100000, batch:    24/  840, ite: 1262] train loss: 2.175210, tar: 0.288548 \n",
            "l0: 0.275085, l1: 0.294805, l2: 0.302622, l3: 0.277595, l4: 0.270152, l5: 0.276917, l6: 0.303182\n",
            "\n",
            "[epoch:  19/100000, batch:    36/  840, ite: 1263] train loss: 2.175071, tar: 0.288537 \n",
            "l0: 0.198528, l1: 0.197589, l2: 0.208972, l3: 0.215925, l4: 0.216245, l5: 0.252751, l6: 0.287844\n",
            "\n",
            "[epoch:  19/100000, batch:    48/  840, ite: 1264] train loss: 2.174599, tar: 0.288466 \n",
            "l0: 0.164686, l1: 0.179238, l2: 0.165018, l3: 0.192112, l4: 0.196264, l5: 0.208250, l6: 0.242311\n",
            "\n",
            "[epoch:  19/100000, batch:    60/  840, ite: 1265] train loss: 2.173945, tar: 0.288368 \n",
            "l0: 0.214574, l1: 0.227764, l2: 0.212933, l3: 0.229322, l4: 0.240202, l5: 0.256062, l6: 0.301829\n",
            "\n",
            "[epoch:  19/100000, batch:    72/  840, ite: 1266] train loss: 2.173557, tar: 0.288310 \n",
            "l0: 0.303810, l1: 0.285706, l2: 0.305214, l3: 0.306884, l4: 0.341256, l5: 0.364975, l6: 0.363345\n",
            "\n",
            "[epoch:  19/100000, batch:    84/  840, ite: 1267] train loss: 2.173634, tar: 0.288322 \n",
            "l0: 0.156092, l1: 0.161093, l2: 0.158940, l3: 0.176222, l4: 0.178549, l5: 0.198065, l6: 0.237446\n",
            "\n",
            "[epoch:  19/100000, batch:    96/  840, ite: 1268] train loss: 2.172919, tar: 0.288218 \n",
            "l0: 0.163119, l1: 0.152810, l2: 0.171279, l3: 0.200004, l4: 0.204793, l5: 0.246798, l6: 0.268370\n",
            "\n",
            "[epoch:  19/100000, batch:   108/  840, ite: 1269] train loss: 2.172315, tar: 0.288119 \n",
            "l0: 0.207479, l1: 0.200046, l2: 0.203754, l3: 0.258853, l4: 0.259605, l5: 0.298693, l6: 0.366283\n",
            "\n",
            "[epoch:  19/100000, batch:   120/  840, ite: 1270] train loss: 2.172018, tar: 0.288056 \n",
            "l0: 0.214142, l1: 0.228763, l2: 0.218605, l3: 0.233591, l4: 0.235105, l5: 0.254482, l6: 0.286037\n",
            "\n",
            "[epoch:  19/100000, batch:   132/  840, ite: 1271] train loss: 2.171623, tar: 0.287998 \n",
            "l0: 0.261569, l1: 0.262553, l2: 0.263163, l3: 0.265965, l4: 0.275781, l5: 0.303818, l6: 0.345143\n",
            "\n",
            "[epoch:  19/100000, batch:   144/  840, ite: 1272] train loss: 2.171471, tar: 0.287977 \n",
            "l0: 0.259476, l1: 0.266013, l2: 0.256000, l3: 0.286541, l4: 0.290322, l5: 0.315363, l6: 0.352721\n",
            "\n",
            "[epoch:  19/100000, batch:   156/  840, ite: 1273] train loss: 2.171357, tar: 0.287955 \n",
            "l0: 0.180915, l1: 0.188139, l2: 0.192548, l3: 0.189486, l4: 0.199624, l5: 0.208446, l6: 0.234604\n",
            "\n",
            "[epoch:  19/100000, batch:   168/  840, ite: 1274] train loss: 2.170747, tar: 0.287870 \n",
            "l0: 0.226831, l1: 0.231585, l2: 0.230618, l3: 0.247918, l4: 0.257930, l5: 0.267141, l6: 0.297999\n",
            "\n",
            "[epoch:  19/100000, batch:   180/  840, ite: 1275] train loss: 2.170425, tar: 0.287823 \n",
            "l0: 0.267663, l1: 0.280803, l2: 0.283736, l3: 0.282473, l4: 0.279307, l5: 0.307351, l6: 0.349062\n",
            "\n",
            "[epoch:  19/100000, batch:   192/  840, ite: 1276] train loss: 2.170331, tar: 0.287807 \n",
            "l0: 0.199028, l1: 0.195395, l2: 0.203506, l3: 0.195510, l4: 0.196056, l5: 0.224468, l6: 0.273642\n",
            "\n",
            "[epoch:  19/100000, batch:   204/  840, ite: 1277] train loss: 2.169796, tar: 0.287737 \n",
            "l0: 0.217308, l1: 0.224888, l2: 0.227829, l3: 0.223211, l4: 0.222900, l5: 0.247556, l6: 0.296730\n",
            "\n",
            "[epoch:  19/100000, batch:   216/  840, ite: 1278] train loss: 2.169397, tar: 0.287682 \n",
            "l0: 0.115970, l1: 0.108002, l2: 0.126422, l3: 0.144183, l4: 0.153790, l5: 0.168480, l6: 0.182636\n",
            "\n",
            "[epoch:  19/100000, batch:   228/  840, ite: 1279] train loss: 2.168483, tar: 0.287548 \n",
            "l0: 0.133967, l1: 0.137183, l2: 0.134616, l3: 0.153904, l4: 0.160965, l5: 0.165451, l6: 0.205464\n",
            "\n",
            "[epoch:  19/100000, batch:   240/  840, ite: 1280] train loss: 2.167641, tar: 0.287428 \n",
            "l0: 0.214293, l1: 0.201073, l2: 0.228135, l3: 0.241018, l4: 0.244032, l5: 0.253815, l6: 0.279912\n",
            "\n",
            "[epoch:  19/100000, batch:   252/  840, ite: 1281] train loss: 2.167247, tar: 0.287371 \n",
            "l0: 0.142239, l1: 0.143696, l2: 0.145122, l3: 0.164037, l4: 0.166825, l5: 0.178635, l6: 0.211135\n",
            "\n",
            "[epoch:  19/100000, batch:   264/  840, ite: 1282] train loss: 2.166455, tar: 0.287258 \n",
            "l0: 0.134875, l1: 0.141132, l2: 0.144036, l3: 0.145092, l4: 0.149563, l5: 0.161860, l6: 0.203348\n",
            "\n",
            "[epoch:  19/100000, batch:   276/  840, ite: 1283] train loss: 2.165608, tar: 0.287139 \n",
            "l0: 0.440050, l1: 0.417854, l2: 0.449151, l3: 0.479642, l4: 0.483686, l5: 0.508157, l6: 0.515937\n",
            "\n",
            "[epoch:  19/100000, batch:   288/  840, ite: 1284] train loss: 2.166487, tar: 0.287258 \n",
            "l0: 0.180061, l1: 0.174234, l2: 0.188958, l3: 0.199028, l4: 0.199685, l5: 0.216679, l6: 0.247494\n",
            "\n",
            "[epoch:  19/100000, batch:   300/  840, ite: 1285] train loss: 2.165895, tar: 0.287175 \n",
            "l0: 0.283211, l1: 0.274704, l2: 0.286489, l3: 0.323551, l4: 0.329513, l5: 0.334742, l6: 0.349818\n",
            "\n",
            "[epoch:  19/100000, batch:   312/  840, ite: 1286] train loss: 2.165908, tar: 0.287171 \n",
            "l0: 0.154693, l1: 0.149980, l2: 0.159612, l3: 0.175570, l4: 0.185053, l5: 0.191967, l6: 0.233620\n",
            "\n",
            "[epoch:  19/100000, batch:   324/  840, ite: 1287] train loss: 2.165197, tar: 0.287069 \n",
            "l0: 0.274542, l1: 0.266176, l2: 0.269745, l3: 0.331008, l4: 0.328666, l5: 0.342746, l6: 0.414857\n",
            "\n",
            "[epoch:  19/100000, batch:   336/  840, ite: 1288] train loss: 2.165245, tar: 0.287059 \n",
            "l0: 0.210053, l1: 0.210963, l2: 0.221835, l3: 0.235844, l4: 0.233716, l5: 0.240243, l6: 0.279719\n",
            "\n",
            "[epoch:  19/100000, batch:   348/  840, ite: 1289] train loss: 2.164832, tar: 0.286999 \n",
            "l0: 0.344110, l1: 0.319450, l2: 0.332112, l3: 0.390849, l4: 0.414617, l5: 0.447641, l6: 0.524931\n",
            "\n",
            "[epoch:  19/100000, batch:   360/  840, ite: 1290] train loss: 2.165304, tar: 0.287043 \n",
            "l0: 0.212437, l1: 0.216778, l2: 0.226717, l3: 0.219743, l4: 0.223646, l5: 0.229174, l6: 0.262115\n",
            "\n",
            "[epoch:  19/100000, batch:   372/  840, ite: 1291] train loss: 2.164859, tar: 0.286986 \n",
            "l0: 0.224192, l1: 0.218512, l2: 0.237510, l3: 0.248487, l4: 0.248731, l5: 0.259003, l6: 0.306967\n",
            "\n",
            "[epoch:  19/100000, batch:   384/  840, ite: 1292] train loss: 2.164532, tar: 0.286937 \n",
            "l0: 0.161137, l1: 0.165314, l2: 0.176982, l3: 0.174157, l4: 0.178058, l5: 0.180170, l6: 0.208745\n",
            "\n",
            "[epoch:  19/100000, batch:   396/  840, ite: 1293] train loss: 2.163821, tar: 0.286840 \n",
            "l0: 0.146693, l1: 0.148249, l2: 0.152624, l3: 0.152485, l4: 0.159853, l5: 0.183336, l6: 0.235234\n",
            "\n",
            "[epoch:  19/100000, batch:   408/  840, ite: 1294] train loss: 2.163059, tar: 0.286731 \n",
            "l0: 0.306114, l1: 0.294237, l2: 0.330174, l3: 0.321122, l4: 0.316669, l5: 0.346817, l6: 0.409539\n",
            "\n",
            "[epoch:  19/100000, batch:   420/  840, ite: 1295] train loss: 2.163184, tar: 0.286746 \n",
            "l0: 0.180363, l1: 0.185828, l2: 0.187271, l3: 0.209434, l4: 0.207356, l5: 0.217159, l6: 0.265893\n",
            "\n",
            "[epoch:  19/100000, batch:   432/  840, ite: 1296] train loss: 2.162637, tar: 0.286664 \n",
            "l0: 0.252329, l1: 0.266402, l2: 0.258235, l3: 0.263805, l4: 0.265980, l5: 0.278551, l6: 0.295475\n",
            "\n",
            "[epoch:  19/100000, batch:   444/  840, ite: 1297] train loss: 2.162419, tar: 0.286638 \n",
            "l0: 0.183147, l1: 0.180862, l2: 0.193102, l3: 0.221296, l4: 0.223063, l5: 0.212487, l6: 0.239699\n",
            "\n",
            "[epoch:  19/100000, batch:   456/  840, ite: 1298] train loss: 2.161873, tar: 0.286558 \n",
            "l0: 0.245300, l1: 0.250579, l2: 0.260488, l3: 0.276995, l4: 0.279390, l5: 0.268106, l6: 0.291013\n",
            "\n",
            "[epoch:  19/100000, batch:   468/  840, ite: 1299] train loss: 2.161650, tar: 0.286526 \n",
            "l0: 0.204953, l1: 0.197868, l2: 0.223129, l3: 0.205636, l4: 0.215380, l5: 0.232078, l6: 0.285552\n",
            "\n",
            "[epoch:  19/100000, batch:   480/  840, ite: 1300] train loss: 2.161191, tar: 0.286463 \n",
            "l0: 0.184476, l1: 0.169591, l2: 0.195702, l3: 0.204332, l4: 0.216949, l5: 0.218680, l6: 0.258530\n",
            "\n",
            "[epoch:  19/100000, batch:   492/  840, ite: 1301] train loss: 2.160643, tar: 0.286385 \n",
            "l0: 0.158877, l1: 0.168974, l2: 0.165599, l3: 0.178271, l4: 0.191317, l5: 0.182994, l6: 0.216042\n",
            "\n",
            "[epoch:  19/100000, batch:   504/  840, ite: 1302] train loss: 2.159953, tar: 0.286287 \n",
            "l0: 0.207772, l1: 0.240905, l2: 0.214903, l3: 0.216793, l4: 0.225083, l5: 0.206537, l6: 0.233580\n",
            "\n",
            "[epoch:  19/100000, batch:   516/  840, ite: 1303] train loss: 2.159481, tar: 0.286227 \n",
            "l0: 0.198029, l1: 0.205228, l2: 0.205545, l3: 0.216433, l4: 0.211514, l5: 0.238637, l6: 0.281642\n",
            "\n",
            "[epoch:  19/100000, batch:   528/  840, ite: 1304] train loss: 2.159019, tar: 0.286159 \n",
            "l0: 0.261783, l1: 0.250358, l2: 0.266498, l3: 0.281746, l4: 0.275785, l5: 0.332436, l6: 0.381009\n",
            "\n",
            "[epoch:  19/100000, batch:   540/  840, ite: 1305] train loss: 2.158935, tar: 0.286141 \n",
            "l0: 0.125810, l1: 0.130096, l2: 0.130504, l3: 0.150275, l4: 0.154367, l5: 0.152880, l6: 0.181638\n",
            "\n",
            "[epoch:  19/100000, batch:   552/  840, ite: 1306] train loss: 2.158068, tar: 0.286018 \n",
            "l0: 0.129601, l1: 0.138587, l2: 0.139934, l3: 0.149835, l4: 0.150376, l5: 0.173777, l6: 0.215403\n",
            "\n",
            "[epoch:  19/100000, batch:   564/  840, ite: 1307] train loss: 2.157256, tar: 0.285898 \n",
            "l0: 0.210462, l1: 0.223206, l2: 0.222826, l3: 0.209859, l4: 0.224722, l5: 0.226657, l6: 0.261559\n",
            "\n",
            "[epoch:  19/100000, batch:   576/  840, ite: 1308] train loss: 2.156814, tar: 0.285841 \n",
            "l0: 0.244497, l1: 0.257771, l2: 0.250334, l3: 0.258085, l4: 0.262469, l5: 0.282852, l6: 0.332702\n",
            "\n",
            "[epoch:  19/100000, batch:   588/  840, ite: 1309] train loss: 2.156610, tar: 0.285809 \n",
            "l0: 0.203402, l1: 0.224150, l2: 0.198691, l3: 0.223437, l4: 0.224888, l5: 0.227803, l6: 0.262006\n",
            "\n",
            "[epoch:  19/100000, batch:   600/  840, ite: 1310] train loss: 2.156157, tar: 0.285746 \n",
            "l0: 0.189358, l1: 0.199985, l2: 0.206169, l3: 0.191308, l4: 0.195002, l5: 0.206666, l6: 0.259056\n",
            "\n",
            "[epoch:  19/100000, batch:   612/  840, ite: 1311] train loss: 2.155617, tar: 0.285672 \n",
            "l0: 0.261317, l1: 0.266213, l2: 0.260882, l3: 0.269323, l4: 0.277883, l5: 0.296177, l6: 0.327655\n",
            "\n",
            "[epoch:  19/100000, batch:   624/  840, ite: 1312] train loss: 2.155467, tar: 0.285654 \n",
            "l0: 0.217240, l1: 0.215564, l2: 0.221235, l3: 0.236549, l4: 0.237253, l5: 0.251491, l6: 0.266975\n",
            "\n",
            "[epoch:  19/100000, batch:   636/  840, ite: 1313] train loss: 2.155080, tar: 0.285602 \n",
            "l0: 0.248290, l1: 0.246774, l2: 0.256192, l3: 0.264361, l4: 0.274809, l5: 0.294095, l6: 0.318065\n",
            "\n",
            "[epoch:  19/100000, batch:   648/  840, ite: 1314] train loss: 2.154887, tar: 0.285573 \n",
            "l0: 0.253598, l1: 0.253867, l2: 0.271943, l3: 0.281900, l4: 0.281427, l5: 0.291412, l6: 0.306556\n",
            "\n",
            "[epoch:  19/100000, batch:   660/  840, ite: 1315] train loss: 2.154724, tar: 0.285549 \n",
            "l0: 0.221582, l1: 0.233581, l2: 0.227568, l3: 0.262491, l4: 0.254235, l5: 0.261451, l6: 0.265795\n",
            "\n",
            "[epoch:  19/100000, batch:   672/  840, ite: 1316] train loss: 2.154399, tar: 0.285501 \n",
            "l0: 0.220759, l1: 0.243584, l2: 0.231457, l3: 0.213626, l4: 0.214440, l5: 0.239436, l6: 0.269394\n",
            "\n",
            "[epoch:  19/100000, batch:   684/  840, ite: 1317] train loss: 2.154003, tar: 0.285451 \n",
            "l0: 0.278024, l1: 0.290770, l2: 0.283732, l3: 0.314325, l4: 0.323149, l5: 0.338649, l6: 0.350606\n",
            "\n",
            "[epoch:  19/100000, batch:   696/  840, ite: 1318] train loss: 2.154022, tar: 0.285446 \n",
            "l0: 0.300089, l1: 0.285698, l2: 0.303210, l3: 0.315191, l4: 0.325123, l5: 0.362764, l6: 0.380370\n",
            "\n",
            "[epoch:  19/100000, batch:   708/  840, ite: 1319] train loss: 2.154112, tar: 0.285457 \n",
            "l0: 0.210278, l1: 0.214526, l2: 0.216198, l3: 0.222407, l4: 0.227418, l5: 0.242871, l6: 0.287636\n",
            "\n",
            "[epoch:  19/100000, batch:   720/  840, ite: 1320] train loss: 2.153708, tar: 0.285400 \n",
            "l0: 0.355126, l1: 0.353383, l2: 0.351467, l3: 0.357045, l4: 0.361554, l5: 0.349359, l6: 0.395791\n",
            "\n",
            "[epoch:  19/100000, batch:   732/  840, ite: 1321] train loss: 2.153989, tar: 0.285453 \n",
            "l0: 0.193163, l1: 0.201067, l2: 0.202728, l3: 0.218722, l4: 0.216647, l5: 0.212464, l6: 0.245505\n",
            "\n",
            "[epoch:  19/100000, batch:   744/  840, ite: 1322] train loss: 2.153486, tar: 0.285383 \n",
            "l0: 0.152756, l1: 0.150183, l2: 0.161219, l3: 0.166393, l4: 0.168406, l5: 0.194062, l6: 0.229972\n",
            "\n",
            "[epoch:  19/100000, batch:   756/  840, ite: 1323] train loss: 2.152783, tar: 0.285283 \n",
            "l0: 0.184207, l1: 0.192271, l2: 0.193731, l3: 0.197416, l4: 0.201465, l5: 0.224721, l6: 0.273767\n",
            "\n",
            "[epoch:  19/100000, batch:   768/  840, ite: 1324] train loss: 2.152266, tar: 0.285206 \n",
            "l0: 0.240997, l1: 0.247608, l2: 0.247395, l3: 0.241369, l4: 0.249947, l5: 0.255069, l6: 0.290198\n",
            "\n",
            "[epoch:  19/100000, batch:   780/  840, ite: 1325] train loss: 2.151979, tar: 0.285173 \n",
            "l0: 0.188802, l1: 0.189823, l2: 0.199130, l3: 0.201372, l4: 0.198461, l5: 0.219434, l6: 0.241314\n",
            "\n",
            "[epoch:  19/100000, batch:   792/  840, ite: 1326] train loss: 2.151441, tar: 0.285100 \n",
            "l0: 0.179782, l1: 0.183888, l2: 0.179941, l3: 0.199121, l4: 0.205422, l5: 0.230218, l6: 0.259979\n",
            "\n",
            "[epoch:  19/100000, batch:   804/  840, ite: 1327] train loss: 2.150903, tar: 0.285021 \n",
            "l0: 0.195748, l1: 0.193770, l2: 0.203245, l3: 0.217193, l4: 0.224596, l5: 0.231453, l6: 0.245579\n",
            "\n",
            "[epoch:  19/100000, batch:   816/  840, ite: 1328] train loss: 2.150422, tar: 0.284954 \n",
            "l0: 0.317520, l1: 0.296213, l2: 0.319189, l3: 0.360408, l4: 0.357087, l5: 0.391175, l6: 0.477255\n",
            "\n",
            "[epoch:  19/100000, batch:   828/  840, ite: 1329] train loss: 2.150699, tar: 0.284978 \n",
            "l0: 0.263608, l1: 0.258918, l2: 0.257466, l3: 0.267554, l4: 0.275028, l5: 0.309161, l6: 0.374789\n",
            "\n",
            "[epoch:  19/100000, batch:   840/  840, ite: 1330] train loss: 2.150591, tar: 0.284962 \n",
            "l0: 0.185945, l1: 0.190581, l2: 0.191032, l3: 0.191996, l4: 0.202323, l5: 0.208497, l6: 0.246437\n",
            "\n",
            "[epoch:  20/100000, batch:    12/  840, ite: 1331] train loss: 2.150039, tar: 0.284888 \n",
            "l0: 0.216545, l1: 0.223101, l2: 0.217870, l3: 0.242980, l4: 0.245687, l5: 0.251705, l6: 0.292294\n",
            "\n",
            "[epoch:  20/100000, batch:    24/  840, ite: 1332] train loss: 2.149694, tar: 0.284836 \n",
            "l0: 0.244143, l1: 0.250680, l2: 0.247155, l3: 0.265275, l4: 0.261957, l5: 0.273116, l6: 0.303918\n",
            "\n",
            "[epoch:  20/100000, batch:    36/  840, ite: 1333] train loss: 2.149466, tar: 0.284806 \n",
            "l0: 0.255759, l1: 0.246331, l2: 0.252317, l3: 0.263877, l4: 0.282790, l5: 0.302414, l6: 0.327214\n",
            "\n",
            "[epoch:  20/100000, batch:    48/  840, ite: 1334] train loss: 2.149302, tar: 0.284784 \n",
            "l0: 0.290669, l1: 0.300940, l2: 0.295975, l3: 0.313848, l4: 0.312907, l5: 0.297722, l6: 0.315738\n",
            "\n",
            "[epoch:  20/100000, batch:    60/  840, ite: 1335] train loss: 2.149286, tar: 0.284789 \n",
            "l0: 0.182861, l1: 0.181481, l2: 0.188329, l3: 0.199451, l4: 0.198889, l5: 0.218160, l6: 0.266106\n",
            "\n",
            "[epoch:  20/100000, batch:    72/  840, ite: 1336] train loss: 2.148752, tar: 0.284712 \n",
            "l0: 0.194879, l1: 0.188551, l2: 0.203274, l3: 0.208774, l4: 0.216957, l5: 0.226921, l6: 0.252417\n",
            "\n",
            "[epoch:  20/100000, batch:    84/  840, ite: 1337] train loss: 2.148260, tar: 0.284645 \n",
            "l0: 0.231279, l1: 0.234817, l2: 0.244005, l3: 0.253028, l4: 0.252648, l5: 0.259260, l6: 0.289837\n",
            "\n",
            "[epoch:  20/100000, batch:    96/  840, ite: 1338] train loss: 2.147974, tar: 0.284605 \n",
            "l0: 0.159964, l1: 0.150301, l2: 0.171056, l3: 0.173078, l4: 0.176489, l5: 0.188653, l6: 0.216076\n",
            "\n",
            "[epoch:  20/100000, batch:   108/  840, ite: 1339] train loss: 2.147292, tar: 0.284512 \n",
            "l0: 0.212160, l1: 0.234828, l2: 0.214892, l3: 0.225057, l4: 0.214325, l5: 0.222350, l6: 0.260613\n",
            "\n",
            "[epoch:  20/100000, batch:   120/  840, ite: 1340] train loss: 2.146872, tar: 0.284458 \n",
            "l0: 0.225967, l1: 0.232784, l2: 0.230100, l3: 0.222572, l4: 0.232472, l5: 0.242712, l6: 0.281361\n",
            "\n",
            "[epoch:  20/100000, batch:   132/  840, ite: 1341] train loss: 2.146515, tar: 0.284414 \n",
            "l0: 0.203688, l1: 0.219472, l2: 0.217202, l3: 0.207685, l4: 0.199296, l5: 0.210775, l6: 0.250219\n",
            "\n",
            "[epoch:  20/100000, batch:   144/  840, ite: 1342] train loss: 2.146039, tar: 0.284354 \n",
            "l0: 0.171150, l1: 0.168629, l2: 0.175529, l3: 0.206848, l4: 0.206400, l5: 0.220355, l6: 0.262806\n",
            "\n",
            "[epoch:  20/100000, batch:   156/  840, ite: 1343] train loss: 2.145493, tar: 0.284270 \n",
            "l0: 0.150545, l1: 0.151039, l2: 0.161788, l3: 0.182928, l4: 0.176825, l5: 0.183591, l6: 0.215939\n",
            "\n",
            "[epoch:  20/100000, batch:   168/  840, ite: 1344] train loss: 2.144806, tar: 0.284171 \n",
            "l0: 0.220014, l1: 0.211866, l2: 0.230570, l3: 0.254718, l4: 0.253097, l5: 0.261488, l6: 0.303664\n",
            "\n",
            "[epoch:  20/100000, batch:   180/  840, ite: 1345] train loss: 2.144501, tar: 0.284123 \n",
            "l0: 0.175641, l1: 0.165407, l2: 0.181310, l3: 0.205851, l4: 0.207981, l5: 0.219200, l6: 0.261340\n",
            "\n",
            "[epoch:  20/100000, batch:   192/  840, ite: 1346] train loss: 2.143961, tar: 0.284042 \n",
            "l0: 0.194292, l1: 0.191640, l2: 0.210358, l3: 0.201810, l4: 0.210638, l5: 0.236805, l6: 0.271847\n",
            "\n",
            "[epoch:  20/100000, batch:   204/  840, ite: 1347] train loss: 2.143496, tar: 0.283976 \n",
            "l0: 0.267520, l1: 0.257229, l2: 0.281803, l3: 0.312352, l4: 0.294203, l5: 0.296931, l6: 0.320812\n",
            "\n",
            "[epoch:  20/100000, batch:   216/  840, ite: 1348] train loss: 2.143412, tar: 0.283963 \n",
            "l0: 0.141180, l1: 0.139279, l2: 0.154029, l3: 0.167354, l4: 0.165248, l5: 0.178359, l6: 0.205778\n",
            "\n",
            "[epoch:  20/100000, batch:   228/  840, ite: 1349] train loss: 2.142676, tar: 0.283858 \n",
            "l0: 0.251401, l1: 0.253594, l2: 0.252688, l3: 0.286979, l4: 0.288105, l5: 0.276259, l6: 0.284436\n",
            "\n",
            "[epoch:  20/100000, batch:   240/  840, ite: 1350] train loss: 2.142492, tar: 0.283833 \n",
            "l0: 0.318728, l1: 0.331456, l2: 0.333419, l3: 0.330673, l4: 0.325084, l5: 0.338298, l6: 0.358098\n",
            "\n",
            "[epoch:  20/100000, batch:   252/  840, ite: 1351] train loss: 2.142635, tar: 0.283859 \n",
            "l0: 0.250205, l1: 0.262906, l2: 0.246932, l3: 0.266263, l4: 0.273435, l5: 0.277011, l6: 0.296981\n",
            "\n",
            "[epoch:  20/100000, batch:   264/  840, ite: 1352] train loss: 2.142436, tar: 0.283834 \n",
            "l0: 0.120383, l1: 0.128619, l2: 0.122418, l3: 0.142465, l4: 0.136493, l5: 0.166431, l6: 0.222181\n",
            "\n",
            "[epoch:  20/100000, batch:   276/  840, ite: 1353] train loss: 2.141620, tar: 0.283714 \n",
            "l0: 0.334045, l1: 0.327639, l2: 0.360603, l3: 0.331296, l4: 0.340828, l5: 0.342502, l6: 0.356617\n",
            "\n",
            "[epoch:  20/100000, batch:   288/  840, ite: 1354] train loss: 2.141807, tar: 0.283751 \n",
            "l0: 0.213184, l1: 0.221820, l2: 0.220332, l3: 0.247736, l4: 0.241837, l5: 0.237052, l6: 0.256898\n",
            "\n",
            "[epoch:  20/100000, batch:   300/  840, ite: 1355] train loss: 2.141435, tar: 0.283699 \n",
            "l0: 0.189992, l1: 0.195946, l2: 0.197268, l3: 0.193369, l4: 0.199090, l5: 0.213241, l6: 0.242013\n",
            "\n",
            "[epoch:  20/100000, batch:   312/  840, ite: 1356] train loss: 2.140911, tar: 0.283630 \n",
            "l0: 0.277635, l1: 0.290928, l2: 0.305171, l3: 0.314450, l4: 0.289638, l5: 0.301688, l6: 0.330217\n",
            "\n",
            "[epoch:  20/100000, batch:   324/  840, ite: 1357] train loss: 2.140888, tar: 0.283625 \n",
            "l0: 0.276004, l1: 0.294529, l2: 0.282099, l3: 0.255618, l4: 0.267851, l5: 0.286720, l6: 0.312723\n",
            "\n",
            "[epoch:  20/100000, batch:   336/  840, ite: 1358] train loss: 2.140767, tar: 0.283620 \n",
            "l0: 0.136236, l1: 0.134404, l2: 0.138602, l3: 0.155614, l4: 0.155767, l5: 0.155250, l6: 0.192835\n",
            "\n",
            "[epoch:  20/100000, batch:   348/  840, ite: 1359] train loss: 2.139978, tar: 0.283511 \n",
            "l0: 0.236508, l1: 0.248805, l2: 0.237424, l3: 0.263599, l4: 0.268342, l5: 0.261452, l6: 0.275770\n",
            "\n",
            "[epoch:  20/100000, batch:   360/  840, ite: 1360] train loss: 2.139722, tar: 0.283477 \n",
            "l0: 0.164043, l1: 0.165359, l2: 0.169841, l3: 0.186790, l4: 0.184059, l5: 0.188649, l6: 0.224274\n",
            "\n",
            "[epoch:  20/100000, batch:   372/  840, ite: 1361] train loss: 2.139092, tar: 0.283389 \n",
            "l0: 0.127166, l1: 0.132638, l2: 0.139522, l3: 0.142845, l4: 0.135450, l5: 0.141661, l6: 0.169368\n",
            "\n",
            "[epoch:  20/100000, batch:   384/  840, ite: 1362] train loss: 2.138248, tar: 0.283274 \n",
            "l0: 0.292152, l1: 0.295012, l2: 0.290818, l3: 0.305397, l4: 0.313708, l5: 0.333092, l6: 0.358135\n",
            "\n",
            "[epoch:  20/100000, batch:   396/  840, ite: 1363] train loss: 2.138284, tar: 0.283281 \n",
            "l0: 0.363186, l1: 0.366306, l2: 0.373869, l3: 0.372394, l4: 0.377494, l5: 0.386138, l6: 0.372967\n",
            "\n",
            "[epoch:  20/100000, batch:   408/  840, ite: 1364] train loss: 2.138632, tar: 0.283339 \n",
            "l0: 0.250555, l1: 0.266756, l2: 0.264447, l3: 0.270472, l4: 0.262326, l5: 0.252786, l6: 0.273535\n",
            "\n",
            "[epoch:  20/100000, batch:   420/  840, ite: 1365] train loss: 2.138414, tar: 0.283315 \n",
            "l0: 0.258191, l1: 0.266627, l2: 0.263377, l3: 0.269655, l4: 0.275479, l5: 0.274535, l6: 0.314189\n",
            "\n",
            "[epoch:  20/100000, batch:   432/  840, ite: 1366] train loss: 2.138255, tar: 0.283297 \n",
            "l0: 0.451137, l1: 0.477403, l2: 0.449946, l3: 0.448736, l4: 0.467341, l5: 0.465780, l6: 0.447789\n",
            "\n",
            "[epoch:  20/100000, batch:   444/  840, ite: 1367] train loss: 2.139038, tar: 0.283420 \n",
            "l0: 0.206533, l1: 0.219268, l2: 0.220449, l3: 0.201731, l4: 0.207925, l5: 0.211466, l6: 0.252816\n",
            "\n",
            "[epoch:  20/100000, batch:   456/  840, ite: 1368] train loss: 2.138586, tar: 0.283363 \n",
            "l0: 0.169703, l1: 0.173091, l2: 0.184343, l3: 0.181785, l4: 0.188831, l5: 0.196688, l6: 0.266357\n",
            "\n",
            "[epoch:  20/100000, batch:   468/  840, ite: 1369] train loss: 2.138018, tar: 0.283280 \n",
            "l0: 0.125705, l1: 0.133549, l2: 0.128341, l3: 0.131271, l4: 0.141469, l5: 0.148854, l6: 0.200854\n",
            "\n",
            "[epoch:  20/100000, batch:   480/  840, ite: 1370] train loss: 2.137194, tar: 0.283165 \n",
            "l0: 0.210644, l1: 0.214167, l2: 0.217593, l3: 0.210554, l4: 0.225677, l5: 0.242356, l6: 0.274234\n",
            "\n",
            "[epoch:  20/100000, batch:   492/  840, ite: 1371] train loss: 2.136799, tar: 0.283112 \n",
            "l0: 0.301562, l1: 0.325512, l2: 0.301678, l3: 0.313429, l4: 0.305248, l5: 0.324316, l6: 0.371020\n",
            "\n",
            "[epoch:  20/100000, batch:   504/  840, ite: 1372] train loss: 2.136876, tar: 0.283126 \n",
            "l0: 0.191264, l1: 0.197400, l2: 0.202546, l3: 0.204801, l4: 0.210627, l5: 0.219887, l6: 0.239490\n",
            "\n",
            "[epoch:  20/100000, batch:   516/  840, ite: 1373] train loss: 2.136388, tar: 0.283059 \n",
            "l0: 0.304828, l1: 0.284639, l2: 0.325024, l3: 0.328239, l4: 0.346535, l5: 0.332132, l6: 0.342659\n",
            "\n",
            "[epoch:  20/100000, batch:   528/  840, ite: 1374] train loss: 2.136481, tar: 0.283075 \n",
            "l0: 0.146955, l1: 0.161023, l2: 0.155131, l3: 0.161006, l4: 0.154922, l5: 0.171126, l6: 0.214167\n",
            "\n",
            "[epoch:  20/100000, batch:   540/  840, ite: 1375] train loss: 2.135774, tar: 0.282976 \n",
            "l0: 0.188316, l1: 0.188531, l2: 0.198383, l3: 0.199242, l4: 0.206785, l5: 0.211522, l6: 0.236039\n",
            "\n",
            "[epoch:  20/100000, batch:   552/  840, ite: 1376] train loss: 2.135260, tar: 0.282907 \n",
            "l0: 0.302313, l1: 0.305785, l2: 0.329674, l3: 0.334845, l4: 0.311377, l5: 0.276030, l6: 0.321721\n",
            "\n",
            "[epoch:  20/100000, batch:   564/  840, ite: 1377] train loss: 2.135293, tar: 0.282921 \n",
            "l0: 0.175320, l1: 0.174071, l2: 0.204324, l3: 0.222357, l4: 0.205707, l5: 0.206792, l6: 0.234778\n",
            "\n",
            "[epoch:  20/100000, batch:   576/  840, ite: 1378] train loss: 2.134777, tar: 0.282843 \n",
            "l0: 0.254983, l1: 0.242858, l2: 0.261216, l3: 0.280902, l4: 0.300058, l5: 0.316258, l6: 0.374902\n",
            "\n",
            "[epoch:  20/100000, batch:   588/  840, ite: 1379] train loss: 2.134702, tar: 0.282823 \n",
            "l0: 0.501858, l1: 0.545041, l2: 0.525753, l3: 0.533715, l4: 0.513540, l5: 0.460108, l6: 0.441687\n",
            "\n",
            "[epoch:  20/100000, batch:   600/  840, ite: 1380] train loss: 2.135707, tar: 0.282982 \n",
            "l0: 0.160288, l1: 0.173834, l2: 0.169793, l3: 0.186736, l4: 0.184528, l5: 0.191759, l6: 0.229154\n",
            "\n",
            "[epoch:  20/100000, batch:   612/  840, ite: 1381] train loss: 2.135099, tar: 0.282893 \n",
            "l0: 0.194269, l1: 0.168213, l2: 0.231692, l3: 0.224610, l4: 0.221663, l5: 0.247356, l6: 0.274827\n",
            "\n",
            "[epoch:  20/100000, batch:   624/  840, ite: 1382] train loss: 2.134685, tar: 0.282829 \n",
            "l0: 0.159326, l1: 0.162177, l2: 0.159376, l3: 0.166004, l4: 0.176524, l5: 0.206956, l6: 0.264799\n",
            "\n",
            "[epoch:  20/100000, batch:   636/  840, ite: 1383] train loss: 2.134078, tar: 0.282739 \n",
            "l0: 0.189763, l1: 0.202471, l2: 0.202480, l3: 0.217780, l4: 0.217843, l5: 0.220966, l6: 0.248107\n",
            "\n",
            "[epoch:  20/100000, batch:   648/  840, ite: 1384] train loss: 2.133619, tar: 0.282672 \n",
            "l0: 0.276250, l1: 0.276523, l2: 0.306053, l3: 0.332955, l4: 0.308615, l5: 0.300924, l6: 0.325304\n",
            "\n",
            "[epoch:  20/100000, batch:   660/  840, ite: 1385] train loss: 2.133614, tar: 0.282667 \n",
            "l0: 0.145850, l1: 0.134411, l2: 0.152977, l3: 0.165493, l4: 0.176761, l5: 0.175466, l6: 0.214310\n",
            "\n",
            "[epoch:  20/100000, batch:   672/  840, ite: 1386] train loss: 2.132915, tar: 0.282569 \n",
            "l0: 0.137993, l1: 0.141933, l2: 0.146174, l3: 0.156211, l4: 0.165314, l5: 0.161360, l6: 0.214002\n",
            "\n",
            "[epoch:  20/100000, batch:   684/  840, ite: 1387] train loss: 2.132187, tar: 0.282465 \n",
            "l0: 0.289707, l1: 0.254446, l2: 0.303467, l3: 0.335896, l4: 0.353701, l5: 0.307705, l6: 0.343049\n",
            "\n",
            "[epoch:  20/100000, batch:   696/  840, ite: 1388] train loss: 2.132228, tar: 0.282470 \n",
            "l0: 0.222986, l1: 0.219313, l2: 0.246506, l3: 0.258946, l4: 0.251493, l5: 0.231837, l6: 0.264642\n",
            "\n",
            "[epoch:  20/100000, batch:   708/  840, ite: 1389] train loss: 2.131913, tar: 0.282427 \n",
            "l0: 0.216422, l1: 0.213656, l2: 0.226373, l3: 0.246026, l4: 0.240615, l5: 0.274473, l6: 0.330274\n",
            "\n",
            "[epoch:  20/100000, batch:   720/  840, ite: 1390] train loss: 2.131637, tar: 0.282379 \n",
            "l0: 0.274212, l1: 0.258842, l2: 0.305341, l3: 0.321785, l4: 0.312543, l5: 0.296107, l6: 0.338393\n",
            "\n",
            "[epoch:  20/100000, batch:   732/  840, ite: 1391] train loss: 2.131620, tar: 0.282374 \n",
            "l0: 0.198323, l1: 0.197716, l2: 0.216415, l3: 0.213798, l4: 0.212889, l5: 0.232324, l6: 0.276353\n",
            "\n",
            "[epoch:  20/100000, batch:   744/  840, ite: 1392] train loss: 2.131200, tar: 0.282313 \n",
            "l0: 0.266830, l1: 0.275576, l2: 0.255496, l3: 0.279503, l4: 0.299432, l5: 0.344634, l6: 0.417730\n",
            "\n",
            "[epoch:  20/100000, batch:   756/  840, ite: 1393] train loss: 2.131206, tar: 0.282302 \n",
            "l0: 0.235682, l1: 0.248338, l2: 0.230611, l3: 0.266739, l4: 0.270497, l5: 0.286655, l6: 0.368459\n",
            "\n",
            "[epoch:  20/100000, batch:   768/  840, ite: 1394] train loss: 2.131045, tar: 0.282269 \n",
            "l0: 0.189622, l1: 0.207066, l2: 0.200347, l3: 0.205010, l4: 0.209775, l5: 0.231922, l6: 0.277144\n",
            "\n",
            "[epoch:  20/100000, batch:   780/  840, ite: 1395] train loss: 2.130608, tar: 0.282202 \n",
            "l0: 0.233999, l1: 0.246484, l2: 0.227973, l3: 0.263664, l4: 0.286739, l5: 0.280677, l6: 0.314530\n",
            "\n",
            "[epoch:  20/100000, batch:   792/  840, ite: 1396] train loss: 2.130410, tar: 0.282168 \n",
            "l0: 0.187815, l1: 0.192705, l2: 0.188996, l3: 0.238032, l4: 0.253247, l5: 0.227383, l6: 0.260865\n",
            "\n",
            "[epoch:  20/100000, batch:   804/  840, ite: 1397] train loss: 2.129993, tar: 0.282100 \n",
            "l0: 0.141219, l1: 0.159019, l2: 0.142143, l3: 0.182426, l4: 0.193030, l5: 0.180084, l6: 0.207186\n",
            "\n",
            "[epoch:  20/100000, batch:   816/  840, ite: 1398] train loss: 2.129332, tar: 0.281999 \n",
            "l0: 0.201980, l1: 0.218242, l2: 0.195533, l3: 0.221877, l4: 0.228507, l5: 0.278026, l6: 0.341483\n",
            "\n",
            "[epoch:  20/100000, batch:   828/  840, ite: 1399] train loss: 2.129015, tar: 0.281942 \n",
            "l0: 0.166548, l1: 0.177355, l2: 0.173780, l3: 0.197811, l4: 0.206066, l5: 0.220844, l6: 0.264036\n",
            "\n",
            "[epoch:  20/100000, batch:   840/  840, ite: 1400] train loss: 2.128499, tar: 0.281860 \n",
            "l0: 0.233947, l1: 0.240333, l2: 0.230020, l3: 0.301842, l4: 0.308826, l5: 0.309177, l6: 0.343550\n",
            "\n",
            "[epoch:  21/100000, batch:    12/  840, ite: 1401] train loss: 2.128384, tar: 0.281825 \n",
            "l0: 0.240731, l1: 0.238690, l2: 0.249180, l3: 0.263698, l4: 0.267373, l5: 0.282245, l6: 0.334510\n",
            "\n",
            "[epoch:  21/100000, batch:    24/  840, ite: 1402] train loss: 2.128204, tar: 0.281796 \n",
            "l0: 0.174794, l1: 0.168078, l2: 0.195068, l3: 0.216885, l4: 0.214194, l5: 0.214977, l6: 0.238608\n",
            "\n",
            "[epoch:  21/100000, batch:    36/  840, ite: 1403] train loss: 2.127701, tar: 0.281720 \n",
            "l0: 0.137002, l1: 0.131332, l2: 0.145250, l3: 0.170836, l4: 0.170746, l5: 0.183419, l6: 0.231912\n",
            "\n",
            "[epoch:  21/100000, batch:    48/  840, ite: 1404] train loss: 2.127019, tar: 0.281617 \n",
            "l0: 0.196399, l1: 0.213356, l2: 0.194484, l3: 0.207166, l4: 0.212110, l5: 0.219355, l6: 0.260251\n",
            "\n",
            "[epoch:  21/100000, batch:    60/  840, ite: 1405] train loss: 2.126575, tar: 0.281556 \n",
            "l0: 0.286219, l1: 0.304634, l2: 0.279006, l3: 0.304653, l4: 0.322629, l5: 0.342365, l6: 0.387508\n",
            "\n",
            "[epoch:  21/100000, batch:    72/  840, ite: 1406] train loss: 2.126647, tar: 0.281560 \n",
            "l0: 0.162155, l1: 0.157071, l2: 0.174603, l3: 0.176839, l4: 0.177873, l5: 0.198631, l6: 0.228977\n",
            "\n",
            "[epoch:  21/100000, batch:    84/  840, ite: 1407] train loss: 2.126042, tar: 0.281475 \n",
            "l0: 0.228341, l1: 0.235391, l2: 0.239382, l3: 0.250223, l4: 0.253626, l5: 0.275526, l6: 0.292959\n",
            "\n",
            "[epoch:  21/100000, batch:    96/  840, ite: 1408] train loss: 2.125793, tar: 0.281437 \n",
            "l0: 0.112691, l1: 0.114215, l2: 0.117896, l3: 0.137355, l4: 0.139547, l5: 0.147732, l6: 0.185685\n",
            "\n",
            "[epoch:  21/100000, batch:   108/  840, ite: 1409] train loss: 2.124962, tar: 0.281317 \n",
            "l0: 0.185084, l1: 0.183565, l2: 0.189308, l3: 0.190539, l4: 0.195803, l5: 0.203743, l6: 0.258439\n",
            "\n",
            "[epoch:  21/100000, batch:   120/  840, ite: 1410] train loss: 2.124453, tar: 0.281249 \n",
            "l0: 0.199161, l1: 0.204281, l2: 0.201751, l3: 0.216813, l4: 0.217730, l5: 0.218443, l6: 0.282048\n",
            "\n",
            "[epoch:  21/100000, batch:   132/  840, ite: 1411] train loss: 2.124039, tar: 0.281191 \n",
            "l0: 0.224469, l1: 0.225924, l2: 0.222826, l3: 0.246890, l4: 0.245223, l5: 0.251717, l6: 0.320788\n",
            "\n",
            "[epoch:  21/100000, batch:   144/  840, ite: 1412] train loss: 2.123765, tar: 0.281151 \n",
            "l0: 0.590845, l1: 0.576766, l2: 0.560922, l3: 0.612797, l4: 0.670612, l5: 0.735862, l6: 0.749746\n",
            "\n",
            "[epoch:  21/100000, batch:   156/  840, ite: 1413] train loss: 2.125445, tar: 0.281370 \n",
            "l0: 0.270391, l1: 0.279179, l2: 0.279320, l3: 0.299467, l4: 0.303769, l5: 0.305316, l6: 0.324235\n",
            "\n",
            "[epoch:  21/100000, batch:   168/  840, ite: 1414] train loss: 2.125400, tar: 0.281362 \n",
            "l0: 0.144875, l1: 0.148943, l2: 0.152667, l3: 0.151426, l4: 0.149607, l5: 0.153561, l6: 0.189071\n",
            "\n",
            "[epoch:  21/100000, batch:   180/  840, ite: 1415] train loss: 2.124669, tar: 0.281265 \n",
            "l0: 0.168594, l1: 0.164069, l2: 0.185890, l3: 0.166518, l4: 0.174411, l5: 0.191241, l6: 0.224220\n",
            "\n",
            "[epoch:  21/100000, batch:   192/  840, ite: 1416] train loss: 2.124068, tar: 0.281186 \n",
            "l0: 0.208608, l1: 0.192857, l2: 0.224175, l3: 0.213601, l4: 0.226431, l5: 0.258993, l6: 0.306402\n",
            "\n",
            "[epoch:  21/100000, batch:   204/  840, ite: 1417] train loss: 2.123721, tar: 0.281135 \n",
            "l0: 0.162237, l1: 0.172075, l2: 0.173581, l3: 0.158452, l4: 0.167841, l5: 0.185339, l6: 0.229048\n",
            "\n",
            "[epoch:  21/100000, batch:   216/  840, ite: 1418] train loss: 2.123103, tar: 0.281051 \n",
            "l0: 0.292152, l1: 0.281553, l2: 0.289291, l3: 0.276825, l4: 0.306550, l5: 0.340190, l6: 0.393919\n",
            "\n",
            "[epoch:  21/100000, batch:   228/  840, ite: 1419] train loss: 2.123144, tar: 0.281059 \n",
            "l0: 0.192398, l1: 0.205995, l2: 0.203645, l3: 0.219925, l4: 0.206492, l5: 0.235184, l6: 0.273161\n",
            "\n",
            "[epoch:  21/100000, batch:   240/  840, ite: 1420] train loss: 2.122731, tar: 0.280996 \n",
            "l0: 0.172362, l1: 0.183319, l2: 0.183951, l3: 0.169068, l4: 0.172963, l5: 0.197386, l6: 0.241169\n",
            "\n",
            "[epoch:  21/100000, batch:   252/  840, ite: 1421] train loss: 2.122166, tar: 0.280920 \n",
            "l0: 0.200035, l1: 0.192509, l2: 0.211181, l3: 0.221771, l4: 0.231920, l5: 0.242853, l6: 0.300108\n",
            "\n",
            "[epoch:  21/100000, batch:   264/  840, ite: 1422] train loss: 2.121799, tar: 0.280863 \n",
            "l0: 0.209073, l1: 0.200334, l2: 0.213322, l3: 0.240660, l4: 0.238526, l5: 0.279412, l6: 0.325019\n",
            "\n",
            "[epoch:  21/100000, batch:   276/  840, ite: 1423] train loss: 2.121507, tar: 0.280812 \n",
            "l0: 0.251211, l1: 0.266966, l2: 0.244196, l3: 0.255399, l4: 0.263198, l5: 0.281398, l6: 0.335065\n",
            "\n",
            "[epoch:  21/100000, batch:   288/  840, ite: 1424] train loss: 2.121350, tar: 0.280792 \n",
            "l0: 0.141041, l1: 0.143025, l2: 0.147102, l3: 0.153324, l4: 0.158079, l5: 0.176914, l6: 0.197336\n",
            "\n",
            "[epoch:  21/100000, batch:   300/  840, ite: 1425] train loss: 2.120645, tar: 0.280694 \n",
            "l0: 0.278000, l1: 0.297317, l2: 0.296912, l3: 0.259952, l4: 0.264731, l5: 0.269207, l6: 0.302894\n",
            "\n",
            "[epoch:  21/100000, batch:   312/  840, ite: 1426] train loss: 2.120538, tar: 0.280692 \n",
            "l0: 0.194327, l1: 0.204044, l2: 0.192559, l3: 0.204286, l4: 0.216734, l5: 0.230081, l6: 0.273278\n",
            "\n",
            "[epoch:  21/100000, batch:   324/  840, ite: 1427] train loss: 2.120114, tar: 0.280631 \n",
            "l0: 0.265555, l1: 0.256277, l2: 0.274688, l3: 0.286798, l4: 0.290074, l5: 0.304864, l6: 0.331530\n",
            "\n",
            "[epoch:  21/100000, batch:   336/  840, ite: 1428] train loss: 2.120037, tar: 0.280621 \n",
            "l0: 0.148820, l1: 0.158493, l2: 0.155112, l3: 0.149167, l4: 0.154536, l5: 0.165072, l6: 0.192990\n",
            "\n",
            "[epoch:  21/100000, batch:   348/  840, ite: 1429] train loss: 2.119340, tar: 0.280528 \n",
            "l0: 0.176252, l1: 0.171555, l2: 0.179090, l3: 0.205404, l4: 0.205686, l5: 0.218550, l6: 0.264181\n",
            "\n",
            "[epoch:  21/100000, batch:   360/  840, ite: 1430] train loss: 2.118852, tar: 0.280455 \n",
            "l0: 0.256823, l1: 0.266296, l2: 0.272948, l3: 0.272369, l4: 0.260740, l5: 0.274367, l6: 0.301700\n",
            "\n",
            "[epoch:  21/100000, batch:   372/  840, ite: 1431] train loss: 2.118702, tar: 0.280439 \n",
            "l0: 0.300237, l1: 0.311947, l2: 0.323552, l3: 0.335884, l4: 0.307688, l5: 0.307601, l6: 0.325085\n",
            "\n",
            "[epoch:  21/100000, batch:   384/  840, ite: 1432] train loss: 2.118767, tar: 0.280453 \n",
            "l0: 0.120518, l1: 0.123265, l2: 0.120855, l3: 0.131969, l4: 0.139096, l5: 0.156340, l6: 0.204312\n",
            "\n",
            "[epoch:  21/100000, batch:   396/  840, ite: 1433] train loss: 2.117984, tar: 0.280341 \n",
            "l0: 0.227348, l1: 0.230961, l2: 0.237545, l3: 0.269130, l4: 0.266732, l5: 0.277137, l6: 0.301663\n",
            "\n",
            "[epoch:  21/100000, batch:   408/  840, ite: 1434] train loss: 2.117770, tar: 0.280304 \n",
            "l0: 0.232605, l1: 0.238033, l2: 0.243048, l3: 0.270062, l4: 0.261119, l5: 0.277646, l6: 0.303315\n",
            "\n",
            "[epoch:  21/100000, batch:   420/  840, ite: 1435] train loss: 2.117567, tar: 0.280271 \n",
            "l0: 0.129813, l1: 0.130987, l2: 0.125848, l3: 0.130750, l4: 0.149530, l5: 0.172675, l6: 0.214185\n",
            "\n",
            "[epoch:  21/100000, batch:   432/  840, ite: 1436] train loss: 2.116826, tar: 0.280166 \n",
            "l0: 0.292312, l1: 0.279421, l2: 0.295177, l3: 0.310006, l4: 0.334089, l5: 0.353600, l6: 0.382208\n",
            "\n",
            "[epoch:  21/100000, batch:   444/  840, ite: 1437] train loss: 2.116916, tar: 0.280175 \n",
            "l0: 0.265207, l1: 0.261528, l2: 0.272961, l3: 0.268482, l4: 0.281613, l5: 0.292004, l6: 0.322202\n",
            "\n",
            "[epoch:  21/100000, batch:   456/  840, ite: 1438] train loss: 2.116810, tar: 0.280164 \n",
            "l0: 0.284692, l1: 0.297503, l2: 0.293769, l3: 0.297597, l4: 0.297216, l5: 0.304151, l6: 0.324777\n",
            "\n",
            "[epoch:  21/100000, batch:   468/  840, ite: 1439] train loss: 2.116798, tar: 0.280167 \n",
            "l0: 0.219352, l1: 0.223119, l2: 0.221417, l3: 0.255210, l4: 0.269216, l5: 0.250614, l6: 0.280006\n",
            "\n",
            "[epoch:  21/100000, batch:   480/  840, ite: 1440] train loss: 2.116522, tar: 0.280125 \n",
            "l0: 0.253078, l1: 0.235718, l2: 0.249243, l3: 0.263892, l4: 0.270908, l5: 0.324618, l6: 0.394182\n",
            "\n",
            "[epoch:  21/100000, batch:   492/  840, ite: 1441] train loss: 2.116435, tar: 0.280106 \n",
            "l0: 0.247458, l1: 0.264218, l2: 0.255206, l3: 0.277791, l4: 0.268773, l5: 0.260438, l6: 0.325014\n",
            "\n",
            "[epoch:  21/100000, batch:   504/  840, ite: 1442] train loss: 2.116284, tar: 0.280084 \n",
            "l0: 0.169877, l1: 0.172195, l2: 0.179596, l3: 0.182437, l4: 0.182481, l5: 0.210495, l6: 0.283888\n",
            "\n",
            "[epoch:  21/100000, batch:   516/  840, ite: 1443] train loss: 2.115775, tar: 0.280007 \n",
            "l0: 0.193057, l1: 0.195632, l2: 0.199272, l3: 0.206240, l4: 0.214848, l5: 0.238173, l6: 0.287793\n",
            "\n",
            "[epoch:  21/100000, batch:   528/  840, ite: 1444] train loss: 2.115372, tar: 0.279947 \n",
            "l0: 0.275687, l1: 0.274961, l2: 0.282970, l3: 0.284001, l4: 0.296190, l5: 0.310673, l6: 0.338323\n",
            "\n",
            "[epoch:  21/100000, batch:   540/  840, ite: 1445] train loss: 2.115336, tar: 0.279944 \n",
            "l0: 0.195742, l1: 0.200626, l2: 0.207554, l3: 0.200247, l4: 0.204472, l5: 0.204729, l6: 0.243737\n",
            "\n",
            "[epoch:  21/100000, batch:   552/  840, ite: 1446] train loss: 2.114881, tar: 0.279886 \n",
            "l0: 0.366121, l1: 0.385898, l2: 0.421959, l3: 0.405671, l4: 0.401795, l5: 0.373843, l6: 0.343294\n",
            "\n",
            "[epoch:  21/100000, batch:   564/  840, ite: 1447] train loss: 2.115284, tar: 0.279946 \n",
            "l0: 0.233663, l1: 0.257480, l2: 0.257954, l3: 0.261694, l4: 0.265006, l5: 0.261656, l6: 0.285401\n",
            "\n",
            "[epoch:  21/100000, batch:   576/  840, ite: 1448] train loss: 2.115082, tar: 0.279914 \n",
            "l0: 0.125897, l1: 0.133964, l2: 0.132651, l3: 0.135808, l4: 0.138035, l5: 0.154471, l6: 0.219254\n",
            "\n",
            "[epoch:  21/100000, batch:   588/  840, ite: 1449] train loss: 2.114340, tar: 0.279807 \n",
            "l0: 0.178455, l1: 0.179687, l2: 0.181621, l3: 0.199503, l4: 0.209362, l5: 0.229614, l6: 0.293518\n",
            "\n",
            "[epoch:  21/100000, batch:   600/  840, ite: 1450] train loss: 2.113897, tar: 0.279737 \n",
            "l0: 0.139101, l1: 0.139129, l2: 0.139367, l3: 0.150529, l4: 0.157462, l5: 0.173124, l6: 0.245431\n",
            "\n",
            "[epoch:  21/100000, batch:   612/  840, ite: 1451] train loss: 2.113228, tar: 0.279641 \n",
            "l0: 0.363740, l1: 0.376129, l2: 0.349580, l3: 0.378861, l4: 0.387493, l5: 0.432554, l6: 0.482727\n",
            "\n",
            "[epoch:  21/100000, batch:   624/  840, ite: 1452] train loss: 2.113682, tar: 0.279698 \n",
            "l0: 0.221629, l1: 0.212730, l2: 0.225004, l3: 0.247961, l4: 0.258797, l5: 0.291499, l6: 0.317910\n",
            "\n",
            "[epoch:  21/100000, batch:   636/  840, ite: 1453] train loss: 2.113449, tar: 0.279658 \n",
            "l0: 0.234186, l1: 0.229931, l2: 0.238477, l3: 0.248623, l4: 0.255079, l5: 0.268025, l6: 0.301124\n",
            "\n",
            "[epoch:  21/100000, batch:   648/  840, ite: 1454] train loss: 2.113216, tar: 0.279627 \n",
            "l0: 0.228241, l1: 0.226433, l2: 0.236827, l3: 0.233166, l4: 0.241426, l5: 0.260730, l6: 0.297788\n",
            "\n",
            "[epoch:  21/100000, batch:   660/  840, ite: 1455] train loss: 2.112949, tar: 0.279592 \n",
            "l0: 0.171186, l1: 0.163717, l2: 0.176218, l3: 0.206494, l4: 0.205858, l5: 0.220939, l6: 0.294430\n",
            "\n",
            "[epoch:  21/100000, batch:   672/  840, ite: 1456] train loss: 2.112486, tar: 0.279517 \n",
            "l0: 0.163100, l1: 0.168895, l2: 0.174829, l3: 0.174718, l4: 0.185223, l5: 0.177080, l6: 0.208042\n",
            "\n",
            "[epoch:  21/100000, batch:   684/  840, ite: 1457] train loss: 2.111895, tar: 0.279438 \n",
            "l0: 0.186838, l1: 0.185194, l2: 0.190654, l3: 0.201477, l4: 0.209109, l5: 0.226935, l6: 0.283825\n",
            "\n",
            "[epoch:  21/100000, batch:   696/  840, ite: 1458] train loss: 2.111465, tar: 0.279374 \n",
            "l0: 0.177218, l1: 0.191222, l2: 0.182400, l3: 0.195611, l4: 0.195021, l5: 0.205244, l6: 0.241773\n",
            "\n",
            "[epoch:  21/100000, batch:   708/  840, ite: 1459] train loss: 2.110969, tar: 0.279304 \n",
            "l0: 0.328436, l1: 0.325057, l2: 0.348495, l3: 0.345280, l4: 0.340901, l5: 0.355873, l6: 0.361327\n",
            "\n",
            "[epoch:  21/100000, batch:   720/  840, ite: 1460] train loss: 2.111171, tar: 0.279338 \n",
            "l0: 0.131396, l1: 0.133204, l2: 0.134611, l3: 0.147480, l4: 0.148168, l5: 0.165301, l6: 0.204361\n",
            "\n",
            "[epoch:  21/100000, batch:   732/  840, ite: 1461] train loss: 2.110455, tar: 0.279236 \n",
            "l0: 0.185328, l1: 0.181307, l2: 0.192127, l3: 0.212288, l4: 0.216161, l5: 0.214842, l6: 0.262625\n",
            "\n",
            "[epoch:  21/100000, batch:   744/  840, ite: 1462] train loss: 2.110013, tar: 0.279172 \n",
            "l0: 0.171727, l1: 0.176378, l2: 0.179649, l3: 0.203086, l4: 0.198271, l5: 0.203217, l6: 0.250413\n",
            "\n",
            "[epoch:  21/100000, batch:   756/  840, ite: 1463] train loss: 2.109516, tar: 0.279099 \n",
            "l0: 0.333999, l1: 0.321198, l2: 0.325726, l3: 0.367975, l4: 0.364800, l5: 0.380324, l6: 0.470596\n",
            "\n",
            "[epoch:  21/100000, batch:   768/  840, ite: 1464] train loss: 2.109827, tar: 0.279136 \n",
            "l0: 0.252911, l1: 0.242175, l2: 0.258018, l3: 0.265644, l4: 0.287156, l5: 0.293105, l6: 0.323680\n",
            "\n",
            "[epoch:  21/100000, batch:   780/  840, ite: 1465] train loss: 2.109699, tar: 0.279118 \n",
            "l0: 0.245008, l1: 0.248720, l2: 0.259505, l3: 0.249747, l4: 0.254010, l5: 0.254791, l6: 0.270148\n",
            "\n",
            "[epoch:  21/100000, batch:   792/  840, ite: 1466] train loss: 2.109475, tar: 0.279095 \n",
            "l0: 0.218442, l1: 0.222216, l2: 0.225289, l3: 0.232105, l4: 0.242680, l5: 0.247852, l6: 0.265572\n",
            "\n",
            "[epoch:  21/100000, batch:   804/  840, ite: 1467] train loss: 2.109165, tar: 0.279054 \n",
            "l0: 0.178848, l1: 0.191433, l2: 0.171667, l3: 0.188254, l4: 0.199683, l5: 0.217385, l6: 0.306676\n",
            "\n",
            "[epoch:  21/100000, batch:   816/  840, ite: 1468] train loss: 2.108718, tar: 0.278985 \n",
            "l0: 0.172839, l1: 0.166776, l2: 0.177238, l3: 0.198278, l4: 0.210507, l5: 0.212052, l6: 0.268724\n",
            "\n",
            "[epoch:  21/100000, batch:   828/  840, ite: 1469] train loss: 2.108240, tar: 0.278913 \n",
            "l0: 0.198030, l1: 0.204810, l2: 0.199796, l3: 0.205103, l4: 0.216410, l5: 0.231427, l6: 0.268870\n",
            "\n",
            "[epoch:  21/100000, batch:   840/  840, ite: 1470] train loss: 2.107843, tar: 0.278858 \n",
            "l0: 0.253942, l1: 0.236909, l2: 0.266158, l3: 0.304251, l4: 0.305738, l5: 0.312161, l6: 0.327604\n",
            "\n",
            "[epoch:  22/100000, batch:    12/  840, ite: 1471] train loss: 2.107774, tar: 0.278841 \n",
            "l0: 0.227566, l1: 0.224708, l2: 0.227814, l3: 0.282862, l4: 0.283578, l5: 0.281132, l6: 0.325110\n",
            "\n",
            "[epoch:  22/100000, batch:    24/  840, ite: 1472] train loss: 2.107601, tar: 0.278806 \n",
            "l0: 0.161160, l1: 0.169429, l2: 0.168874, l3: 0.185328, l4: 0.191294, l5: 0.193641, l6: 0.228226\n",
            "\n",
            "[epoch:  22/100000, batch:    36/  840, ite: 1473] train loss: 2.107052, tar: 0.278726 \n",
            "l0: 0.209847, l1: 0.185971, l2: 0.216346, l3: 0.265891, l4: 0.274400, l5: 0.284671, l6: 0.349505\n",
            "\n",
            "[epoch:  22/100000, batch:    48/  840, ite: 1474] train loss: 2.106834, tar: 0.278680 \n",
            "l0: 0.214220, l1: 0.213811, l2: 0.225677, l3: 0.243593, l4: 0.242157, l5: 0.255224, l6: 0.289991\n",
            "\n",
            "[epoch:  22/100000, batch:    60/  840, ite: 1475] train loss: 2.106548, tar: 0.278636 \n",
            "l0: 0.216398, l1: 0.217859, l2: 0.225968, l3: 0.240468, l4: 0.245934, l5: 0.263096, l6: 0.302043\n",
            "\n",
            "[epoch:  22/100000, batch:    72/  840, ite: 1476] train loss: 2.106280, tar: 0.278594 \n",
            "l0: 0.145905, l1: 0.148156, l2: 0.150822, l3: 0.146925, l4: 0.162512, l5: 0.169406, l6: 0.220189\n",
            "\n",
            "[epoch:  22/100000, batch:    84/  840, ite: 1477] train loss: 2.105629, tar: 0.278504 \n",
            "l0: 0.243446, l1: 0.255753, l2: 0.249774, l3: 0.262902, l4: 0.264949, l5: 0.282302, l6: 0.318117\n",
            "\n",
            "[epoch:  22/100000, batch:    96/  840, ite: 1478] train loss: 2.105474, tar: 0.278480 \n",
            "l0: 0.182281, l1: 0.169039, l2: 0.193720, l3: 0.233601, l4: 0.237969, l5: 0.252395, l6: 0.291821\n",
            "\n",
            "[epoch:  22/100000, batch:   108/  840, ite: 1479] train loss: 2.105106, tar: 0.278415 \n",
            "l0: 0.131593, l1: 0.134630, l2: 0.139055, l3: 0.143198, l4: 0.155502, l5: 0.152234, l6: 0.192918\n",
            "\n",
            "[epoch:  22/100000, batch:   120/  840, ite: 1480] train loss: 2.104393, tar: 0.278316 \n",
            "l0: 0.173881, l1: 0.171560, l2: 0.176390, l3: 0.198848, l4: 0.211003, l5: 0.227085, l6: 0.274054\n",
            "\n",
            "[epoch:  22/100000, batch:   132/  840, ite: 1481] train loss: 2.103939, tar: 0.278246 \n",
            "l0: 0.217378, l1: 0.218843, l2: 0.223792, l3: 0.227499, l4: 0.231286, l5: 0.263587, l6: 0.316028\n",
            "\n",
            "[epoch:  22/100000, batch:   144/  840, ite: 1482] train loss: 2.103665, tar: 0.278205 \n",
            "l0: 0.102811, l1: 0.101726, l2: 0.112802, l3: 0.108882, l4: 0.113653, l5: 0.129956, l6: 0.199569\n",
            "\n",
            "[epoch:  22/100000, batch:   156/  840, ite: 1483] train loss: 2.102833, tar: 0.278086 \n",
            "l0: 0.203601, l1: 0.208424, l2: 0.220892, l3: 0.189730, l4: 0.188580, l5: 0.217118, l6: 0.260405\n",
            "\n",
            "[epoch:  22/100000, batch:   168/  840, ite: 1484] train loss: 2.102419, tar: 0.278036 \n",
            "l0: 0.197059, l1: 0.200204, l2: 0.204746, l3: 0.207086, l4: 0.208824, l5: 0.228900, l6: 0.274195\n",
            "\n",
            "[epoch:  22/100000, batch:   180/  840, ite: 1485] train loss: 2.102028, tar: 0.277981 \n",
            "l0: 0.251854, l1: 0.268199, l2: 0.254349, l3: 0.255826, l4: 0.261178, l5: 0.269444, l6: 0.314913\n",
            "\n",
            "[epoch:  22/100000, batch:   192/  840, ite: 1486] train loss: 2.101876, tar: 0.277964 \n",
            "l0: 0.166925, l1: 0.168671, l2: 0.171304, l3: 0.196903, l4: 0.196179, l5: 0.197334, l6: 0.242391\n",
            "\n",
            "[epoch:  22/100000, batch:   204/  840, ite: 1487] train loss: 2.101363, tar: 0.277889 \n",
            "l0: 0.292568, l1: 0.317119, l2: 0.317733, l3: 0.309344, l4: 0.293948, l5: 0.277739, l6: 0.286935\n",
            "\n",
            "[epoch:  22/100000, batch:   216/  840, ite: 1488] train loss: 2.101359, tar: 0.277899 \n",
            "l0: 0.178479, l1: 0.197354, l2: 0.177676, l3: 0.204362, l4: 0.207819, l5: 0.208379, l6: 0.241919\n",
            "\n",
            "[epoch:  22/100000, batch:   228/  840, ite: 1489] train loss: 2.100899, tar: 0.277832 \n",
            "l0: 0.331493, l1: 0.340146, l2: 0.355960, l3: 0.329964, l4: 0.332755, l5: 0.335251, l6: 0.366600\n",
            "\n",
            "[epoch:  22/100000, batch:   240/  840, ite: 1490] train loss: 2.101094, tar: 0.277868 \n",
            "l0: 0.181113, l1: 0.177541, l2: 0.185501, l3: 0.213194, l4: 0.220514, l5: 0.228627, l6: 0.286970\n",
            "\n",
            "[epoch:  22/100000, batch:   252/  840, ite: 1491] train loss: 2.100686, tar: 0.277803 \n",
            "l0: 0.185471, l1: 0.189984, l2: 0.205431, l3: 0.195095, l4: 0.194103, l5: 0.207125, l6: 0.240231\n",
            "\n",
            "[epoch:  22/100000, batch:   264/  840, ite: 1492] train loss: 2.100229, tar: 0.277742 \n",
            "l0: 0.153239, l1: 0.160795, l2: 0.154919, l3: 0.162997, l4: 0.165877, l5: 0.189670, l6: 0.240528\n",
            "\n",
            "[epoch:  22/100000, batch:   276/  840, ite: 1493] train loss: 2.099644, tar: 0.277658 \n",
            "l0: 0.104090, l1: 0.109155, l2: 0.108109, l3: 0.126439, l4: 0.125584, l5: 0.151230, l6: 0.188611\n",
            "\n",
            "[epoch:  22/100000, batch:   288/  840, ite: 1494] train loss: 2.098850, tar: 0.277542 \n",
            "l0: 0.200486, l1: 0.206531, l2: 0.220292, l3: 0.213556, l4: 0.203728, l5: 0.215856, l6: 0.229003\n",
            "\n",
            "[epoch:  22/100000, batch:   300/  840, ite: 1495] train loss: 2.098443, tar: 0.277490 \n",
            "l0: 0.267793, l1: 0.261288, l2: 0.271187, l3: 0.291222, l4: 0.299955, l5: 0.303184, l6: 0.349632\n",
            "\n",
            "[epoch:  22/100000, batch:   312/  840, ite: 1496] train loss: 2.098407, tar: 0.277484 \n",
            "l0: 0.125520, l1: 0.134701, l2: 0.133712, l3: 0.128233, l4: 0.130495, l5: 0.142985, l6: 0.158840\n",
            "\n",
            "[epoch:  22/100000, batch:   324/  840, ite: 1497] train loss: 2.097642, tar: 0.277383 \n",
            "l0: 0.172031, l1: 0.204541, l2: 0.170642, l3: 0.176264, l4: 0.180161, l5: 0.179974, l6: 0.248388\n",
            "\n",
            "[epoch:  22/100000, batch:   336/  840, ite: 1498] train loss: 2.097131, tar: 0.277312 \n",
            "l0: 0.236171, l1: 0.245925, l2: 0.235399, l3: 0.264631, l4: 0.262890, l5: 0.262414, l6: 0.302124\n",
            "\n",
            "[epoch:  22/100000, batch:   348/  840, ite: 1499] train loss: 2.096940, tar: 0.277285 \n",
            "l0: 0.179021, l1: 0.185416, l2: 0.195597, l3: 0.198565, l4: 0.196460, l5: 0.200918, l6: 0.240848\n",
            "\n",
            "[epoch:  22/100000, batch:   360/  840, ite: 1500] train loss: 2.096473, tar: 0.277219 \n",
            "l0: 0.232724, l1: 0.213987, l2: 0.247977, l3: 0.268393, l4: 0.272813, l5: 0.310094, l6: 0.342905\n",
            "\n",
            "[epoch:  22/100000, batch:   372/  840, ite: 1501] train loss: 2.096334, tar: 0.277190 \n",
            "l0: 0.191788, l1: 0.193834, l2: 0.205973, l3: 0.221476, l4: 0.213678, l5: 0.211066, l6: 0.231592\n",
            "\n",
            "[epoch:  22/100000, batch:   384/  840, ite: 1502] train loss: 2.095917, tar: 0.277133 \n",
            "l0: 0.215439, l1: 0.210323, l2: 0.224673, l3: 0.260594, l4: 0.256506, l5: 0.260023, l6: 0.300972\n",
            "\n",
            "[epoch:  22/100000, batch:   396/  840, ite: 1503] train loss: 2.095673, tar: 0.277092 \n",
            "l0: 0.234228, l1: 0.237898, l2: 0.243315, l3: 0.237042, l4: 0.240623, l5: 0.261973, l6: 0.315794\n",
            "\n",
            "[epoch:  22/100000, batch:   408/  840, ite: 1504] train loss: 2.095457, tar: 0.277063 \n",
            "l0: 0.172602, l1: 0.184132, l2: 0.182911, l3: 0.184066, l4: 0.181680, l5: 0.193831, l6: 0.220151\n",
            "\n",
            "[epoch:  22/100000, batch:   420/  840, ite: 1505] train loss: 2.094941, tar: 0.276994 \n",
            "l0: 0.161285, l1: 0.162576, l2: 0.164339, l3: 0.160841, l4: 0.165107, l5: 0.172530, l6: 0.209099\n",
            "\n",
            "[epoch:  22/100000, batch:   432/  840, ite: 1506] train loss: 2.094344, tar: 0.276917 \n",
            "l0: 0.214616, l1: 0.238144, l2: 0.227536, l3: 0.218959, l4: 0.212617, l5: 0.208982, l6: 0.235483\n",
            "\n",
            "[epoch:  22/100000, batch:   444/  840, ite: 1507] train loss: 2.093987, tar: 0.276876 \n",
            "l0: 0.156291, l1: 0.156772, l2: 0.169603, l3: 0.171378, l4: 0.175545, l5: 0.173143, l6: 0.203234\n",
            "\n",
            "[epoch:  22/100000, batch:   456/  840, ite: 1508] train loss: 2.093398, tar: 0.276796 \n",
            "l0: 0.191170, l1: 0.190717, l2: 0.203596, l3: 0.212277, l4: 0.207038, l5: 0.224081, l6: 0.267524\n",
            "\n",
            "[epoch:  22/100000, batch:   468/  840, ite: 1509] train loss: 2.093003, tar: 0.276739 \n",
            "l0: 0.227660, l1: 0.228949, l2: 0.234979, l3: 0.242234, l4: 0.244363, l5: 0.266178, l6: 0.295051\n",
            "\n",
            "[epoch:  22/100000, batch:   480/  840, ite: 1510] train loss: 2.092768, tar: 0.276706 \n",
            "l0: 0.238093, l1: 0.238186, l2: 0.241997, l3: 0.261445, l4: 0.255940, l5: 0.279076, l6: 0.320170\n",
            "\n",
            "[epoch:  22/100000, batch:   492/  840, ite: 1511] train loss: 2.092598, tar: 0.276681 \n",
            "l0: 0.213396, l1: 0.220550, l2: 0.230023, l3: 0.214183, l4: 0.216661, l5: 0.232222, l6: 0.252553\n",
            "\n",
            "[epoch:  22/100000, batch:   504/  840, ite: 1512] train loss: 2.092258, tar: 0.276639 \n",
            "l0: 0.101823, l1: 0.099725, l2: 0.108808, l3: 0.115045, l4: 0.121049, l5: 0.141863, l6: 0.176006\n",
            "\n",
            "[epoch:  22/100000, batch:   516/  840, ite: 1513] train loss: 2.091447, tar: 0.276523 \n",
            "l0: 0.154510, l1: 0.171509, l2: 0.165833, l3: 0.164665, l4: 0.160336, l5: 0.172181, l6: 0.220528\n",
            "\n",
            "[epoch:  22/100000, batch:   528/  840, ite: 1514] train loss: 2.090864, tar: 0.276443 \n",
            "l0: 0.246219, l1: 0.264145, l2: 0.264428, l3: 0.236409, l4: 0.235123, l5: 0.242199, l6: 0.283959\n",
            "\n",
            "[epoch:  22/100000, batch:   540/  840, ite: 1515] train loss: 2.090654, tar: 0.276423 \n",
            "l0: 0.211194, l1: 0.238595, l2: 0.212302, l3: 0.215097, l4: 0.213008, l5: 0.217902, l6: 0.242707\n",
            "\n",
            "[epoch:  22/100000, batch:   552/  840, ite: 1516] train loss: 2.090298, tar: 0.276380 \n",
            "l0: 0.300767, l1: 0.257253, l2: 0.305211, l3: 0.352288, l4: 0.363063, l5: 0.355014, l6: 0.402982\n",
            "\n",
            "[epoch:  22/100000, batch:   564/  840, ite: 1517] train loss: 2.090460, tar: 0.276396 \n",
            "l0: 0.154057, l1: 0.157513, l2: 0.158299, l3: 0.164023, l4: 0.165660, l5: 0.171099, l6: 0.212294\n",
            "\n",
            "[epoch:  22/100000, batch:   576/  840, ite: 1518] train loss: 2.089862, tar: 0.276315 \n",
            "l0: 0.246498, l1: 0.245802, l2: 0.263977, l3: 0.273771, l4: 0.269681, l5: 0.279264, l6: 0.287287\n",
            "\n",
            "[epoch:  22/100000, batch:   588/  840, ite: 1519] train loss: 2.089715, tar: 0.276296 \n",
            "l0: 0.182087, l1: 0.171886, l2: 0.177486, l3: 0.253664, l4: 0.257289, l5: 0.279452, l6: 0.304075\n",
            "\n",
            "[epoch:  22/100000, batch:   600/  840, ite: 1520] train loss: 2.089410, tar: 0.276234 \n",
            "l0: 0.206885, l1: 0.200164, l2: 0.223815, l3: 0.225613, l4: 0.230016, l5: 0.232989, l6: 0.260676\n",
            "\n",
            "[epoch:  22/100000, batch:   612/  840, ite: 1521] train loss: 2.089075, tar: 0.276188 \n",
            "l0: 0.217144, l1: 0.203094, l2: 0.219311, l3: 0.232559, l4: 0.243467, l5: 0.268799, l6: 0.320213\n",
            "\n",
            "[epoch:  22/100000, batch:   624/  840, ite: 1522] train loss: 2.088823, tar: 0.276149 \n",
            "l0: 0.288216, l1: 0.294250, l2: 0.282944, l3: 0.320224, l4: 0.321532, l5: 0.329937, l6: 0.372855\n",
            "\n",
            "[epoch:  22/100000, batch:   636/  840, ite: 1523] train loss: 2.088902, tar: 0.276157 \n",
            "l0: 0.163903, l1: 0.168073, l2: 0.172159, l3: 0.182605, l4: 0.187771, l5: 0.202888, l6: 0.227122\n",
            "\n",
            "[epoch:  22/100000, batch:   648/  840, ite: 1524] train loss: 2.088387, tar: 0.276084 \n",
            "l0: 0.261195, l1: 0.272163, l2: 0.257102, l3: 0.303057, l4: 0.309250, l5: 0.313706, l6: 0.332840\n",
            "\n",
            "[epoch:  22/100000, batch:   660/  840, ite: 1525] train loss: 2.088362, tar: 0.276074 \n",
            "l0: 0.159694, l1: 0.180779, l2: 0.155617, l3: 0.169234, l4: 0.177041, l5: 0.187141, l6: 0.226786\n",
            "\n",
            "[epoch:  22/100000, batch:   672/  840, ite: 1526] train loss: 2.087817, tar: 0.275998 \n",
            "l0: 0.219260, l1: 0.205047, l2: 0.225952, l3: 0.245599, l4: 0.250689, l5: 0.280438, l6: 0.328687\n",
            "\n",
            "[epoch:  22/100000, batch:   684/  840, ite: 1527] train loss: 2.087599, tar: 0.275960 \n",
            "l0: 0.176758, l1: 0.183659, l2: 0.182550, l3: 0.188986, l4: 0.192660, l5: 0.208578, l6: 0.247311\n",
            "\n",
            "[epoch:  22/100000, batch:   696/  840, ite: 1528] train loss: 2.087136, tar: 0.275896 \n",
            "l0: 0.247586, l1: 0.252711, l2: 0.259143, l3: 0.246694, l4: 0.250223, l5: 0.258605, l6: 0.275597\n",
            "\n",
            "[epoch:  22/100000, batch:   708/  840, ite: 1529] train loss: 2.086942, tar: 0.275877 \n",
            "l0: 0.211062, l1: 0.205397, l2: 0.218238, l3: 0.245615, l4: 0.244046, l5: 0.256244, l6: 0.279052\n",
            "\n",
            "[epoch:  22/100000, batch:   720/  840, ite: 1530] train loss: 2.086663, tar: 0.275835 \n",
            "l0: 0.131007, l1: 0.139148, l2: 0.137337, l3: 0.153332, l4: 0.153353, l5: 0.159448, l6: 0.197366\n",
            "\n",
            "[epoch:  22/100000, batch:   732/  840, ite: 1531] train loss: 2.085999, tar: 0.275740 \n",
            "l0: 0.175263, l1: 0.185946, l2: 0.175256, l3: 0.190428, l4: 0.195120, l5: 0.209606, l6: 0.255937\n",
            "\n",
            "[epoch:  22/100000, batch:   744/  840, ite: 1532] train loss: 2.085544, tar: 0.275674 \n",
            "l0: 0.132949, l1: 0.144742, l2: 0.127330, l3: 0.158198, l4: 0.161282, l5: 0.167505, l6: 0.206363\n",
            "\n",
            "[epoch:  22/100000, batch:   756/  840, ite: 1533] train loss: 2.084900, tar: 0.275581 \n",
            "l0: 0.210500, l1: 0.224125, l2: 0.213923, l3: 0.221709, l4: 0.224611, l5: 0.230621, l6: 0.272534\n",
            "\n",
            "[epoch:  22/100000, batch:   768/  840, ite: 1534] train loss: 2.084582, tar: 0.275539 \n",
            "l0: 0.203872, l1: 0.200664, l2: 0.210232, l3: 0.216861, l4: 0.225058, l5: 0.236924, l6: 0.267459\n",
            "\n",
            "[epoch:  22/100000, batch:   780/  840, ite: 1535] train loss: 2.084241, tar: 0.275492 \n",
            "l0: 0.193809, l1: 0.189989, l2: 0.215306, l3: 0.197936, l4: 0.198684, l5: 0.201052, l6: 0.213492\n",
            "\n",
            "[epoch:  22/100000, batch:   792/  840, ite: 1536] train loss: 2.083803, tar: 0.275439 \n",
            "l0: 0.228851, l1: 0.229279, l2: 0.246195, l3: 0.242262, l4: 0.241132, l5: 0.256260, l6: 0.299705\n",
            "\n",
            "[epoch:  22/100000, batch:   804/  840, ite: 1537] train loss: 2.083581, tar: 0.275409 \n",
            "l0: 0.245895, l1: 0.223905, l2: 0.256066, l3: 0.254085, l4: 0.258896, l5: 0.310655, l6: 0.339218\n",
            "\n",
            "[epoch:  22/100000, batch:   816/  840, ite: 1538] train loss: 2.083455, tar: 0.275390 \n",
            "l0: 0.218175, l1: 0.229945, l2: 0.218857, l3: 0.242607, l4: 0.240375, l5: 0.251787, l6: 0.279287\n",
            "\n",
            "[epoch:  22/100000, batch:   828/  840, ite: 1539] train loss: 2.083193, tar: 0.275352 \n",
            "l0: 0.189029, l1: 0.184407, l2: 0.194068, l3: 0.201155, l4: 0.210132, l5: 0.225023, l6: 0.264009\n",
            "\n",
            "[epoch:  22/100000, batch:   840/  840, ite: 1540] train loss: 2.082793, tar: 0.275296 \n",
            "l0: 0.244370, l1: 0.258407, l2: 0.265900, l3: 0.245452, l4: 0.240366, l5: 0.222393, l6: 0.243647\n",
            "\n",
            "[epoch:  23/100000, batch:    12/  840, ite: 1541] train loss: 2.082558, tar: 0.275276 \n",
            "l0: 0.205415, l1: 0.198588, l2: 0.210532, l3: 0.233378, l4: 0.241452, l5: 0.247619, l6: 0.268517\n",
            "\n",
            "[epoch:  23/100000, batch:    24/  840, ite: 1542] train loss: 2.082249, tar: 0.275231 \n",
            "l0: 0.365215, l1: 0.338805, l2: 0.384291, l3: 0.403214, l4: 0.403164, l5: 0.407319, l6: 0.425094\n",
            "\n",
            "[epoch:  23/100000, batch:    36/  840, ite: 1543] train loss: 2.082667, tar: 0.275289 \n",
            "l0: 0.132824, l1: 0.146561, l2: 0.136825, l3: 0.156214, l4: 0.149652, l5: 0.144862, l6: 0.171740\n",
            "\n",
            "[epoch:  23/100000, batch:    48/  840, ite: 1544] train loss: 2.081990, tar: 0.275197 \n",
            "l0: 0.154652, l1: 0.157266, l2: 0.162713, l3: 0.174436, l4: 0.176924, l5: 0.179191, l6: 0.196687\n",
            "\n",
            "[epoch:  23/100000, batch:    60/  840, ite: 1545] train loss: 2.081421, tar: 0.275119 \n",
            "l0: 0.165966, l1: 0.167240, l2: 0.166502, l3: 0.179727, l4: 0.183642, l5: 0.193472, l6: 0.235500\n",
            "\n",
            "[epoch:  23/100000, batch:    72/  840, ite: 1546] train loss: 2.080910, tar: 0.275048 \n",
            "l0: 0.246432, l1: 0.250902, l2: 0.254900, l3: 0.250259, l4: 0.255530, l5: 0.276889, l6: 0.308323\n",
            "\n",
            "[epoch:  23/100000, batch:    84/  840, ite: 1547] train loss: 2.080757, tar: 0.275030 \n",
            "l0: 0.157582, l1: 0.154401, l2: 0.166741, l3: 0.177256, l4: 0.183750, l5: 0.197177, l6: 0.235406\n",
            "\n",
            "[epoch:  23/100000, batch:    96/  840, ite: 1548] train loss: 2.080235, tar: 0.274954 \n",
            "l0: 0.139712, l1: 0.143310, l2: 0.140683, l3: 0.148266, l4: 0.151768, l5: 0.163071, l6: 0.206924\n",
            "\n",
            "[epoch:  23/100000, batch:   108/  840, ite: 1549] train loss: 2.079597, tar: 0.274867 \n",
            "l0: 0.359752, l1: 0.368245, l2: 0.365281, l3: 0.382184, l4: 0.389188, l5: 0.391927, l6: 0.412496\n",
            "\n",
            "[epoch:  23/100000, batch:   120/  840, ite: 1550] train loss: 2.079978, tar: 0.274921 \n",
            "l0: 0.212409, l1: 0.212343, l2: 0.226598, l3: 0.267729, l4: 0.262291, l5: 0.251893, l6: 0.284389\n",
            "\n",
            "[epoch:  23/100000, batch:   132/  840, ite: 1551] train loss: 2.079744, tar: 0.274881 \n",
            "l0: 0.247080, l1: 0.248297, l2: 0.253148, l3: 0.285111, l4: 0.277429, l5: 0.280571, l6: 0.319392\n",
            "\n",
            "[epoch:  23/100000, batch:   144/  840, ite: 1552] train loss: 2.079636, tar: 0.274863 \n",
            "l0: 0.175097, l1: 0.190676, l2: 0.180604, l3: 0.194216, l4: 0.192752, l5: 0.192162, l6: 0.224096\n",
            "\n",
            "[epoch:  23/100000, batch:   156/  840, ite: 1553] train loss: 2.079166, tar: 0.274799 \n",
            "l0: 0.148175, l1: 0.147390, l2: 0.152347, l3: 0.143946, l4: 0.150465, l5: 0.172911, l6: 0.237531\n",
            "\n",
            "[epoch:  23/100000, batch:   168/  840, ite: 1554] train loss: 2.078570, tar: 0.274718 \n",
            "l0: 0.147453, l1: 0.142348, l2: 0.158955, l3: 0.168501, l4: 0.172086, l5: 0.186289, l6: 0.234182\n",
            "\n",
            "[epoch:  23/100000, batch:   180/  840, ite: 1555] train loss: 2.078011, tar: 0.274636 \n",
            "l0: 0.158838, l1: 0.167135, l2: 0.164768, l3: 0.173140, l4: 0.171410, l5: 0.188301, l6: 0.225319\n",
            "\n",
            "[epoch:  23/100000, batch:   192/  840, ite: 1556] train loss: 2.077478, tar: 0.274561 \n",
            "l0: 0.148259, l1: 0.144780, l2: 0.149060, l3: 0.165535, l4: 0.171539, l5: 0.188985, l6: 0.240177\n",
            "\n",
            "[epoch:  23/100000, batch:   204/  840, ite: 1557] train loss: 2.076920, tar: 0.274480 \n",
            "l0: 0.156419, l1: 0.162138, l2: 0.162807, l3: 0.158884, l4: 0.164840, l5: 0.181739, l6: 0.224664\n",
            "\n",
            "[epoch:  23/100000, batch:   216/  840, ite: 1558] train loss: 2.076364, tar: 0.274404 \n",
            "l0: 0.163427, l1: 0.161588, l2: 0.167511, l3: 0.178312, l4: 0.181175, l5: 0.188429, l6: 0.223508\n",
            "\n",
            "[epoch:  23/100000, batch:   228/  840, ite: 1559] train loss: 2.075843, tar: 0.274333 \n",
            "l0: 0.259610, l1: 0.254488, l2: 0.278102, l3: 0.292479, l4: 0.307171, l5: 0.283293, l6: 0.311022\n",
            "\n",
            "[epoch:  23/100000, batch:   240/  840, ite: 1560] train loss: 2.075786, tar: 0.274324 \n",
            "l0: 0.223183, l1: 0.223366, l2: 0.237448, l3: 0.222351, l4: 0.228968, l5: 0.246319, l6: 0.312492\n",
            "\n",
            "[epoch:  23/100000, batch:   252/  840, ite: 1561] train loss: 2.075541, tar: 0.274291 \n",
            "l0: 0.148671, l1: 0.165781, l2: 0.156198, l3: 0.159792, l4: 0.162826, l5: 0.176090, l6: 0.222855\n",
            "\n",
            "[epoch:  23/100000, batch:   264/  840, ite: 1562] train loss: 2.074975, tar: 0.274211 \n",
            "l0: 0.115354, l1: 0.111907, l2: 0.121168, l3: 0.123091, l4: 0.129878, l5: 0.155042, l6: 0.219349\n",
            "\n",
            "[epoch:  23/100000, batch:   276/  840, ite: 1563] train loss: 2.074272, tar: 0.274109 \n",
            "l0: 0.215538, l1: 0.227272, l2: 0.219990, l3: 0.234756, l4: 0.233272, l5: 0.228037, l6: 0.237461\n",
            "\n",
            "[epoch:  23/100000, batch:   288/  840, ite: 1564] train loss: 2.073967, tar: 0.274071 \n",
            "l0: 0.144751, l1: 0.141527, l2: 0.153072, l3: 0.164384, l4: 0.176457, l5: 0.189040, l6: 0.226782\n",
            "\n",
            "[epoch:  23/100000, batch:   300/  840, ite: 1565] train loss: 2.073406, tar: 0.273989 \n",
            "l0: 0.188139, l1: 0.189373, l2: 0.195067, l3: 0.198402, l4: 0.214052, l5: 0.219222, l6: 0.272440\n",
            "\n",
            "[epoch:  23/100000, batch:   312/  840, ite: 1566] train loss: 2.073025, tar: 0.273934 \n",
            "l0: 0.189820, l1: 0.193344, l2: 0.194953, l3: 0.211871, l4: 0.219830, l5: 0.220458, l6: 0.255858\n",
            "\n",
            "[epoch:  23/100000, batch:   324/  840, ite: 1567] train loss: 2.072650, tar: 0.273880 \n",
            "l0: 0.174765, l1: 0.178253, l2: 0.180464, l3: 0.188587, l4: 0.197965, l5: 0.206321, l6: 0.270139\n",
            "\n",
            "[epoch:  23/100000, batch:   336/  840, ite: 1568] train loss: 2.072219, tar: 0.273817 \n",
            "l0: 0.216448, l1: 0.238321, l2: 0.222145, l3: 0.227062, l4: 0.224555, l5: 0.226925, l6: 0.257977\n",
            "\n",
            "[epoch:  23/100000, batch:   348/  840, ite: 1569] train loss: 2.071927, tar: 0.273781 \n",
            "l0: 0.164657, l1: 0.157928, l2: 0.166848, l3: 0.188544, l4: 0.198257, l5: 0.218493, l6: 0.254477\n",
            "\n",
            "[epoch:  23/100000, batch:   360/  840, ite: 1570] train loss: 2.071466, tar: 0.273711 \n",
            "l0: 0.137285, l1: 0.142702, l2: 0.142129, l3: 0.147865, l4: 0.148557, l5: 0.160636, l6: 0.216939\n",
            "\n",
            "[epoch:  23/100000, batch:   372/  840, ite: 1571] train loss: 2.070845, tar: 0.273624 \n",
            "l0: 0.237771, l1: 0.249139, l2: 0.237053, l3: 0.256396, l4: 0.249900, l5: 0.251826, l6: 0.283744\n",
            "\n",
            "[epoch:  23/100000, batch:   384/  840, ite: 1572] train loss: 2.070652, tar: 0.273601 \n",
            "l0: 0.262124, l1: 0.264342, l2: 0.269138, l3: 0.252649, l4: 0.252087, l5: 0.273923, l6: 0.315955\n",
            "\n",
            "[epoch:  23/100000, batch:   396/  840, ite: 1573] train loss: 2.070537, tar: 0.273594 \n",
            "l0: 0.086080, l1: 0.085193, l2: 0.089721, l3: 0.096200, l4: 0.108000, l5: 0.126504, l6: 0.163339\n",
            "\n",
            "[epoch:  23/100000, batch:   408/  840, ite: 1574] train loss: 2.069701, tar: 0.273475 \n",
            "l0: 0.134132, l1: 0.130655, l2: 0.139556, l3: 0.156534, l4: 0.163308, l5: 0.168527, l6: 0.196387\n",
            "\n",
            "[epoch:  23/100000, batch:   420/  840, ite: 1575] train loss: 2.069078, tar: 0.273387 \n",
            "l0: 0.185801, l1: 0.187016, l2: 0.186192, l3: 0.201955, l4: 0.203060, l5: 0.215685, l6: 0.249827\n",
            "\n",
            "[epoch:  23/100000, batch:   432/  840, ite: 1576] train loss: 2.068673, tar: 0.273331 \n",
            "l0: 0.175673, l1: 0.172857, l2: 0.176941, l3: 0.198745, l4: 0.198803, l5: 0.216185, l6: 0.248586\n",
            "\n",
            "[epoch:  23/100000, batch:   444/  840, ite: 1577] train loss: 2.068241, tar: 0.273269 \n",
            "l0: 0.238770, l1: 0.233261, l2: 0.237153, l3: 0.236102, l4: 0.242315, l5: 0.264911, l6: 0.317275\n",
            "\n",
            "[epoch:  23/100000, batch:   456/  840, ite: 1578] train loss: 2.068052, tar: 0.273247 \n",
            "l0: 0.290342, l1: 0.279249, l2: 0.295730, l3: 0.316974, l4: 0.325409, l5: 0.326573, l6: 0.360534\n",
            "\n",
            "[epoch:  23/100000, batch:   468/  840, ite: 1579] train loss: 2.068132, tar: 0.273258 \n",
            "l0: 0.168830, l1: 0.167996, l2: 0.177412, l3: 0.190496, l4: 0.194323, l5: 0.186371, l6: 0.222360\n",
            "\n",
            "[epoch:  23/100000, batch:   480/  840, ite: 1580] train loss: 2.067651, tar: 0.273192 \n",
            "l0: 0.197922, l1: 0.189331, l2: 0.220638, l3: 0.229816, l4: 0.228802, l5: 0.210116, l6: 0.243520\n",
            "\n",
            "[epoch:  23/100000, batch:   492/  840, ite: 1581] train loss: 2.067304, tar: 0.273144 \n",
            "l0: 0.187078, l1: 0.194961, l2: 0.199096, l3: 0.203022, l4: 0.202393, l5: 0.224768, l6: 0.253921\n",
            "\n",
            "[epoch:  23/100000, batch:   504/  840, ite: 1582] train loss: 2.066924, tar: 0.273090 \n",
            "l0: 0.194285, l1: 0.187653, l2: 0.199306, l3: 0.203181, l4: 0.208499, l5: 0.248769, l6: 0.288675\n",
            "\n",
            "[epoch:  23/100000, batch:   516/  840, ite: 1583] train loss: 2.066585, tar: 0.273040 \n",
            "l0: 0.182608, l1: 0.175893, l2: 0.180609, l3: 0.225422, l4: 0.230660, l5: 0.247406, l6: 0.275642\n",
            "\n",
            "[epoch:  23/100000, batch:   528/  840, ite: 1584] train loss: 2.066239, tar: 0.272983 \n",
            "l0: 0.240294, l1: 0.246133, l2: 0.247302, l3: 0.239393, l4: 0.242748, l5: 0.251391, l6: 0.288812\n",
            "\n",
            "[epoch:  23/100000, batch:   540/  840, ite: 1585] train loss: 2.066043, tar: 0.272962 \n",
            "l0: 0.377031, l1: 0.362391, l2: 0.377654, l3: 0.370744, l4: 0.390813, l5: 0.378277, l6: 0.397351\n",
            "\n",
            "[epoch:  23/100000, batch:   552/  840, ite: 1586] train loss: 2.066414, tar: 0.273028 \n",
            "l0: 0.190355, l1: 0.187650, l2: 0.199595, l3: 0.207600, l4: 0.213682, l5: 0.232596, l6: 0.272559\n",
            "\n",
            "[epoch:  23/100000, batch:   564/  840, ite: 1587] train loss: 2.066060, tar: 0.272976 \n",
            "l0: 0.278555, l1: 0.279698, l2: 0.273185, l3: 0.285780, l4: 0.283946, l5: 0.363632, l6: 0.390438\n",
            "\n",
            "[epoch:  23/100000, batch:   576/  840, ite: 1588] train loss: 2.066116, tar: 0.272979 \n",
            "l0: 0.123620, l1: 0.122857, l2: 0.124951, l3: 0.141003, l4: 0.145797, l5: 0.184660, l6: 0.226520\n",
            "\n",
            "[epoch:  23/100000, batch:   588/  840, ite: 1589] train loss: 2.065489, tar: 0.272885 \n",
            "l0: 0.180461, l1: 0.167088, l2: 0.194192, l3: 0.216454, l4: 0.217519, l5: 0.237773, l6: 0.278189\n",
            "\n",
            "[epoch:  23/100000, batch:   600/  840, ite: 1590] train loss: 2.065128, tar: 0.272827 \n",
            "l0: 0.174421, l1: 0.174759, l2: 0.178666, l3: 0.179242, l4: 0.183490, l5: 0.214450, l6: 0.266327\n",
            "\n",
            "[epoch:  23/100000, batch:   612/  840, ite: 1591] train loss: 2.064692, tar: 0.272765 \n",
            "l0: 0.131287, l1: 0.138456, l2: 0.142527, l3: 0.159340, l4: 0.155128, l5: 0.166115, l6: 0.210804\n",
            "\n",
            "[epoch:  23/100000, batch:   624/  840, ite: 1592] train loss: 2.064088, tar: 0.272677 \n",
            "l0: 0.107308, l1: 0.110082, l2: 0.110188, l3: 0.113783, l4: 0.120465, l5: 0.133885, l6: 0.185549\n",
            "\n",
            "[epoch:  23/100000, batch:   636/  840, ite: 1593] train loss: 2.063346, tar: 0.272573 \n",
            "l0: 0.094713, l1: 0.100196, l2: 0.092993, l3: 0.107386, l4: 0.110650, l5: 0.129033, l6: 0.186703\n",
            "\n",
            "[epoch:  23/100000, batch:   648/  840, ite: 1594] train loss: 2.062567, tar: 0.272461 \n",
            "l0: 0.217218, l1: 0.222703, l2: 0.227512, l3: 0.237292, l4: 0.233725, l5: 0.247549, l6: 0.292169\n",
            "\n",
            "[epoch:  23/100000, batch:   660/  840, ite: 1595] train loss: 2.062326, tar: 0.272427 \n",
            "l0: 0.093949, l1: 0.091489, l2: 0.094630, l3: 0.107681, l4: 0.114916, l5: 0.151554, l6: 0.206915\n",
            "\n",
            "[epoch:  23/100000, batch:   672/  840, ite: 1596] train loss: 2.061573, tar: 0.272315 \n",
            "l0: 0.205824, l1: 0.206006, l2: 0.207604, l3: 0.224006, l4: 0.225549, l5: 0.259511, l6: 0.308529\n",
            "\n",
            "[epoch:  23/100000, batch:   684/  840, ite: 1597] train loss: 2.061307, tar: 0.272273 \n",
            "l0: 0.212214, l1: 0.224339, l2: 0.225043, l3: 0.232621, l4: 0.224176, l5: 0.254295, l6: 0.311358\n",
            "\n",
            "[epoch:  23/100000, batch:   696/  840, ite: 1598] train loss: 2.061071, tar: 0.272236 \n",
            "l0: 0.206519, l1: 0.210793, l2: 0.217551, l3: 0.207456, l4: 0.207143, l5: 0.211062, l6: 0.264068\n",
            "\n",
            "[epoch:  23/100000, batch:   708/  840, ite: 1599] train loss: 2.060736, tar: 0.272194 \n",
            "l0: 0.175840, l1: 0.177142, l2: 0.192704, l3: 0.187331, l4: 0.182927, l5: 0.191412, l6: 0.224992\n",
            "\n",
            "[epoch:  23/100000, batch:   720/  840, ite: 1600] train loss: 2.060280, tar: 0.272134 \n",
            "l0: 0.175164, l1: 0.171334, l2: 0.182353, l3: 0.193806, l4: 0.197138, l5: 0.203528, l6: 0.227108\n",
            "\n",
            "[epoch:  23/100000, batch:   732/  840, ite: 1601] train loss: 2.059837, tar: 0.272074 \n",
            "l0: 0.523670, l1: 0.529068, l2: 0.546018, l3: 0.524898, l4: 0.516969, l5: 0.564369, l6: 0.523952\n",
            "\n",
            "[epoch:  23/100000, batch:   744/  840, ite: 1602] train loss: 2.060879, tar: 0.272231 \n",
            "l0: 0.171227, l1: 0.173103, l2: 0.172590, l3: 0.195734, l4: 0.194809, l5: 0.209557, l6: 0.253931\n",
            "\n",
            "[epoch:  23/100000, batch:   756/  840, ite: 1603] train loss: 2.060448, tar: 0.272168 \n",
            "l0: 0.153929, l1: 0.159874, l2: 0.177803, l3: 0.184289, l4: 0.178091, l5: 0.185877, l6: 0.192491\n",
            "\n",
            "[epoch:  23/100000, batch:   768/  840, ite: 1604] train loss: 2.059932, tar: 0.272094 \n",
            "l0: 0.168480, l1: 0.180740, l2: 0.184499, l3: 0.172612, l4: 0.168446, l5: 0.181813, l6: 0.198062\n",
            "\n",
            "[epoch:  23/100000, batch:   780/  840, ite: 1605] train loss: 2.059430, tar: 0.272029 \n",
            "l0: 0.219780, l1: 0.219032, l2: 0.220313, l3: 0.252048, l4: 0.256043, l5: 0.262825, l6: 0.317112\n",
            "\n",
            "[epoch:  23/100000, batch:   792/  840, ite: 1606] train loss: 2.059236, tar: 0.271997 \n",
            "l0: 0.274623, l1: 0.276700, l2: 0.277569, l3: 0.284572, l4: 0.294587, l5: 0.303359, l6: 0.340971\n",
            "\n",
            "[epoch:  23/100000, batch:   804/  840, ite: 1607] train loss: 2.059232, tar: 0.271999 \n",
            "l0: 0.350569, l1: 0.327174, l2: 0.335795, l3: 0.373165, l4: 0.390407, l5: 0.386925, l6: 0.481182\n",
            "\n",
            "[epoch:  23/100000, batch:   816/  840, ite: 1608] train loss: 2.059596, tar: 0.272047 \n",
            "l0: 0.224651, l1: 0.238770, l2: 0.228802, l3: 0.223828, l4: 0.220345, l5: 0.241082, l6: 0.262029\n",
            "\n",
            "[epoch:  23/100000, batch:   828/  840, ite: 1609] train loss: 2.059335, tar: 0.272018 \n",
            "l0: 0.188197, l1: 0.194903, l2: 0.183100, l3: 0.199716, l4: 0.206532, l5: 0.227094, l6: 0.260215\n",
            "\n",
            "[epoch:  23/100000, batch:   840/  840, ite: 1610] train loss: 2.058963, tar: 0.271966 \n",
            "l0: 0.214803, l1: 0.229517, l2: 0.216732, l3: 0.244868, l4: 0.240899, l5: 0.243494, l6: 0.267090\n",
            "\n",
            "[epoch:  24/100000, batch:    12/  840, ite: 1611] train loss: 2.058714, tar: 0.271930 \n",
            "l0: 0.209634, l1: 0.230739, l2: 0.218733, l3: 0.223268, l4: 0.220229, l5: 0.217696, l6: 0.232722\n",
            "\n",
            "[epoch:  24/100000, batch:    24/  840, ite: 1612] train loss: 2.058400, tar: 0.271892 \n",
            "l0: 0.237280, l1: 0.248945, l2: 0.243850, l3: 0.251269, l4: 0.250608, l5: 0.264352, l6: 0.320734\n",
            "\n",
            "[epoch:  24/100000, batch:    36/  840, ite: 1613] train loss: 2.058250, tar: 0.271870 \n",
            "l0: 0.189095, l1: 0.169970, l2: 0.199561, l3: 0.213661, l4: 0.217273, l5: 0.263971, l6: 0.336923\n",
            "\n",
            "[epoch:  24/100000, batch:    48/  840, ite: 1614] train loss: 2.057960, tar: 0.271819 \n",
            "l0: 0.241916, l1: 0.242361, l2: 0.250579, l3: 0.266025, l4: 0.264478, l5: 0.285588, l6: 0.317329\n",
            "\n",
            "[epoch:  24/100000, batch:    60/  840, ite: 1615] train loss: 2.057843, tar: 0.271800 \n",
            "l0: 0.150828, l1: 0.155435, l2: 0.157449, l3: 0.161676, l4: 0.165411, l5: 0.179938, l6: 0.204052\n",
            "\n",
            "[epoch:  24/100000, batch:    72/  840, ite: 1616] train loss: 2.057296, tar: 0.271726 \n",
            "l0: 0.146510, l1: 0.149964, l2: 0.150144, l3: 0.159881, l4: 0.162879, l5: 0.176161, l6: 0.211247\n",
            "\n",
            "[epoch:  24/100000, batch:    84/  840, ite: 1617] train loss: 2.056739, tar: 0.271648 \n",
            "l0: 0.206209, l1: 0.211932, l2: 0.209599, l3: 0.222408, l4: 0.220575, l5: 0.233178, l6: 0.287429\n",
            "\n",
            "[epoch:  24/100000, batch:    96/  840, ite: 1618] train loss: 2.056452, tar: 0.271608 \n",
            "l0: 0.182738, l1: 0.203410, l2: 0.185681, l3: 0.169733, l4: 0.174279, l5: 0.177339, l6: 0.211168\n",
            "\n",
            "[epoch:  24/100000, batch:   108/  840, ite: 1619] train loss: 2.055987, tar: 0.271553 \n",
            "l0: 0.171779, l1: 0.169004, l2: 0.184316, l3: 0.176263, l4: 0.179893, l5: 0.202904, l6: 0.243779\n",
            "\n",
            "[epoch:  24/100000, batch:   120/  840, ite: 1620] train loss: 2.055538, tar: 0.271491 \n",
            "l0: 0.244691, l1: 0.257677, l2: 0.250916, l3: 0.254250, l4: 0.259242, l5: 0.273039, l6: 0.333230\n",
            "\n",
            "[epoch:  24/100000, batch:   132/  840, ite: 1621] train loss: 2.055425, tar: 0.271475 \n",
            "l0: 0.122270, l1: 0.123286, l2: 0.119817, l3: 0.147973, l4: 0.153634, l5: 0.161111, l6: 0.209167\n",
            "\n",
            "[epoch:  24/100000, batch:   144/  840, ite: 1622] train loss: 2.054797, tar: 0.271383 \n",
            "l0: 0.234223, l1: 0.239979, l2: 0.242111, l3: 0.260329, l4: 0.253294, l5: 0.261451, l6: 0.278312\n",
            "\n",
            "[epoch:  24/100000, batch:   156/  840, ite: 1623] train loss: 2.054622, tar: 0.271360 \n",
            "l0: 0.197116, l1: 0.204929, l2: 0.206502, l3: 0.190632, l4: 0.195482, l5: 0.201364, l6: 0.234915\n",
            "\n",
            "[epoch:  24/100000, batch:   168/  840, ite: 1624] train loss: 2.054238, tar: 0.271314 \n",
            "l0: 0.167001, l1: 0.174218, l2: 0.175083, l3: 0.183319, l4: 0.191317, l5: 0.196504, l6: 0.228969\n",
            "\n",
            "[epoch:  24/100000, batch:   180/  840, ite: 1625] train loss: 2.053784, tar: 0.271250 \n",
            "l0: 0.179822, l1: 0.178922, l2: 0.187185, l3: 0.210796, l4: 0.212136, l5: 0.220982, l6: 0.239394\n",
            "\n",
            "[epoch:  24/100000, batch:   192/  840, ite: 1626] train loss: 2.053400, tar: 0.271194 \n",
            "l0: 0.125745, l1: 0.133786, l2: 0.130399, l3: 0.132364, l4: 0.134655, l5: 0.152106, l6: 0.206868\n",
            "\n",
            "[epoch:  24/100000, batch:   204/  840, ite: 1627] train loss: 2.052762, tar: 0.271104 \n",
            "l0: 0.174799, l1: 0.168265, l2: 0.189708, l3: 0.184785, l4: 0.191333, l5: 0.195461, l6: 0.224384\n",
            "\n",
            "[epoch:  24/100000, batch:   216/  840, ite: 1628] train loss: 2.052317, tar: 0.271045 \n",
            "l0: 0.178239, l1: 0.184391, l2: 0.183907, l3: 0.189541, l4: 0.196035, l5: 0.212630, l6: 0.230662\n",
            "\n",
            "[epoch:  24/100000, batch:   228/  840, ite: 1629] train loss: 2.051902, tar: 0.270988 \n",
            "l0: 0.162751, l1: 0.160024, l2: 0.161158, l3: 0.190860, l4: 0.194791, l5: 0.215767, l6: 0.245808\n",
            "\n",
            "[epoch:  24/100000, batch:   240/  840, ite: 1630] train loss: 2.051460, tar: 0.270922 \n",
            "l0: 0.164301, l1: 0.173022, l2: 0.183846, l3: 0.152736, l4: 0.152039, l5: 0.157199, l6: 0.192429\n",
            "\n",
            "[epoch:  24/100000, batch:   252/  840, ite: 1631] train loss: 2.050922, tar: 0.270856 \n",
            "l0: 0.196409, l1: 0.203815, l2: 0.199633, l3: 0.198680, l4: 0.204703, l5: 0.209278, l6: 0.233622\n",
            "\n",
            "[epoch:  24/100000, batch:   264/  840, ite: 1632] train loss: 2.050552, tar: 0.270811 \n",
            "l0: 0.147373, l1: 0.155109, l2: 0.150189, l3: 0.154330, l4: 0.152112, l5: 0.176707, l6: 0.206154\n",
            "\n",
            "[epoch:  24/100000, batch:   276/  840, ite: 1633] train loss: 2.049995, tar: 0.270735 \n",
            "l0: 0.197431, l1: 0.204828, l2: 0.197643, l3: 0.228004, l4: 0.229712, l5: 0.229499, l6: 0.240364\n",
            "\n",
            "[epoch:  24/100000, batch:   288/  840, ite: 1634] train loss: 2.049676, tar: 0.270690 \n",
            "l0: 0.168357, l1: 0.157587, l2: 0.175238, l3: 0.194545, l4: 0.197094, l5: 0.215115, l6: 0.269695\n",
            "\n",
            "[epoch:  24/100000, batch:   300/  840, ite: 1635] train loss: 2.049265, tar: 0.270628 \n",
            "l0: 0.190004, l1: 0.193544, l2: 0.192683, l3: 0.188550, l4: 0.191401, l5: 0.206027, l6: 0.256898\n",
            "\n",
            "[epoch:  24/100000, batch:   312/  840, ite: 1636] train loss: 2.048880, tar: 0.270578 \n",
            "l0: 0.129388, l1: 0.132007, l2: 0.131651, l3: 0.138990, l4: 0.142510, l5: 0.152825, l6: 0.207019\n",
            "\n",
            "[epoch:  24/100000, batch:   324/  840, ite: 1637] train loss: 2.048260, tar: 0.270492 \n",
            "l0: 0.119354, l1: 0.129663, l2: 0.117726, l3: 0.127377, l4: 0.132199, l5: 0.154997, l6: 0.198693\n",
            "\n",
            "[epoch:  24/100000, batch:   336/  840, ite: 1638] train loss: 2.047608, tar: 0.270400 \n",
            "l0: 0.345156, l1: 0.361671, l2: 0.328560, l3: 0.398133, l4: 0.404843, l5: 0.425666, l6: 0.462907\n",
            "\n",
            "[epoch:  24/100000, batch:   348/  840, ite: 1639] train loss: 2.048022, tar: 0.270446 \n",
            "l0: 0.133789, l1: 0.156552, l2: 0.129801, l3: 0.138680, l4: 0.140653, l5: 0.159114, l6: 0.211329\n",
            "\n",
            "[epoch:  24/100000, batch:   360/  840, ite: 1640] train loss: 2.047426, tar: 0.270362 \n",
            "l0: 0.247946, l1: 0.230662, l2: 0.265757, l3: 0.291299, l4: 0.288200, l5: 0.286335, l6: 0.309318\n",
            "\n",
            "[epoch:  24/100000, batch:   372/  840, ite: 1641] train loss: 2.047348, tar: 0.270349 \n",
            "l0: 0.375578, l1: 0.336819, l2: 0.382843, l3: 0.401112, l4: 0.417863, l5: 0.417014, l6: 0.446719\n",
            "\n",
            "[epoch:  24/100000, batch:   384/  840, ite: 1642] train loss: 2.047793, tar: 0.270413 \n",
            "l0: 0.199644, l1: 0.201370, l2: 0.202865, l3: 0.220173, l4: 0.224271, l5: 0.238958, l6: 0.280493\n",
            "\n",
            "[epoch:  24/100000, batch:   396/  840, ite: 1643] train loss: 2.047501, tar: 0.270370 \n",
            "l0: 0.238415, l1: 0.230213, l2: 0.235240, l3: 0.268814, l4: 0.276688, l5: 0.300458, l6: 0.365936\n",
            "\n",
            "[epoch:  24/100000, batch:   408/  840, ite: 1644] train loss: 2.047421, tar: 0.270350 \n",
            "l0: 0.344000, l1: 0.342818, l2: 0.377032, l3: 0.387550, l4: 0.369303, l5: 0.361754, l6: 0.339179\n",
            "\n",
            "[epoch:  24/100000, batch:   420/  840, ite: 1645] train loss: 2.047709, tar: 0.270395 \n",
            "l0: 0.208808, l1: 0.225628, l2: 0.212412, l3: 0.228421, l4: 0.233293, l5: 0.232701, l6: 0.246208\n",
            "\n",
            "[epoch:  24/100000, batch:   432/  840, ite: 1646] train loss: 2.047429, tar: 0.270357 \n",
            "l0: 0.175368, l1: 0.180780, l2: 0.184742, l3: 0.211553, l4: 0.207198, l5: 0.209092, l6: 0.226773\n",
            "\n",
            "[epoch:  24/100000, batch:   444/  840, ite: 1647] train loss: 2.047033, tar: 0.270300 \n",
            "l0: 0.209247, l1: 0.199585, l2: 0.220187, l3: 0.223460, l4: 0.229832, l5: 0.251059, l6: 0.285735\n",
            "\n",
            "[epoch:  24/100000, batch:   456/  840, ite: 1648] train loss: 2.046774, tar: 0.270263 \n",
            "l0: 0.129623, l1: 0.136553, l2: 0.138072, l3: 0.142429, l4: 0.151841, l5: 0.151459, l6: 0.183271\n",
            "\n",
            "[epoch:  24/100000, batch:   468/  840, ite: 1649] train loss: 2.046159, tar: 0.270177 \n",
            "l0: 0.239074, l1: 0.252257, l2: 0.252866, l3: 0.246632, l4: 0.251178, l5: 0.258933, l6: 0.267046\n",
            "\n",
            "[epoch:  24/100000, batch:   480/  840, ite: 1650] train loss: 2.045990, tar: 0.270159 \n",
            "l0: 0.220417, l1: 0.218012, l2: 0.220799, l3: 0.248374, l4: 0.249819, l5: 0.277798, l6: 0.332264\n",
            "\n",
            "[epoch:  24/100000, batch:   492/  840, ite: 1651] train loss: 2.045822, tar: 0.270128 \n",
            "l0: 0.258066, l1: 0.263433, l2: 0.267978, l3: 0.264125, l4: 0.272818, l5: 0.304275, l6: 0.343000\n",
            "\n",
            "[epoch:  24/100000, batch:   504/  840, ite: 1652] train loss: 2.045778, tar: 0.270121 \n",
            "l0: 0.208678, l1: 0.193057, l2: 0.209683, l3: 0.239153, l4: 0.245113, l5: 0.252569, l6: 0.308816\n",
            "\n",
            "[epoch:  24/100000, batch:   516/  840, ite: 1653] train loss: 2.045543, tar: 0.270084 \n",
            "l0: 0.270563, l1: 0.263538, l2: 0.277805, l3: 0.295169, l4: 0.301771, l5: 0.327737, l6: 0.344026\n",
            "\n",
            "[epoch:  24/100000, batch:   528/  840, ite: 1654] train loss: 2.045564, tar: 0.270084 \n",
            "l0: 0.222689, l1: 0.222093, l2: 0.217235, l3: 0.262748, l4: 0.267048, l5: 0.264214, l6: 0.333714\n",
            "\n",
            "[epoch:  24/100000, batch:   540/  840, ite: 1655] train loss: 2.045410, tar: 0.270056 \n",
            "l0: 0.204010, l1: 0.202770, l2: 0.210720, l3: 0.237050, l4: 0.236746, l5: 0.224532, l6: 0.259501\n",
            "\n",
            "[epoch:  24/100000, batch:   552/  840, ite: 1656] train loss: 2.045126, tar: 0.270016 \n",
            "l0: 0.244521, l1: 0.244419, l2: 0.245061, l3: 0.255429, l4: 0.266689, l5: 0.310117, l6: 0.360824\n",
            "\n",
            "[epoch:  24/100000, batch:   564/  840, ite: 1657] train loss: 2.045054, tar: 0.270000 \n",
            "l0: 0.155659, l1: 0.149622, l2: 0.162223, l3: 0.167135, l4: 0.171635, l5: 0.197924, l6: 0.241677\n",
            "\n",
            "[epoch:  24/100000, batch:   576/  840, ite: 1658] train loss: 2.044573, tar: 0.269931 \n",
            "l0: 0.367055, l1: 0.335962, l2: 0.361299, l3: 0.392874, l4: 0.420080, l5: 0.467979, l6: 0.515497\n",
            "\n",
            "[epoch:  24/100000, batch:   588/  840, ite: 1659] train loss: 2.045064, tar: 0.269990 \n",
            "l0: 0.246229, l1: 0.244169, l2: 0.253027, l3: 0.263326, l4: 0.265482, l5: 0.275636, l6: 0.308125\n",
            "\n",
            "[epoch:  24/100000, batch:   600/  840, ite: 1660] train loss: 2.044950, tar: 0.269976 \n",
            "l0: 0.103009, l1: 0.100029, l2: 0.107697, l3: 0.128460, l4: 0.133181, l5: 0.151159, l6: 0.170070\n",
            "\n",
            "[epoch:  24/100000, batch:   612/  840, ite: 1661] train loss: 2.044257, tar: 0.269875 \n",
            "l0: 0.174374, l1: 0.182548, l2: 0.182834, l3: 0.193645, l4: 0.190203, l5: 0.210224, l6: 0.228511\n",
            "\n",
            "[epoch:  24/100000, batch:   624/  840, ite: 1662] train loss: 2.043847, tar: 0.269818 \n",
            "l0: 0.203258, l1: 0.204376, l2: 0.208639, l3: 0.223317, l4: 0.221582, l5: 0.251931, l6: 0.274137\n",
            "\n",
            "[epoch:  24/100000, batch:   636/  840, ite: 1663] train loss: 2.043572, tar: 0.269778 \n",
            "l0: 0.178433, l1: 0.178914, l2: 0.171264, l3: 0.184468, l4: 0.190790, l5: 0.225412, l6: 0.293920\n",
            "\n",
            "[epoch:  24/100000, batch:   648/  840, ite: 1664] train loss: 2.043200, tar: 0.269723 \n",
            "l0: 0.278676, l1: 0.270874, l2: 0.271162, l3: 0.295566, l4: 0.310270, l5: 0.346698, l6: 0.424911\n",
            "\n",
            "[epoch:  24/100000, batch:   660/  840, ite: 1665] train loss: 2.043293, tar: 0.269728 \n",
            "l0: 0.153026, l1: 0.152664, l2: 0.157898, l3: 0.163146, l4: 0.163757, l5: 0.164371, l6: 0.213630\n",
            "\n",
            "[epoch:  24/100000, batch:   672/  840, ite: 1666] train loss: 2.042768, tar: 0.269658 \n",
            "l0: 0.171240, l1: 0.173548, l2: 0.184490, l3: 0.188166, l4: 0.186586, l5: 0.188957, l6: 0.236686\n",
            "\n",
            "[epoch:  24/100000, batch:   684/  840, ite: 1667] train loss: 2.042340, tar: 0.269599 \n",
            "l0: 0.145635, l1: 0.146723, l2: 0.153420, l3: 0.162259, l4: 0.162075, l5: 0.183002, l6: 0.215621\n",
            "\n",
            "[epoch:  24/100000, batch:   696/  840, ite: 1668] train loss: 2.041816, tar: 0.269525 \n",
            "l0: 0.205473, l1: 0.209751, l2: 0.221384, l3: 0.217688, l4: 0.226567, l5: 0.231635, l6: 0.262678\n",
            "\n",
            "[epoch:  24/100000, batch:   708/  840, ite: 1669] train loss: 2.041537, tar: 0.269486 \n",
            "l0: 0.222758, l1: 0.226564, l2: 0.222644, l3: 0.227682, l4: 0.245938, l5: 0.313520, l6: 0.353180\n",
            "\n",
            "[epoch:  24/100000, batch:   720/  840, ite: 1670] train loss: 2.041399, tar: 0.269458 \n",
            "l0: 0.178273, l1: 0.177205, l2: 0.187009, l3: 0.191560, l4: 0.198945, l5: 0.228419, l6: 0.283409\n",
            "\n",
            "[epoch:  24/100000, batch:   732/  840, ite: 1671] train loss: 2.041042, tar: 0.269404 \n",
            "l0: 0.124554, l1: 0.120234, l2: 0.139244, l3: 0.155736, l4: 0.156732, l5: 0.184126, l6: 0.206981\n",
            "\n",
            "[epoch:  24/100000, batch:   744/  840, ite: 1672] train loss: 2.040472, tar: 0.269317 \n",
            "l0: 0.240815, l1: 0.258101, l2: 0.247119, l3: 0.234579, l4: 0.240694, l5: 0.279769, l6: 0.296732\n",
            "\n",
            "[epoch:  24/100000, batch:   756/  840, ite: 1673] train loss: 2.040327, tar: 0.269300 \n",
            "l0: 0.180294, l1: 0.184729, l2: 0.191629, l3: 0.197740, l4: 0.206966, l5: 0.237695, l6: 0.219633\n",
            "\n",
            "[epoch:  24/100000, batch:   768/  840, ite: 1674] train loss: 2.039955, tar: 0.269247 \n",
            "l0: 0.327164, l1: 0.365469, l2: 0.333194, l3: 0.348891, l4: 0.334726, l5: 0.336036, l6: 0.317858\n",
            "\n",
            "[epoch:  24/100000, batch:   780/  840, ite: 1675] train loss: 2.040148, tar: 0.269282 \n",
            "l0: 0.213491, l1: 0.222304, l2: 0.224401, l3: 0.237538, l4: 0.238415, l5: 0.249900, l6: 0.298873\n",
            "\n",
            "[epoch:  24/100000, batch:   792/  840, ite: 1676] train loss: 2.039937, tar: 0.269248 \n",
            "l0: 0.213875, l1: 0.230160, l2: 0.216314, l3: 0.226503, l4: 0.231066, l5: 0.245168, l6: 0.295893\n",
            "\n",
            "[epoch:  24/100000, batch:   804/  840, ite: 1677] train loss: 2.039709, tar: 0.269215 \n",
            "l0: 0.181589, l1: 0.172198, l2: 0.184406, l3: 0.203001, l4: 0.204234, l5: 0.258318, l6: 0.327858\n",
            "\n",
            "[epoch:  24/100000, batch:   816/  840, ite: 1678] train loss: 2.039407, tar: 0.269163 \n",
            "l0: 0.116750, l1: 0.115093, l2: 0.122266, l3: 0.128895, l4: 0.135559, l5: 0.157688, l6: 0.203847\n",
            "\n",
            "[epoch:  24/100000, batch:   828/  840, ite: 1679] train loss: 2.038775, tar: 0.269072 \n",
            "l0: 0.182072, l1: 0.194008, l2: 0.201696, l3: 0.188710, l4: 0.192996, l5: 0.185639, l6: 0.212903\n",
            "\n",
            "[epoch:  24/100000, batch:   840/  840, ite: 1680] train loss: 2.038370, tar: 0.269020 \n",
            "l0: 0.247934, l1: 0.241703, l2: 0.254186, l3: 0.250678, l4: 0.250245, l5: 0.292857, l6: 0.352172\n",
            "\n",
            "[epoch:  25/100000, batch:    12/  840, ite: 1681] train loss: 2.038282, tar: 0.269008 \n",
            "l0: 0.296463, l1: 0.320608, l2: 0.298022, l3: 0.299500, l4: 0.305645, l5: 0.338812, l6: 0.347993\n",
            "\n",
            "[epoch:  25/100000, batch:    24/  840, ite: 1682] train loss: 2.038382, tar: 0.269024 \n",
            "l0: 0.189959, l1: 0.206022, l2: 0.187605, l3: 0.198001, l4: 0.196869, l5: 0.205792, l6: 0.260664\n",
            "\n",
            "[epoch:  25/100000, batch:    36/  840, ite: 1683] train loss: 2.038029, tar: 0.268977 \n",
            "l0: 0.237953, l1: 0.241486, l2: 0.241046, l3: 0.240095, l4: 0.245367, l5: 0.276087, l6: 0.334484\n",
            "\n",
            "[epoch:  25/100000, batch:    48/  840, ite: 1684] train loss: 2.037898, tar: 0.268959 \n",
            "l0: 0.227596, l1: 0.215708, l2: 0.229090, l3: 0.280414, l4: 0.290345, l5: 0.291243, l6: 0.346362\n",
            "\n",
            "[epoch:  25/100000, batch:    60/  840, ite: 1685] train loss: 2.037805, tar: 0.268934 \n",
            "l0: 0.122813, l1: 0.130925, l2: 0.132503, l3: 0.124899, l4: 0.122842, l5: 0.136818, l6: 0.177850\n",
            "\n",
            "[epoch:  25/100000, batch:    72/  840, ite: 1686] train loss: 2.037159, tar: 0.268848 \n",
            "l0: 0.136517, l1: 0.152173, l2: 0.138914, l3: 0.153081, l4: 0.152379, l5: 0.169764, l6: 0.201154\n",
            "\n",
            "[epoch:  25/100000, batch:    84/  840, ite: 1687] train loss: 2.036606, tar: 0.268769 \n",
            "l0: 0.194061, l1: 0.218772, l2: 0.197238, l3: 0.208928, l4: 0.213545, l5: 0.228253, l6: 0.271597\n",
            "\n",
            "[epoch:  25/100000, batch:    96/  840, ite: 1688] train loss: 2.036307, tar: 0.268725 \n",
            "l0: 0.206468, l1: 0.208050, l2: 0.212925, l3: 0.227059, l4: 0.236187, l5: 0.240976, l6: 0.275931\n",
            "\n",
            "[epoch:  25/100000, batch:   108/  840, ite: 1689] train loss: 2.036053, tar: 0.268688 \n",
            "l0: 0.222950, l1: 0.234556, l2: 0.241413, l3: 0.220726, l4: 0.226010, l5: 0.217052, l6: 0.271722\n",
            "\n",
            "[epoch:  25/100000, batch:   120/  840, ite: 1690] train loss: 2.035815, tar: 0.268661 \n",
            "l0: 0.235959, l1: 0.237034, l2: 0.245107, l3: 0.251929, l4: 0.258570, l5: 0.273771, l6: 0.311842\n",
            "\n",
            "[epoch:  25/100000, batch:   132/  840, ite: 1691] train loss: 2.035684, tar: 0.268642 \n",
            "l0: 0.176877, l1: 0.178043, l2: 0.183362, l3: 0.204317, l4: 0.206782, l5: 0.220592, l6: 0.253324\n",
            "\n",
            "[epoch:  25/100000, batch:   144/  840, ite: 1692] train loss: 2.035323, tar: 0.268587 \n",
            "l0: 0.165929, l1: 0.178306, l2: 0.178798, l3: 0.160532, l4: 0.168907, l5: 0.186696, l6: 0.218994\n",
            "\n",
            "[epoch:  25/100000, batch:   156/  840, ite: 1693] train loss: 2.034864, tar: 0.268527 \n",
            "l0: 0.289445, l1: 0.320211, l2: 0.295183, l3: 0.285697, l4: 0.288328, l5: 0.281734, l6: 0.301626\n",
            "\n",
            "[epoch:  25/100000, batch:   168/  840, ite: 1694] train loss: 2.034880, tar: 0.268539 \n",
            "l0: 0.133212, l1: 0.151140, l2: 0.137637, l3: 0.149140, l4: 0.155315, l5: 0.164215, l6: 0.189653\n",
            "\n",
            "[epoch:  25/100000, batch:   180/  840, ite: 1695] train loss: 2.034317, tar: 0.268459 \n",
            "l0: 0.178760, l1: 0.181611, l2: 0.183920, l3: 0.201163, l4: 0.207185, l5: 0.215766, l6: 0.239944\n",
            "\n",
            "[epoch:  25/100000, batch:   192/  840, ite: 1696] train loss: 2.033947, tar: 0.268406 \n",
            "l0: 0.173417, l1: 0.190080, l2: 0.173404, l3: 0.181944, l4: 0.186564, l5: 0.189422, l6: 0.230745\n",
            "\n",
            "[epoch:  25/100000, batch:   204/  840, ite: 1697] train loss: 2.033530, tar: 0.268350 \n",
            "l0: 0.307736, l1: 0.319977, l2: 0.323051, l3: 0.328173, l4: 0.313315, l5: 0.301277, l6: 0.324666\n",
            "\n",
            "[epoch:  25/100000, batch:   216/  840, ite: 1698] train loss: 2.033639, tar: 0.268374 \n",
            "l0: 0.140931, l1: 0.136734, l2: 0.152866, l3: 0.155507, l4: 0.156385, l5: 0.176852, l6: 0.227613\n",
            "\n",
            "[epoch:  25/100000, batch:   228/  840, ite: 1699] train loss: 2.033117, tar: 0.268299 \n",
            "l0: 0.179595, l1: 0.183786, l2: 0.180363, l3: 0.196570, l4: 0.192640, l5: 0.205560, l6: 0.249624\n",
            "\n",
            "[epoch:  25/100000, batch:   240/  840, ite: 1700] train loss: 2.032737, tar: 0.268246 \n",
            "l0: 0.332262, l1: 0.337434, l2: 0.351392, l3: 0.303563, l4: 0.300899, l5: 0.315930, l6: 0.357257\n",
            "\n",
            "[epoch:  25/100000, batch:   252/  840, ite: 1701] train loss: 2.032894, tar: 0.268284 \n",
            "l0: 0.278098, l1: 0.267168, l2: 0.287923, l3: 0.282088, l4: 0.285344, l5: 0.311917, l6: 0.331845\n",
            "\n",
            "[epoch:  25/100000, batch:   264/  840, ite: 1702] train loss: 2.032901, tar: 0.268290 \n",
            "l0: 0.154116, l1: 0.152934, l2: 0.163208, l3: 0.176763, l4: 0.180932, l5: 0.203452, l6: 0.223964\n",
            "\n",
            "[epoch:  25/100000, batch:   276/  840, ite: 1703] train loss: 2.032444, tar: 0.268223 \n",
            "l0: 0.117040, l1: 0.121276, l2: 0.122104, l3: 0.136572, l4: 0.139839, l5: 0.149114, l6: 0.166881\n",
            "\n",
            "[epoch:  25/100000, batch:   288/  840, ite: 1704] train loss: 2.031811, tar: 0.268134 \n",
            "l0: 0.158991, l1: 0.160009, l2: 0.168010, l3: 0.184631, l4: 0.188870, l5: 0.201973, l6: 0.219917\n",
            "\n",
            "[epoch:  25/100000, batch:   300/  840, ite: 1705] train loss: 2.031371, tar: 0.268070 \n",
            "l0: 0.253842, l1: 0.238261, l2: 0.263782, l3: 0.290618, l4: 0.300303, l5: 0.308431, l6: 0.365419\n",
            "\n",
            "[epoch:  25/100000, batch:   312/  840, ite: 1706] train loss: 2.031365, tar: 0.268062 \n",
            "l0: 0.146285, l1: 0.152210, l2: 0.146705, l3: 0.169294, l4: 0.174603, l5: 0.178251, l6: 0.205903\n",
            "\n",
            "[epoch:  25/100000, batch:   324/  840, ite: 1707] train loss: 2.030862, tar: 0.267990 \n",
            "l0: 0.071668, l1: 0.072970, l2: 0.075720, l3: 0.080220, l4: 0.078279, l5: 0.092735, l6: 0.119980\n",
            "\n",
            "[epoch:  25/100000, batch:   336/  840, ite: 1708] train loss: 2.030020, tar: 0.267875 \n",
            "l0: 0.207028, l1: 0.205366, l2: 0.218689, l3: 0.213529, l4: 0.219210, l5: 0.225229, l6: 0.262407\n",
            "\n",
            "[epoch:  25/100000, batch:   348/  840, ite: 1709] train loss: 2.029740, tar: 0.267840 \n",
            "l0: 0.232966, l1: 0.233450, l2: 0.238910, l3: 0.261879, l4: 0.263624, l5: 0.272880, l6: 0.292315\n",
            "\n",
            "[epoch:  25/100000, batch:   360/  840, ite: 1710] train loss: 2.029603, tar: 0.267819 \n",
            "l0: 0.158015, l1: 0.154427, l2: 0.168031, l3: 0.185443, l4: 0.185742, l5: 0.202706, l6: 0.249384\n",
            "\n",
            "[epoch:  25/100000, batch:   372/  840, ite: 1711] train loss: 2.029179, tar: 0.267755 \n",
            "l0: 0.154202, l1: 0.155677, l2: 0.158432, l3: 0.177096, l4: 0.181440, l5: 0.186297, l6: 0.211860\n",
            "\n",
            "[epoch:  25/100000, batch:   384/  840, ite: 1712] train loss: 2.028709, tar: 0.267689 \n",
            "l0: 0.151335, l1: 0.152631, l2: 0.152352, l3: 0.184021, l4: 0.183401, l5: 0.195125, l6: 0.249199\n",
            "\n",
            "[epoch:  25/100000, batch:   396/  840, ite: 1713] train loss: 2.028265, tar: 0.267621 \n",
            "l0: 0.238581, l1: 0.240511, l2: 0.249250, l3: 0.245873, l4: 0.243707, l5: 0.245672, l6: 0.269335\n",
            "\n",
            "[epoch:  25/100000, batch:   408/  840, ite: 1714] train loss: 2.028093, tar: 0.267604 \n",
            "l0: 0.132871, l1: 0.148152, l2: 0.138294, l3: 0.149838, l4: 0.147947, l5: 0.134595, l6: 0.177006\n",
            "\n",
            "[epoch:  25/100000, batch:   420/  840, ite: 1715] train loss: 2.027510, tar: 0.267526 \n",
            "l0: 0.185784, l1: 0.191572, l2: 0.192243, l3: 0.181070, l4: 0.179471, l5: 0.204384, l6: 0.244972\n",
            "\n",
            "[epoch:  25/100000, batch:   432/  840, ite: 1716] train loss: 2.027133, tar: 0.267478 \n",
            "l0: 0.215735, l1: 0.208650, l2: 0.223796, l3: 0.226210, l4: 0.243632, l5: 0.244164, l6: 0.273952\n",
            "\n",
            "[epoch:  25/100000, batch:   444/  840, ite: 1717] train loss: 2.026905, tar: 0.267448 \n",
            "l0: 0.168295, l1: 0.175221, l2: 0.171480, l3: 0.172511, l4: 0.182592, l5: 0.222795, l6: 0.253542\n",
            "\n",
            "[epoch:  25/100000, batch:   456/  840, ite: 1718] train loss: 2.026509, tar: 0.267390 \n",
            "l0: 0.156026, l1: 0.152996, l2: 0.159614, l3: 0.170762, l4: 0.176096, l5: 0.195027, l6: 0.232566\n",
            "\n",
            "[epoch:  25/100000, batch:   468/  840, ite: 1719] train loss: 2.026053, tar: 0.267325 \n",
            "l0: 0.088424, l1: 0.096082, l2: 0.087381, l3: 0.090215, l4: 0.097661, l5: 0.114412, l6: 0.158913\n",
            "\n",
            "[epoch:  25/100000, batch:   480/  840, ite: 1720] train loss: 2.025301, tar: 0.267221 \n",
            "l0: 0.276987, l1: 0.257459, l2: 0.292963, l3: 0.298689, l4: 0.307928, l5: 0.332295, l6: 0.381378\n",
            "\n",
            "[epoch:  25/100000, batch:   492/  840, ite: 1721] train loss: 2.025373, tar: 0.267227 \n",
            "l0: 0.102090, l1: 0.101153, l2: 0.103555, l3: 0.111082, l4: 0.124079, l5: 0.137295, l6: 0.176366\n",
            "\n",
            "[epoch:  25/100000, batch:   504/  840, ite: 1722] train loss: 2.024693, tar: 0.267131 \n",
            "l0: 0.182932, l1: 0.174254, l2: 0.197141, l3: 0.217376, l4: 0.213783, l5: 0.225917, l6: 0.284700\n",
            "\n",
            "[epoch:  25/100000, batch:   516/  840, ite: 1723] train loss: 2.024386, tar: 0.267082 \n",
            "l0: 0.161730, l1: 0.186648, l2: 0.175348, l3: 0.159889, l4: 0.150759, l5: 0.164450, l6: 0.191250\n",
            "\n",
            "[epoch:  25/100000, batch:   528/  840, ite: 1724] train loss: 2.023902, tar: 0.267021 \n",
            "l0: 0.182549, l1: 0.176995, l2: 0.192247, l3: 0.194430, l4: 0.205495, l5: 0.218753, l6: 0.253713\n",
            "\n",
            "[epoch:  25/100000, batch:   540/  840, ite: 1725] train loss: 2.023555, tar: 0.266972 \n",
            "l0: 0.153443, l1: 0.154893, l2: 0.164076, l3: 0.180561, l4: 0.176164, l5: 0.204155, l6: 0.204544\n",
            "\n",
            "[epoch:  25/100000, batch:   552/  840, ite: 1726] train loss: 2.023099, tar: 0.266906 \n",
            "l0: 0.302249, l1: 0.294322, l2: 0.313461, l3: 0.326870, l4: 0.325020, l5: 0.319023, l6: 0.337805\n",
            "\n",
            "[epoch:  25/100000, batch:   564/  840, ite: 1727] train loss: 2.023213, tar: 0.266927 \n",
            "l0: 0.265325, l1: 0.282911, l2: 0.277454, l3: 0.271005, l4: 0.254389, l5: 0.267905, l6: 0.306967\n",
            "\n",
            "[epoch:  25/100000, batch:   576/  840, ite: 1728] train loss: 2.023157, tar: 0.266926 \n",
            "l0: 0.227289, l1: 0.242136, l2: 0.221589, l3: 0.237839, l4: 0.241116, l5: 0.255782, l6: 0.294706\n",
            "\n",
            "[epoch:  25/100000, batch:   588/  840, ite: 1729] train loss: 2.022982, tar: 0.266903 \n",
            "l0: 0.178378, l1: 0.188519, l2: 0.186424, l3: 0.177994, l4: 0.183333, l5: 0.195203, l6: 0.219152\n",
            "\n",
            "[epoch:  25/100000, batch:   600/  840, ite: 1730] train loss: 2.022580, tar: 0.266852 \n",
            "l0: 0.351591, l1: 0.355012, l2: 0.358386, l3: 0.391737, l4: 0.392439, l5: 0.384862, l6: 0.404181\n",
            "\n",
            "[epoch:  25/100000, batch:   612/  840, ite: 1731] train loss: 2.022936, tar: 0.266901 \n",
            "l0: 0.228080, l1: 0.230607, l2: 0.243167, l3: 0.263887, l4: 0.255456, l5: 0.270701, l6: 0.291632\n",
            "\n",
            "[epoch:  25/100000, batch:   624/  840, ite: 1732] train loss: 2.022798, tar: 0.266878 \n",
            "l0: 0.241154, l1: 0.245689, l2: 0.229692, l3: 0.263236, l4: 0.278877, l5: 0.297831, l6: 0.348594\n",
            "\n",
            "[epoch:  25/100000, batch:   636/  840, ite: 1733] train loss: 2.022730, tar: 0.266863 \n",
            "l0: 0.210810, l1: 0.195538, l2: 0.228550, l3: 0.250014, l4: 0.251242, l5: 0.252129, l6: 0.289518\n",
            "\n",
            "[epoch:  25/100000, batch:   648/  840, ite: 1734] train loss: 2.022531, tar: 0.266831 \n",
            "l0: 0.273863, l1: 0.286326, l2: 0.273435, l3: 0.258151, l4: 0.267057, l5: 0.295260, l6: 0.317573\n",
            "\n",
            "[epoch:  25/100000, batch:   660/  840, ite: 1735] train loss: 2.022502, tar: 0.266835 \n",
            "l0: 0.265694, l1: 0.262939, l2: 0.271177, l3: 0.265795, l4: 0.278369, l5: 0.279444, l6: 0.296181\n",
            "\n",
            "[epoch:  25/100000, batch:   672/  840, ite: 1736] train loss: 2.022442, tar: 0.266834 \n",
            "l0: 0.133028, l1: 0.141188, l2: 0.140725, l3: 0.138726, l4: 0.140794, l5: 0.146921, l6: 0.192306\n",
            "\n",
            "[epoch:  25/100000, batch:   684/  840, ite: 1737] train loss: 2.021873, tar: 0.266757 \n",
            "l0: 0.132703, l1: 0.140334, l2: 0.145293, l3: 0.154969, l4: 0.151103, l5: 0.145769, l6: 0.168291\n",
            "\n",
            "[epoch:  25/100000, batch:   696/  840, ite: 1738] train loss: 2.021307, tar: 0.266680 \n",
            "l0: 0.165518, l1: 0.180305, l2: 0.167534, l3: 0.191255, l4: 0.187616, l5: 0.193162, l6: 0.232960\n",
            "\n",
            "[epoch:  25/100000, batch:   708/  840, ite: 1739] train loss: 2.020903, tar: 0.266622 \n",
            "l0: 0.146666, l1: 0.158904, l2: 0.156322, l3: 0.187011, l4: 0.175997, l5: 0.169992, l6: 0.187372\n",
            "\n",
            "[epoch:  25/100000, batch:   720/  840, ite: 1740] train loss: 2.020421, tar: 0.266553 \n",
            "l0: 0.207171, l1: 0.204172, l2: 0.225377, l3: 0.236921, l4: 0.230137, l5: 0.228040, l6: 0.254780\n",
            "\n",
            "[epoch:  25/100000, batch:   732/  840, ite: 1741] train loss: 2.020172, tar: 0.266519 \n",
            "l0: 0.168768, l1: 0.173382, l2: 0.176385, l3: 0.181419, l4: 0.185215, l5: 0.207266, l6: 0.244919\n",
            "\n",
            "[epoch:  25/100000, batch:   744/  840, ite: 1742] train loss: 2.019780, tar: 0.266463 \n",
            "l0: 0.220487, l1: 0.211567, l2: 0.231892, l3: 0.259054, l4: 0.261093, l5: 0.282338, l6: 0.298943\n",
            "\n",
            "[epoch:  25/100000, batch:   756/  840, ite: 1743] train loss: 2.019634, tar: 0.266437 \n",
            "l0: 0.239757, l1: 0.239219, l2: 0.247969, l3: 0.248449, l4: 0.269207, l5: 0.285854, l6: 0.301818\n",
            "\n",
            "[epoch:  25/100000, batch:   768/  840, ite: 1744] train loss: 2.019527, tar: 0.266421 \n",
            "l0: 0.159021, l1: 0.162387, l2: 0.175756, l3: 0.158031, l4: 0.160861, l5: 0.166736, l6: 0.191773\n",
            "\n",
            "[epoch:  25/100000, batch:   780/  840, ite: 1745] train loss: 2.019042, tar: 0.266360 \n",
            "l0: 0.138855, l1: 0.148239, l2: 0.141558, l3: 0.145700, l4: 0.157197, l5: 0.161067, l6: 0.188031\n",
            "\n",
            "[epoch:  25/100000, batch:   792/  840, ite: 1746] train loss: 2.018505, tar: 0.266287 \n",
            "l0: 0.203706, l1: 0.247468, l2: 0.205243, l3: 0.210836, l4: 0.202653, l5: 0.246925, l6: 0.267673\n",
            "\n",
            "[epoch:  25/100000, batch:   804/  840, ite: 1747] train loss: 2.018257, tar: 0.266251 \n",
            "l0: 0.206457, l1: 0.202976, l2: 0.209132, l3: 0.223080, l4: 0.236919, l5: 0.276584, l6: 0.316640\n",
            "\n",
            "[epoch:  25/100000, batch:   816/  840, ite: 1748] train loss: 2.018058, tar: 0.266217 \n",
            "l0: 0.148787, l1: 0.152800, l2: 0.157083, l3: 0.162574, l4: 0.167622, l5: 0.177908, l6: 0.207566\n",
            "\n",
            "[epoch:  25/100000, batch:   828/  840, ite: 1749] train loss: 2.017576, tar: 0.266150 \n",
            "l0: 0.405012, l1: 0.388588, l2: 0.422985, l3: 0.438423, l4: 0.436864, l5: 0.478762, l6: 0.478519\n",
            "\n",
            "[epoch:  25/100000, batch:   840/  840, ite: 1750] train loss: 2.018165, tar: 0.266229 \n",
            "l0: 0.131920, l1: 0.139551, l2: 0.136586, l3: 0.133389, l4: 0.139187, l5: 0.159517, l6: 0.209563\n",
            "\n",
            "[epoch:  26/100000, batch:    12/  840, ite: 1751] train loss: 2.017612, tar: 0.266152 \n",
            "l0: 0.144195, l1: 0.151543, l2: 0.150076, l3: 0.156441, l4: 0.162529, l5: 0.181543, l6: 0.214273\n",
            "\n",
            "[epoch:  26/100000, batch:    24/  840, ite: 1752] train loss: 2.017123, tar: 0.266083 \n",
            "l0: 0.146724, l1: 0.151329, l2: 0.153169, l3: 0.148446, l4: 0.152132, l5: 0.182455, l6: 0.238034\n",
            "\n",
            "[epoch:  26/100000, batch:    36/  840, ite: 1753] train loss: 2.016641, tar: 0.266015 \n",
            "l0: 0.281049, l1: 0.261708, l2: 0.308297, l3: 0.314306, l4: 0.330809, l5: 0.329077, l6: 0.337319\n",
            "\n",
            "[epoch:  26/100000, batch:    48/  840, ite: 1754] train loss: 2.016725, tar: 0.266023 \n",
            "l0: 0.162369, l1: 0.157731, l2: 0.181908, l3: 0.201697, l4: 0.199106, l5: 0.216679, l6: 0.242662\n",
            "\n",
            "[epoch:  26/100000, batch:    60/  840, ite: 1755] train loss: 2.016351, tar: 0.265964 \n",
            "l0: 0.188728, l1: 0.192913, l2: 0.201935, l3: 0.195833, l4: 0.197220, l5: 0.210975, l6: 0.226606\n",
            "\n",
            "[epoch:  26/100000, batch:    72/  840, ite: 1756] train loss: 2.016009, tar: 0.265920 \n",
            "l0: 0.157032, l1: 0.166320, l2: 0.166375, l3: 0.165877, l4: 0.161285, l5: 0.177609, l6: 0.200408\n",
            "\n",
            "[epoch:  26/100000, batch:    84/  840, ite: 1757] train loss: 2.015541, tar: 0.265858 \n",
            "l0: 0.157690, l1: 0.162821, l2: 0.164605, l3: 0.182458, l4: 0.178849, l5: 0.189633, l6: 0.209569\n",
            "\n",
            "[epoch:  26/100000, batch:    96/  840, ite: 1758] train loss: 2.015103, tar: 0.265797 \n",
            "l0: 0.161310, l1: 0.148910, l2: 0.176226, l3: 0.190730, l4: 0.185706, l5: 0.201916, l6: 0.223479\n",
            "\n",
            "[epoch:  26/100000, batch:   108/  840, ite: 1759] train loss: 2.014690, tar: 0.265737 \n",
            "l0: 0.236494, l1: 0.245704, l2: 0.243432, l3: 0.253808, l4: 0.258050, l5: 0.271533, l6: 0.300816\n",
            "\n",
            "[epoch:  26/100000, batch:   120/  840, ite: 1760] train loss: 2.014574, tar: 0.265720 \n",
            "l0: 0.168705, l1: 0.187850, l2: 0.172081, l3: 0.170836, l4: 0.171477, l5: 0.181600, l6: 0.217497\n",
            "\n",
            "[epoch:  26/100000, batch:   132/  840, ite: 1761] train loss: 2.014151, tar: 0.265665 \n",
            "l0: 0.190601, l1: 0.191972, l2: 0.200403, l3: 0.188932, l4: 0.186319, l5: 0.219944, l6: 0.275073\n",
            "\n",
            "[epoch:  26/100000, batch:   144/  840, ite: 1762] train loss: 2.013833, tar: 0.265623 \n",
            "l0: 0.198731, l1: 0.207387, l2: 0.199817, l3: 0.211978, l4: 0.217618, l5: 0.241800, l6: 0.296160\n",
            "\n",
            "[epoch:  26/100000, batch:   156/  840, ite: 1763] train loss: 2.013583, tar: 0.265585 \n",
            "l0: 0.057801, l1: 0.060037, l2: 0.059571, l3: 0.062942, l4: 0.068582, l5: 0.090718, l6: 0.128287\n",
            "\n",
            "[epoch:  26/100000, batch:   168/  840, ite: 1764] train loss: 2.012741, tar: 0.265467 \n",
            "l0: 0.145630, l1: 0.148370, l2: 0.151253, l3: 0.154725, l4: 0.150353, l5: 0.156451, l6: 0.188960\n",
            "\n",
            "[epoch:  26/100000, batch:   180/  840, ite: 1765] train loss: 2.012221, tar: 0.265399 \n",
            "l0: 0.224854, l1: 0.239193, l2: 0.228051, l3: 0.245088, l4: 0.245109, l5: 0.269509, l6: 0.293763\n",
            "\n",
            "[epoch:  26/100000, batch:   192/  840, ite: 1766] train loss: 2.012070, tar: 0.265376 \n",
            "l0: 0.204150, l1: 0.206909, l2: 0.208745, l3: 0.202267, l4: 0.205444, l5: 0.225936, l6: 0.269177\n",
            "\n",
            "[epoch:  26/100000, batch:   204/  840, ite: 1767] train loss: 2.011793, tar: 0.265342 \n",
            "l0: 0.169440, l1: 0.161834, l2: 0.185188, l3: 0.181387, l4: 0.184760, l5: 0.198943, l6: 0.221526\n",
            "\n",
            "[epoch:  26/100000, batch:   216/  840, ite: 1768] train loss: 2.011392, tar: 0.265287 \n",
            "l0: 0.177174, l1: 0.182940, l2: 0.180807, l3: 0.202708, l4: 0.193438, l5: 0.197176, l6: 0.234891\n",
            "\n",
            "[epoch:  26/100000, batch:   228/  840, ite: 1769] train loss: 2.011029, tar: 0.265238 \n",
            "l0: 0.302959, l1: 0.298240, l2: 0.320340, l3: 0.306138, l4: 0.309616, l5: 0.334535, l6: 0.379124\n",
            "\n",
            "[epoch:  26/100000, batch:   240/  840, ite: 1770] train loss: 2.011165, tar: 0.265259 \n",
            "l0: 0.198630, l1: 0.202287, l2: 0.207563, l3: 0.224032, l4: 0.234446, l5: 0.232701, l6: 0.252482\n",
            "\n",
            "[epoch:  26/100000, batch:   252/  840, ite: 1771] train loss: 2.010906, tar: 0.265221 \n",
            "l0: 0.148556, l1: 0.151983, l2: 0.158647, l3: 0.154907, l4: 0.153794, l5: 0.167592, l6: 0.203329\n",
            "\n",
            "[epoch:  26/100000, batch:   264/  840, ite: 1772] train loss: 2.010413, tar: 0.265155 \n",
            "l0: 0.122507, l1: 0.115640, l2: 0.123700, l3: 0.151041, l4: 0.167021, l5: 0.171985, l6: 0.198967\n",
            "\n",
            "[epoch:  26/100000, batch:   276/  840, ite: 1773] train loss: 2.009872, tar: 0.265075 \n",
            "l0: 0.220025, l1: 0.215997, l2: 0.205719, l3: 0.212698, l4: 0.235744, l5: 0.245734, l6: 0.285236\n",
            "\n",
            "[epoch:  26/100000, batch:   288/  840, ite: 1774] train loss: 2.009653, tar: 0.265050 \n",
            "l0: 0.265948, l1: 0.276950, l2: 0.258746, l3: 0.280059, l4: 0.278369, l5: 0.281427, l6: 0.339138\n",
            "\n",
            "[epoch:  26/100000, batch:   300/  840, ite: 1775] train loss: 2.009637, tar: 0.265050 \n",
            "l0: 0.189438, l1: 0.194218, l2: 0.194121, l3: 0.213330, l4: 0.217414, l5: 0.229218, l6: 0.264489\n",
            "\n",
            "[epoch:  26/100000, batch:   312/  840, ite: 1776] train loss: 2.009351, tar: 0.265007 \n",
            "l0: 0.166557, l1: 0.161990, l2: 0.174412, l3: 0.181375, l4: 0.188095, l5: 0.195533, l6: 0.237263\n",
            "\n",
            "[epoch:  26/100000, batch:   324/  840, ite: 1777] train loss: 2.008955, tar: 0.264952 \n",
            "l0: 0.177135, l1: 0.176857, l2: 0.182287, l3: 0.199112, l4: 0.203822, l5: 0.205256, l6: 0.237949\n",
            "\n",
            "[epoch:  26/100000, batch:   336/  840, ite: 1778] train loss: 2.008602, tar: 0.264903 \n",
            "l0: 0.185526, l1: 0.179808, l2: 0.190332, l3: 0.204529, l4: 0.211559, l5: 0.207148, l6: 0.237814\n",
            "\n",
            "[epoch:  26/100000, batch:   348/  840, ite: 1779] train loss: 2.008270, tar: 0.264858 \n",
            "l0: 0.213430, l1: 0.217077, l2: 0.220772, l3: 0.225416, l4: 0.229347, l5: 0.243784, l6: 0.279527\n",
            "\n",
            "[epoch:  26/100000, batch:   360/  840, ite: 1780] train loss: 2.008057, tar: 0.264829 \n",
            "l0: 0.214685, l1: 0.208886, l2: 0.232767, l3: 0.230770, l4: 0.237922, l5: 0.259236, l6: 0.288948\n",
            "\n",
            "[epoch:  26/100000, batch:   372/  840, ite: 1781] train loss: 2.007869, tar: 0.264801 \n",
            "l0: 0.302500, l1: 0.313963, l2: 0.312051, l3: 0.341682, l4: 0.332415, l5: 0.314723, l6: 0.343709\n",
            "\n",
            "[epoch:  26/100000, batch:   384/  840, ite: 1782] train loss: 2.008011, tar: 0.264822 \n",
            "l0: 0.279395, l1: 0.291969, l2: 0.280175, l3: 0.297140, l4: 0.305304, l5: 0.335363, l6: 0.363363\n",
            "\n",
            "[epoch:  26/100000, batch:   396/  840, ite: 1783] train loss: 2.008092, tar: 0.264830 \n",
            "l0: 0.141458, l1: 0.151838, l2: 0.150653, l3: 0.159199, l4: 0.163527, l5: 0.169996, l6: 0.196081\n",
            "\n",
            "[epoch:  26/100000, batch:   408/  840, ite: 1784] train loss: 2.007601, tar: 0.264761 \n",
            "l0: 0.296714, l1: 0.300417, l2: 0.284855, l3: 0.324749, l4: 0.327425, l5: 0.342910, l6: 0.394998\n",
            "\n",
            "[epoch:  26/100000, batch:   420/  840, ite: 1785] train loss: 2.007749, tar: 0.264779 \n",
            "l0: 0.198218, l1: 0.202650, l2: 0.206903, l3: 0.224966, l4: 0.217651, l5: 0.229192, l6: 0.272441\n",
            "\n",
            "[epoch:  26/100000, batch:   432/  840, ite: 1786] train loss: 2.007494, tar: 0.264742 \n",
            "l0: 0.176306, l1: 0.183511, l2: 0.187186, l3: 0.187661, l4: 0.184986, l5: 0.207278, l6: 0.266478\n",
            "\n",
            "[epoch:  26/100000, batch:   444/  840, ite: 1787] train loss: 2.007150, tar: 0.264692 \n",
            "l0: 0.136175, l1: 0.150320, l2: 0.146757, l3: 0.138585, l4: 0.141421, l5: 0.156767, l6: 0.177433\n",
            "\n",
            "[epoch:  26/100000, batch:   456/  840, ite: 1788] train loss: 2.006614, tar: 0.264620 \n",
            "l0: 0.155355, l1: 0.155665, l2: 0.159085, l3: 0.171067, l4: 0.176929, l5: 0.191436, l6: 0.227968\n",
            "\n",
            "[epoch:  26/100000, batch:   468/  840, ite: 1789] train loss: 2.006184, tar: 0.264559 \n",
            "l0: 0.103412, l1: 0.106380, l2: 0.105734, l3: 0.109503, l4: 0.112074, l5: 0.128993, l6: 0.169838\n",
            "\n",
            "[epoch:  26/100000, batch:   480/  840, ite: 1790] train loss: 2.005530, tar: 0.264469 \n",
            "l0: 0.184464, l1: 0.184671, l2: 0.191270, l3: 0.176792, l4: 0.177689, l5: 0.183559, l6: 0.251815\n",
            "\n",
            "[epoch:  26/100000, batch:   492/  840, ite: 1791] train loss: 2.005164, tar: 0.264425 \n",
            "l0: 0.196069, l1: 0.198132, l2: 0.194300, l3: 0.196685, l4: 0.202778, l5: 0.219173, l6: 0.258991\n",
            "\n",
            "[epoch:  26/100000, batch:   504/  840, ite: 1792] train loss: 2.004864, tar: 0.264387 \n",
            "l0: 0.149086, l1: 0.147470, l2: 0.155208, l3: 0.166272, l4: 0.168846, l5: 0.178636, l6: 0.218571\n",
            "\n",
            "[epoch:  26/100000, batch:   516/  840, ite: 1793] train loss: 2.004406, tar: 0.264322 \n",
            "l0: 0.207411, l1: 0.204757, l2: 0.224676, l3: 0.220499, l4: 0.218821, l5: 0.223952, l6: 0.262345\n",
            "\n",
            "[epoch:  26/100000, batch:   528/  840, ite: 1794] train loss: 2.004159, tar: 0.264291 \n",
            "l0: 0.210302, l1: 0.209823, l2: 0.218621, l3: 0.237112, l4: 0.232691, l5: 0.256865, l6: 0.304778\n",
            "\n",
            "[epoch:  26/100000, batch:   540/  840, ite: 1795] train loss: 2.003973, tar: 0.264260 \n",
            "l0: 0.194599, l1: 0.203389, l2: 0.207857, l3: 0.211173, l4: 0.200252, l5: 0.201576, l6: 0.217551\n",
            "\n",
            "[epoch:  26/100000, batch:   552/  840, ite: 1796] train loss: 2.003657, tar: 0.264222 \n",
            "l0: 0.225407, l1: 0.227680, l2: 0.227494, l3: 0.235107, l4: 0.242143, l5: 0.254245, l6: 0.290013\n",
            "\n",
            "[epoch:  26/100000, batch:   564/  840, ite: 1797] train loss: 2.003489, tar: 0.264200 \n",
            "l0: 0.217098, l1: 0.211550, l2: 0.220812, l3: 0.233691, l4: 0.242766, l5: 0.264742, l6: 0.308179\n",
            "\n",
            "[epoch:  26/100000, batch:   576/  840, ite: 1798] train loss: 2.003320, tar: 0.264174 \n",
            "l0: 0.256871, l1: 0.243552, l2: 0.270016, l3: 0.254487, l4: 0.269131, l5: 0.273569, l6: 0.300529\n",
            "\n",
            "[epoch:  26/100000, batch:   588/  840, ite: 1799] train loss: 2.003245, tar: 0.264170 \n",
            "l0: 0.159196, l1: 0.156342, l2: 0.169274, l3: 0.168888, l4: 0.172800, l5: 0.183898, l6: 0.229605\n",
            "\n",
            "[epoch:  26/100000, batch:   600/  840, ite: 1800] train loss: 2.002821, tar: 0.264111 \n",
            "l0: 0.271202, l1: 0.259189, l2: 0.273092, l3: 0.306901, l4: 0.315838, l5: 0.345645, l6: 0.370374\n",
            "\n",
            "[epoch:  26/100000, batch:   612/  840, ite: 1801] train loss: 2.002898, tar: 0.264115 \n",
            "l0: 0.204256, l1: 0.206858, l2: 0.214048, l3: 0.230575, l4: 0.231671, l5: 0.237080, l6: 0.271031\n",
            "\n",
            "[epoch:  26/100000, batch:   624/  840, ite: 1802] train loss: 2.002672, tar: 0.264082 \n",
            "l0: 0.172692, l1: 0.174463, l2: 0.176817, l3: 0.208714, l4: 0.209965, l5: 0.209190, l6: 0.233569\n",
            "\n",
            "[epoch:  26/100000, batch:   636/  840, ite: 1803] train loss: 2.002330, tar: 0.264032 \n",
            "l0: 0.294484, l1: 0.306030, l2: 0.314640, l3: 0.307114, l4: 0.306118, l5: 0.306981, l6: 0.289018\n",
            "\n",
            "[epoch:  26/100000, batch:   648/  840, ite: 1804] train loss: 2.002397, tar: 0.264048 \n",
            "l0: 0.208471, l1: 0.200665, l2: 0.219002, l3: 0.244540, l4: 0.252830, l5: 0.274471, l6: 0.287026\n",
            "\n",
            "[epoch:  26/100000, batch:   660/  840, ite: 1805] train loss: 2.002223, tar: 0.264018 \n",
            "l0: 0.170394, l1: 0.165615, l2: 0.177498, l3: 0.191452, l4: 0.194362, l5: 0.205651, l6: 0.271433\n",
            "\n",
            "[epoch:  26/100000, batch:   672/  840, ite: 1806] train loss: 2.001876, tar: 0.263966 \n",
            "l0: 0.100272, l1: 0.102117, l2: 0.103934, l3: 0.119961, l4: 0.118737, l5: 0.131014, l6: 0.173497\n",
            "\n",
            "[epoch:  26/100000, batch:   684/  840, ite: 1807] train loss: 2.001239, tar: 0.263875 \n",
            "l0: 0.153516, l1: 0.148214, l2: 0.158749, l3: 0.172915, l4: 0.182395, l5: 0.191327, l6: 0.244591\n",
            "\n",
            "[epoch:  26/100000, batch:   696/  840, ite: 1808] train loss: 2.000824, tar: 0.263814 \n",
            "l0: 0.262598, l1: 0.261727, l2: 0.275092, l3: 0.290472, l4: 0.292963, l5: 0.302719, l6: 0.331005\n",
            "\n",
            "[epoch:  26/100000, batch:   708/  840, ite: 1809] train loss: 2.000833, tar: 0.263813 \n",
            "l0: 0.191858, l1: 0.199593, l2: 0.193775, l3: 0.209709, l4: 0.211467, l5: 0.225486, l6: 0.249493\n",
            "\n",
            "[epoch:  26/100000, batch:   720/  840, ite: 1810] train loss: 2.000546, tar: 0.263774 \n",
            "l0: 0.195343, l1: 0.203518, l2: 0.200977, l3: 0.216617, l4: 0.217778, l5: 0.236209, l6: 0.266269\n",
            "\n",
            "[epoch:  26/100000, batch:   732/  840, ite: 1811] train loss: 2.000289, tar: 0.263736 \n",
            "l0: 0.214981, l1: 0.206726, l2: 0.232480, l3: 0.221110, l4: 0.219938, l5: 0.227554, l6: 0.280822\n",
            "\n",
            "[epoch:  26/100000, batch:   744/  840, ite: 1812] train loss: 2.000071, tar: 0.263709 \n",
            "l0: 0.071868, l1: 0.072581, l2: 0.075086, l3: 0.076460, l4: 0.081592, l5: 0.092917, l6: 0.143416\n",
            "\n",
            "[epoch:  26/100000, batch:   756/  840, ite: 1813] train loss: 1.999306, tar: 0.263603 \n",
            "l0: 0.223525, l1: 0.228272, l2: 0.236066, l3: 0.248945, l4: 0.240340, l5: 0.240149, l6: 0.261481\n",
            "\n",
            "[epoch:  26/100000, batch:   768/  840, ite: 1814] train loss: 1.999129, tar: 0.263581 \n",
            "l0: 0.178460, l1: 0.187656, l2: 0.188417, l3: 0.196084, l4: 0.200550, l5: 0.202522, l6: 0.211645\n",
            "\n",
            "[epoch:  26/100000, batch:   780/  840, ite: 1815] train loss: 1.998780, tar: 0.263534 \n",
            "l0: 0.096200, l1: 0.103473, l2: 0.101456, l3: 0.097617, l4: 0.102777, l5: 0.115264, l6: 0.145715\n",
            "\n",
            "[epoch:  26/100000, batch:   792/  840, ite: 1816] train loss: 1.998099, tar: 0.263442 \n",
            "l0: 0.159665, l1: 0.164261, l2: 0.155837, l3: 0.172436, l4: 0.188256, l5: 0.202366, l6: 0.233553\n",
            "\n",
            "[epoch:  26/100000, batch:   804/  840, ite: 1817] train loss: 1.997702, tar: 0.263385 \n",
            "l0: 0.148387, l1: 0.140559, l2: 0.147111, l3: 0.184530, l4: 0.193035, l5: 0.204101, l6: 0.244441\n",
            "\n",
            "[epoch:  26/100000, batch:   816/  840, ite: 1818] train loss: 1.997297, tar: 0.263322 \n",
            "l0: 0.109882, l1: 0.121640, l2: 0.110124, l3: 0.112121, l4: 0.119584, l5: 0.140376, l6: 0.196344\n",
            "\n",
            "[epoch:  26/100000, batch:   828/  840, ite: 1819] train loss: 1.996700, tar: 0.263237 \n",
            "l0: 0.263318, l1: 0.284948, l2: 0.265959, l3: 0.248545, l4: 0.238463, l5: 0.246700, l6: 0.294957\n",
            "\n",
            "[epoch:  26/100000, batch:   840/  840, ite: 1820] train loss: 1.996615, tar: 0.263237 \n",
            "l0: 0.253705, l1: 0.263150, l2: 0.262633, l3: 0.277435, l4: 0.276245, l5: 0.270449, l6: 0.329608\n",
            "\n",
            "[epoch:  27/100000, batch:    12/  840, ite: 1821] train loss: 1.996580, tar: 0.263232 \n",
            "l0: 0.228257, l1: 0.236450, l2: 0.236418, l3: 0.239480, l4: 0.242879, l5: 0.251611, l6: 0.289299\n",
            "\n",
            "[epoch:  27/100000, batch:    24/  840, ite: 1822] train loss: 1.996431, tar: 0.263213 \n",
            "l0: 0.163433, l1: 0.179042, l2: 0.162944, l3: 0.175803, l4: 0.180643, l5: 0.187472, l6: 0.227343\n",
            "\n",
            "[epoch:  27/100000, batch:    36/  840, ite: 1823] train loss: 1.996036, tar: 0.263158 \n",
            "l0: 0.169553, l1: 0.168828, l2: 0.174993, l3: 0.191601, l4: 0.198081, l5: 0.206624, l6: 0.253267\n",
            "\n",
            "[epoch:  27/100000, batch:    48/  840, ite: 1824] train loss: 1.995689, tar: 0.263107 \n",
            "l0: 0.229658, l1: 0.218660, l2: 0.232889, l3: 0.242494, l4: 0.249146, l5: 0.281529, l6: 0.354017\n",
            "\n",
            "[epoch:  27/100000, batch:    60/  840, ite: 1825] train loss: 1.995586, tar: 0.263089 \n",
            "l0: 0.285245, l1: 0.296194, l2: 0.310659, l3: 0.293376, l4: 0.282594, l5: 0.296668, l6: 0.328846\n",
            "\n",
            "[epoch:  27/100000, batch:    72/  840, ite: 1826] train loss: 1.995640, tar: 0.263101 \n",
            "l0: 0.219897, l1: 0.212626, l2: 0.235630, l3: 0.235797, l4: 0.237371, l5: 0.252428, l6: 0.298655\n",
            "\n",
            "[epoch:  27/100000, batch:    84/  840, ite: 1827] train loss: 1.995474, tar: 0.263077 \n",
            "l0: 0.342056, l1: 0.364128, l2: 0.364457, l3: 0.372368, l4: 0.356447, l5: 0.350136, l6: 0.354812\n",
            "\n",
            "[epoch:  27/100000, batch:    96/  840, ite: 1828] train loss: 1.995752, tar: 0.263120 \n",
            "l0: 0.127372, l1: 0.125071, l2: 0.136133, l3: 0.144649, l4: 0.146258, l5: 0.151642, l6: 0.195014\n",
            "\n",
            "[epoch:  27/100000, batch:   108/  840, ite: 1829] train loss: 1.995222, tar: 0.263046 \n",
            "l0: 0.132231, l1: 0.144305, l2: 0.139712, l3: 0.138245, l4: 0.135302, l5: 0.147279, l6: 0.211167\n",
            "\n",
            "[epoch:  27/100000, batch:   120/  840, ite: 1830] train loss: 1.994705, tar: 0.262975 \n",
            "l0: 0.137966, l1: 0.136201, l2: 0.135067, l3: 0.138472, l4: 0.149643, l5: 0.207201, l6: 0.300249\n",
            "\n",
            "[epoch:  27/100000, batch:   132/  840, ite: 1831] train loss: 1.994273, tar: 0.262906 \n",
            "l0: 0.152863, l1: 0.157428, l2: 0.143298, l3: 0.137664, l4: 0.151032, l5: 0.200823, l6: 0.311361\n",
            "\n",
            "[epoch:  27/100000, batch:   144/  840, ite: 1832] train loss: 1.993870, tar: 0.262846 \n",
            "l0: 0.198891, l1: 0.203918, l2: 0.217899, l3: 0.210541, l4: 0.213319, l5: 0.246997, l6: 0.294173\n",
            "\n",
            "[epoch:  27/100000, batch:   156/  840, ite: 1833] train loss: 1.993647, tar: 0.262811 \n",
            "l0: 0.208110, l1: 0.206697, l2: 0.214003, l3: 0.215474, l4: 0.225486, l5: 0.257086, l6: 0.290239\n",
            "\n",
            "[epoch:  27/100000, batch:   168/  840, ite: 1834] train loss: 1.993442, tar: 0.262781 \n",
            "l0: 0.106606, l1: 0.115806, l2: 0.110594, l3: 0.099824, l4: 0.108649, l5: 0.128458, l6: 0.176409\n",
            "\n",
            "[epoch:  27/100000, batch:   180/  840, ite: 1835] train loss: 1.992817, tar: 0.262696 \n",
            "l0: 0.161528, l1: 0.171291, l2: 0.159784, l3: 0.169585, l4: 0.189027, l5: 0.189734, l6: 0.240236\n",
            "\n",
            "[epoch:  27/100000, batch:   192/  840, ite: 1836] train loss: 1.992429, tar: 0.262641 \n",
            "l0: 0.254329, l1: 0.240469, l2: 0.277744, l3: 0.285216, l4: 0.299099, l5: 0.274752, l6: 0.295254\n",
            "\n",
            "[epoch:  27/100000, batch:   204/  840, ite: 1837] train loss: 1.992393, tar: 0.262637 \n",
            "l0: 0.272906, l1: 0.264779, l2: 0.272463, l3: 0.282970, l4: 0.289249, l5: 0.346952, l6: 0.444862\n",
            "\n",
            "[epoch:  27/100000, batch:   216/  840, ite: 1838] train loss: 1.992492, tar: 0.262642 \n",
            "l0: 0.203200, l1: 0.202824, l2: 0.195447, l3: 0.215311, l4: 0.223722, l5: 0.308776, l6: 0.441679\n",
            "\n",
            "[epoch:  27/100000, batch:   228/  840, ite: 1839] train loss: 1.992383, tar: 0.262610 \n",
            "l0: 0.180990, l1: 0.177410, l2: 0.186263, l3: 0.192663, l4: 0.196951, l5: 0.249707, l6: 0.338338\n",
            "\n",
            "[epoch:  27/100000, batch:   240/  840, ite: 1840] train loss: 1.992127, tar: 0.262566 \n",
            "l0: 0.147750, l1: 0.143114, l2: 0.154018, l3: 0.167075, l4: 0.173890, l5: 0.228102, l6: 0.266631\n",
            "\n",
            "[epoch:  27/100000, batch:   252/  840, ite: 1841] train loss: 1.991740, tar: 0.262503 \n",
            "l0: 0.145319, l1: 0.152491, l2: 0.145054, l3: 0.152399, l4: 0.154867, l5: 0.170687, l6: 0.206730\n",
            "\n",
            "[epoch:  27/100000, batch:   264/  840, ite: 1842] train loss: 1.991271, tar: 0.262440 \n",
            "l0: 0.091241, l1: 0.096392, l2: 0.092915, l3: 0.095271, l4: 0.105440, l5: 0.116981, l6: 0.133439\n",
            "\n",
            "[epoch:  27/100000, batch:   276/  840, ite: 1843] train loss: 1.990588, tar: 0.262347 \n",
            "l0: 0.224711, l1: 0.216772, l2: 0.230909, l3: 0.247916, l4: 0.256214, l5: 0.272491, l6: 0.307772\n",
            "\n",
            "[epoch:  27/100000, batch:   288/  840, ite: 1844] train loss: 1.990461, tar: 0.262326 \n",
            "l0: 0.222329, l1: 0.212745, l2: 0.221042, l3: 0.237428, l4: 0.248689, l5: 0.286795, l6: 0.411161\n",
            "\n",
            "[epoch:  27/100000, batch:   300/  840, ite: 1845] train loss: 1.990379, tar: 0.262305 \n",
            "l0: 0.180517, l1: 0.186689, l2: 0.184564, l3: 0.194349, l4: 0.193178, l5: 0.198123, l6: 0.270632\n",
            "\n",
            "[epoch:  27/100000, batch:   312/  840, ite: 1846] train loss: 1.990064, tar: 0.262260 \n",
            "l0: 0.157897, l1: 0.158100, l2: 0.158107, l3: 0.167636, l4: 0.175711, l5: 0.201942, l6: 0.260781\n",
            "\n",
            "[epoch:  27/100000, batch:   324/  840, ite: 1847] train loss: 1.989680, tar: 0.262204 \n",
            "l0: 0.135564, l1: 0.139572, l2: 0.144438, l3: 0.149948, l4: 0.156518, l5: 0.185016, l6: 0.239207\n",
            "\n",
            "[epoch:  27/100000, batch:   336/  840, ite: 1848] train loss: 1.989225, tar: 0.262135 \n",
            "l0: 0.211789, l1: 0.199853, l2: 0.223549, l3: 0.229220, l4: 0.236692, l5: 0.261848, l6: 0.303629\n",
            "\n",
            "[epoch:  27/100000, batch:   348/  840, ite: 1849] train loss: 1.989051, tar: 0.262108 \n",
            "l0: 0.204482, l1: 0.213672, l2: 0.199384, l3: 0.216788, l4: 0.222952, l5: 0.236727, l6: 0.288711\n",
            "\n",
            "[epoch:  27/100000, batch:   360/  840, ite: 1850] train loss: 1.988831, tar: 0.262077 \n",
            "l0: 0.217447, l1: 0.218207, l2: 0.223274, l3: 0.228577, l4: 0.227268, l5: 0.238995, l6: 0.274183\n",
            "\n",
            "[epoch:  27/100000, batch:   372/  840, ite: 1851] train loss: 1.988636, tar: 0.262053 \n",
            "l0: 0.208803, l1: 0.207931, l2: 0.212399, l3: 0.237990, l4: 0.238900, l5: 0.240671, l6: 0.309045\n",
            "\n",
            "[epoch:  27/100000, batch:   384/  840, ite: 1852] train loss: 1.988456, tar: 0.262024 \n",
            "l0: 0.169694, l1: 0.171380, l2: 0.179608, l3: 0.189464, l4: 0.191403, l5: 0.199406, l6: 0.246023\n",
            "\n",
            "[epoch:  27/100000, batch:   396/  840, ite: 1853] train loss: 1.988110, tar: 0.261974 \n",
            "l0: 0.136218, l1: 0.134712, l2: 0.147916, l3: 0.148999, l4: 0.147880, l5: 0.161228, l6: 0.227268\n",
            "\n",
            "[epoch:  27/100000, batch:   408/  840, ite: 1854] train loss: 1.987634, tar: 0.261906 \n",
            "l0: 0.133767, l1: 0.143796, l2: 0.142479, l3: 0.134056, l4: 0.141583, l5: 0.155869, l6: 0.207314\n",
            "\n",
            "[epoch:  27/100000, batch:   420/  840, ite: 1855] train loss: 1.987133, tar: 0.261837 \n",
            "l0: 0.123412, l1: 0.129867, l2: 0.125479, l3: 0.124977, l4: 0.133758, l5: 0.157257, l6: 0.195972\n",
            "\n",
            "[epoch:  27/100000, batch:   432/  840, ite: 1856] train loss: 1.986596, tar: 0.261763 \n",
            "l0: 0.200541, l1: 0.212541, l2: 0.201675, l3: 0.215853, l4: 0.218599, l5: 0.246400, l6: 0.275396\n",
            "\n",
            "[epoch:  27/100000, batch:   444/  840, ite: 1857] train loss: 1.986372, tar: 0.261730 \n",
            "l0: 0.164087, l1: 0.161473, l2: 0.167750, l3: 0.186757, l4: 0.194720, l5: 0.223227, l6: 0.246228\n",
            "\n",
            "[epoch:  27/100000, batch:   456/  840, ite: 1858] train loss: 1.986027, tar: 0.261677 \n",
            "l0: 0.238488, l1: 0.242417, l2: 0.245991, l3: 0.250900, l4: 0.257969, l5: 0.267496, l6: 0.279694\n",
            "\n",
            "[epoch:  27/100000, batch:   468/  840, ite: 1859] train loss: 1.985917, tar: 0.261665 \n",
            "l0: 0.217916, l1: 0.215129, l2: 0.218130, l3: 0.238474, l4: 0.252701, l5: 0.275557, l6: 0.332849\n",
            "\n",
            "[epoch:  27/100000, batch:   480/  840, ite: 1860] train loss: 1.985791, tar: 0.261641 \n",
            "l0: 0.137802, l1: 0.139411, l2: 0.146155, l3: 0.147189, l4: 0.148756, l5: 0.172214, l6: 0.232608\n",
            "\n",
            "[epoch:  27/100000, batch:   492/  840, ite: 1861] train loss: 1.985328, tar: 0.261575 \n",
            "l0: 0.176563, l1: 0.180900, l2: 0.178959, l3: 0.202241, l4: 0.202827, l5: 0.207775, l6: 0.255505\n",
            "\n",
            "[epoch:  27/100000, batch:   504/  840, ite: 1862] train loss: 1.985016, tar: 0.261529 \n",
            "l0: 0.134262, l1: 0.129307, l2: 0.134262, l3: 0.149117, l4: 0.143119, l5: 0.185566, l6: 0.253160\n",
            "\n",
            "[epoch:  27/100000, batch:   516/  840, ite: 1863] train loss: 1.984557, tar: 0.261461 \n",
            "l0: 0.192571, l1: 0.189625, l2: 0.188663, l3: 0.216470, l4: 0.220797, l5: 0.259084, l6: 0.310796\n",
            "\n",
            "[epoch:  27/100000, batch:   528/  840, ite: 1864] train loss: 1.984339, tar: 0.261424 \n",
            "l0: 0.205621, l1: 0.205488, l2: 0.206236, l3: 0.230906, l4: 0.232806, l5: 0.248608, l6: 0.279542\n",
            "\n",
            "[epoch:  27/100000, batch:   540/  840, ite: 1865] train loss: 1.984137, tar: 0.261394 \n",
            "l0: 0.137421, l1: 0.142177, l2: 0.139910, l3: 0.181275, l4: 0.171365, l5: 0.187093, l6: 0.225205\n",
            "\n",
            "[epoch:  27/100000, batch:   552/  840, ite: 1866] train loss: 1.983709, tar: 0.261327 \n",
            "l0: 0.081280, l1: 0.088407, l2: 0.079033, l3: 0.078148, l4: 0.083753, l5: 0.104864, l6: 0.176130\n",
            "\n",
            "[epoch:  27/100000, batch:   564/  840, ite: 1867] train loss: 1.983017, tar: 0.261231 \n",
            "l0: 0.185310, l1: 0.181345, l2: 0.182573, l3: 0.198470, l4: 0.201985, l5: 0.224896, l6: 0.275581\n",
            "\n",
            "[epoch:  27/100000, batch:   576/  840, ite: 1868] train loss: 1.982731, tar: 0.261190 \n",
            "l0: 0.218716, l1: 0.216347, l2: 0.223863, l3: 0.244269, l4: 0.250893, l5: 0.267879, l6: 0.305982\n",
            "\n",
            "[epoch:  27/100000, batch:   588/  840, ite: 1869] train loss: 1.982595, tar: 0.261168 \n",
            "l0: 0.162018, l1: 0.158212, l2: 0.163659, l3: 0.175244, l4: 0.185551, l5: 0.209706, l6: 0.273425\n",
            "\n",
            "[epoch:  27/100000, batch:   600/  840, ite: 1870] train loss: 1.982245, tar: 0.261115 \n",
            "l0: 0.367252, l1: 0.368942, l2: 0.372152, l3: 0.399136, l4: 0.408564, l5: 0.429360, l6: 0.432018\n",
            "\n",
            "[epoch:  27/100000, batch:   612/  840, ite: 1871] train loss: 1.982670, tar: 0.261171 \n",
            "l0: 0.174727, l1: 0.170101, l2: 0.182994, l3: 0.185257, l4: 0.186635, l5: 0.207840, l6: 0.254298\n",
            "\n",
            "[epoch:  27/100000, batch:   624/  840, ite: 1872] train loss: 1.982339, tar: 0.261125 \n",
            "l0: 0.241377, l1: 0.240195, l2: 0.249693, l3: 0.254432, l4: 0.263875, l5: 0.268394, l6: 0.286764\n",
            "\n",
            "[epoch:  27/100000, batch:   636/  840, ite: 1873] train loss: 1.982244, tar: 0.261115 \n",
            "l0: 0.122666, l1: 0.116296, l2: 0.125246, l3: 0.153521, l4: 0.157899, l5: 0.172300, l6: 0.218161\n",
            "\n",
            "[epoch:  27/100000, batch:   648/  840, ite: 1874] train loss: 1.981755, tar: 0.261041 \n",
            "l0: 0.158000, l1: 0.167581, l2: 0.173440, l3: 0.171367, l4: 0.172305, l5: 0.169322, l6: 0.198989\n",
            "\n",
            "[epoch:  27/100000, batch:   660/  840, ite: 1875] train loss: 1.981344, tar: 0.260986 \n",
            "l0: 0.192773, l1: 0.191571, l2: 0.231942, l3: 0.203325, l4: 0.197571, l5: 0.189924, l6: 0.232320\n",
            "\n",
            "[epoch:  27/100000, batch:   672/  840, ite: 1876] train loss: 1.981055, tar: 0.260949 \n",
            "l0: 0.176612, l1: 0.183204, l2: 0.189321, l3: 0.203602, l4: 0.197385, l5: 0.208531, l6: 0.260188\n",
            "\n",
            "[epoch:  27/100000, batch:   684/  840, ite: 1877] train loss: 1.980755, tar: 0.260904 \n",
            "l0: 0.176236, l1: 0.172139, l2: 0.207393, l3: 0.207677, l4: 0.201572, l5: 0.192414, l6: 0.219900\n",
            "\n",
            "[epoch:  27/100000, batch:   696/  840, ite: 1878] train loss: 1.980434, tar: 0.260859 \n",
            "l0: 0.137941, l1: 0.139459, l2: 0.143892, l3: 0.152710, l4: 0.153095, l5: 0.179284, l6: 0.240791\n",
            "\n",
            "[epoch:  27/100000, batch:   708/  840, ite: 1879] train loss: 1.979991, tar: 0.260794 \n",
            "l0: 0.216449, l1: 0.226277, l2: 0.229320, l3: 0.218857, l4: 0.223544, l5: 0.224050, l6: 0.256159\n",
            "\n",
            "[epoch:  27/100000, batch:   720/  840, ite: 1880] train loss: 1.979786, tar: 0.260770 \n",
            "l0: 0.268576, l1: 0.280780, l2: 0.265155, l3: 0.296793, l4: 0.297281, l5: 0.300915, l6: 0.347219\n",
            "\n",
            "[epoch:  27/100000, batch:   732/  840, ite: 1881] train loss: 1.979826, tar: 0.260775 \n",
            "l0: 0.147298, l1: 0.145022, l2: 0.153682, l3: 0.160898, l4: 0.167208, l5: 0.196617, l6: 0.218126\n",
            "\n",
            "[epoch:  27/100000, batch:   744/  840, ite: 1882] train loss: 1.979406, tar: 0.260714 \n",
            "l0: 0.358768, l1: 0.343019, l2: 0.334350, l3: 0.377525, l4: 0.430159, l5: 0.458813, l6: 0.482497\n",
            "\n",
            "[epoch:  27/100000, batch:   756/  840, ite: 1883] train loss: 1.979834, tar: 0.260766 \n",
            "l0: 0.230699, l1: 0.254921, l2: 0.231319, l3: 0.235962, l4: 0.239805, l5: 0.257459, l6: 0.313807\n",
            "\n",
            "[epoch:  27/100000, batch:   768/  840, ite: 1884] train loss: 1.979720, tar: 0.260750 \n",
            "l0: 0.237122, l1: 0.253541, l2: 0.241293, l3: 0.217631, l4: 0.207048, l5: 0.236128, l6: 0.329680\n",
            "\n",
            "[epoch:  27/100000, batch:   780/  840, ite: 1885] train loss: 1.979583, tar: 0.260738 \n",
            "l0: 0.157632, l1: 0.153660, l2: 0.160870, l3: 0.182631, l4: 0.188391, l5: 0.222659, l6: 0.293926\n",
            "\n",
            "[epoch:  27/100000, batch:   792/  840, ite: 1886] train loss: 1.979255, tar: 0.260683 \n",
            "l0: 0.227267, l1: 0.237290, l2: 0.247340, l3: 0.230665, l4: 0.220360, l5: 0.222115, l6: 0.263482\n",
            "\n",
            "[epoch:  27/100000, batch:   804/  840, ite: 1887] train loss: 1.979079, tar: 0.260665 \n",
            "l0: 0.144513, l1: 0.151817, l2: 0.150536, l3: 0.145866, l4: 0.155536, l5: 0.175941, l6: 0.218098\n",
            "\n",
            "[epoch:  27/100000, batch:   816/  840, ite: 1888] train loss: 1.978636, tar: 0.260604 \n",
            "l0: 0.213038, l1: 0.217882, l2: 0.223652, l3: 0.249127, l4: 0.236085, l5: 0.249852, l6: 0.272769\n",
            "\n",
            "[epoch:  27/100000, batch:   828/  840, ite: 1889] train loss: 1.978469, tar: 0.260579 \n",
            "l0: 0.140285, l1: 0.133473, l2: 0.148289, l3: 0.165245, l4: 0.172269, l5: 0.191313, l6: 0.236851\n",
            "\n",
            "[epoch:  27/100000, batch:   840/  840, ite: 1890] train loss: 1.978050, tar: 0.260515 \n",
            "l0: 0.274553, l1: 0.269929, l2: 0.287269, l3: 0.297344, l4: 0.309649, l5: 0.336950, l6: 0.367276\n",
            "\n",
            "[epoch:  28/100000, batch:    12/  840, ite: 1891] train loss: 1.978137, tar: 0.260523 \n",
            "l0: 0.140541, l1: 0.133828, l2: 0.152553, l3: 0.159752, l4: 0.161419, l5: 0.170252, l6: 0.210043\n",
            "\n",
            "[epoch:  28/100000, batch:    24/  840, ite: 1892] train loss: 1.977688, tar: 0.260459 \n",
            "l0: 0.078788, l1: 0.076138, l2: 0.082495, l3: 0.096392, l4: 0.101081, l5: 0.117593, l6: 0.170391\n",
            "\n",
            "[epoch:  28/100000, batch:    36/  840, ite: 1893] train loss: 1.977025, tar: 0.260363 \n",
            "l0: 0.190648, l1: 0.198163, l2: 0.187399, l3: 0.200151, l4: 0.207127, l5: 0.238237, l6: 0.289035\n",
            "\n",
            "[epoch:  28/100000, batch:    48/  840, ite: 1894] train loss: 1.976779, tar: 0.260326 \n",
            "l0: 0.183585, l1: 0.187087, l2: 0.191508, l3: 0.220747, l4: 0.216082, l5: 0.233196, l6: 0.279481\n",
            "\n",
            "[epoch:  28/100000, batch:    60/  840, ite: 1895] train loss: 1.976534, tar: 0.260286 \n",
            "l0: 0.185091, l1: 0.174121, l2: 0.193375, l3: 0.204396, l4: 0.218112, l5: 0.237186, l6: 0.306432\n",
            "\n",
            "[epoch:  28/100000, batch:    72/  840, ite: 1896] train loss: 1.976292, tar: 0.260246 \n",
            "l0: 0.249123, l1: 0.254910, l2: 0.272410, l3: 0.249861, l4: 0.258030, l5: 0.254263, l6: 0.278034\n",
            "\n",
            "[epoch:  28/100000, batch:    84/  840, ite: 1897] train loss: 1.976208, tar: 0.260240 \n",
            "l0: 0.164438, l1: 0.177219, l2: 0.171209, l3: 0.178409, l4: 0.174502, l5: 0.185487, l6: 0.205944\n",
            "\n",
            "[epoch:  28/100000, batch:    96/  840, ite: 1898] train loss: 1.975829, tar: 0.260190 \n",
            "l0: 0.224785, l1: 0.220612, l2: 0.231417, l3: 0.229195, l4: 0.237759, l5: 0.251417, l6: 0.298074\n",
            "\n",
            "[epoch:  28/100000, batch:   108/  840, ite: 1899] train loss: 1.975681, tar: 0.260171 \n",
            "l0: 0.144031, l1: 0.147350, l2: 0.152505, l3: 0.166937, l4: 0.162252, l5: 0.163196, l6: 0.195932\n",
            "\n",
            "[epoch:  28/100000, batch:   120/  840, ite: 1900] train loss: 1.975237, tar: 0.260110 \n",
            "l0: 0.355875, l1: 0.371659, l2: 0.361531, l3: 0.357818, l4: 0.361955, l5: 0.379099, l6: 0.410464\n",
            "\n",
            "[epoch:  28/100000, batch:   132/  840, ite: 1901] train loss: 1.975565, tar: 0.260160 \n",
            "l0: 0.138081, l1: 0.132617, l2: 0.144217, l3: 0.134638, l4: 0.141529, l5: 0.158286, l6: 0.222222\n",
            "\n",
            "[epoch:  28/100000, batch:   144/  840, ite: 1902] train loss: 1.975089, tar: 0.260096 \n",
            "l0: 0.123793, l1: 0.121536, l2: 0.128126, l3: 0.133689, l4: 0.139201, l5: 0.151978, l6: 0.183856\n",
            "\n",
            "[epoch:  28/100000, batch:   156/  840, ite: 1903] train loss: 1.974568, tar: 0.260025 \n",
            "l0: 0.242070, l1: 0.241445, l2: 0.271980, l3: 0.280942, l4: 0.266257, l5: 0.269440, l6: 0.291650\n",
            "\n",
            "[epoch:  28/100000, batch:   168/  840, ite: 1904] train loss: 1.974509, tar: 0.260015 \n",
            "l0: 0.223526, l1: 0.226384, l2: 0.231858, l3: 0.224710, l4: 0.232683, l5: 0.232751, l6: 0.261534\n",
            "\n",
            "[epoch:  28/100000, batch:   180/  840, ite: 1905] train loss: 1.974330, tar: 0.259996 \n",
            "l0: 0.126983, l1: 0.137444, l2: 0.121575, l3: 0.141307, l4: 0.142876, l5: 0.156387, l6: 0.198419\n",
            "\n",
            "[epoch:  28/100000, batch:   192/  840, ite: 1906] train loss: 1.973832, tar: 0.259926 \n",
            "l0: 0.186193, l1: 0.193537, l2: 0.183916, l3: 0.189260, l4: 0.198235, l5: 0.209935, l6: 0.256226\n",
            "\n",
            "[epoch:  28/100000, batch:   204/  840, ite: 1907] train loss: 1.973540, tar: 0.259888 \n",
            "l0: 0.196552, l1: 0.194351, l2: 0.216483, l3: 0.222553, l4: 0.217440, l5: 0.212450, l6: 0.233209\n",
            "\n",
            "[epoch:  28/100000, batch:   216/  840, ite: 1908] train loss: 1.973288, tar: 0.259854 \n",
            "l0: 0.211319, l1: 0.219807, l2: 0.220403, l3: 0.213084, l4: 0.217228, l5: 0.223056, l6: 0.251397\n",
            "\n",
            "[epoch:  28/100000, batch:   228/  840, ite: 1909] train loss: 1.973070, tar: 0.259829 \n",
            "l0: 0.146435, l1: 0.151433, l2: 0.146867, l3: 0.169685, l4: 0.174469, l5: 0.184476, l6: 0.215667\n",
            "\n",
            "[epoch:  28/100000, batch:   240/  840, ite: 1910] train loss: 1.972660, tar: 0.259770 \n",
            "l0: 0.106983, l1: 0.120372, l2: 0.109170, l3: 0.105030, l4: 0.106205, l5: 0.119538, l6: 0.156578\n",
            "\n",
            "[epoch:  28/100000, batch:   252/  840, ite: 1911] train loss: 1.972058, tar: 0.259690 \n",
            "l0: 0.164727, l1: 0.168514, l2: 0.166483, l3: 0.177183, l4: 0.178755, l5: 0.185327, l6: 0.218351\n",
            "\n",
            "[epoch:  28/100000, batch:   264/  840, ite: 1912] train loss: 1.971686, tar: 0.259640 \n",
            "l0: 0.222846, l1: 0.235267, l2: 0.229361, l3: 0.223791, l4: 0.224017, l5: 0.233748, l6: 0.270575\n",
            "\n",
            "[epoch:  28/100000, batch:   276/  840, ite: 1913] train loss: 1.971512, tar: 0.259621 \n",
            "l0: 0.096685, l1: 0.101997, l2: 0.098064, l3: 0.110415, l4: 0.112389, l5: 0.124534, l6: 0.179744\n",
            "\n",
            "[epoch:  28/100000, batch:   288/  840, ite: 1914] train loss: 1.970912, tar: 0.259536 \n",
            "l0: 0.163237, l1: 0.177473, l2: 0.176503, l3: 0.159277, l4: 0.157266, l5: 0.171028, l6: 0.215911\n",
            "\n",
            "[epoch:  28/100000, batch:   300/  840, ite: 1915] train loss: 1.970521, tar: 0.259485 \n",
            "l0: 0.195851, l1: 0.203494, l2: 0.196090, l3: 0.206966, l4: 0.214056, l5: 0.219382, l6: 0.251260\n",
            "\n",
            "[epoch:  28/100000, batch:   312/  840, ite: 1916] train loss: 1.970268, tar: 0.259452 \n",
            "l0: 0.143832, l1: 0.144274, l2: 0.140127, l3: 0.157601, l4: 0.167538, l5: 0.181379, l6: 0.243505\n",
            "\n",
            "[epoch:  28/100000, batch:   324/  840, ite: 1917] train loss: 1.969855, tar: 0.259392 \n",
            "l0: 0.246400, l1: 0.231756, l2: 0.265293, l3: 0.242626, l4: 0.255171, l5: 0.278692, l6: 0.324384\n",
            "\n",
            "[epoch:  28/100000, batch:   336/  840, ite: 1918] train loss: 1.969790, tar: 0.259385 \n",
            "l0: 0.164910, l1: 0.172784, l2: 0.169966, l3: 0.171243, l4: 0.175545, l5: 0.184942, l6: 0.240275\n",
            "\n",
            "[epoch:  28/100000, batch:   348/  840, ite: 1919] train loss: 1.969430, tar: 0.259336 \n",
            "l0: 0.176010, l1: 0.169097, l2: 0.185359, l3: 0.205017, l4: 0.200200, l5: 0.207562, l6: 0.249675\n",
            "\n",
            "[epoch:  28/100000, batch:   360/  840, ite: 1920] train loss: 1.969130, tar: 0.259292 \n",
            "l0: 0.284595, l1: 0.283805, l2: 0.276960, l3: 0.326762, l4: 0.325296, l5: 0.359568, l6: 0.421054\n",
            "\n",
            "[epoch:  28/100000, batch:   372/  840, ite: 1921] train loss: 1.969291, tar: 0.259306 \n",
            "l0: 0.160003, l1: 0.159045, l2: 0.166088, l3: 0.151861, l4: 0.158728, l5: 0.172984, l6: 0.222337\n",
            "\n",
            "[epoch:  28/100000, batch:   384/  840, ite: 1922] train loss: 1.968886, tar: 0.259254 \n",
            "l0: 0.191505, l1: 0.190270, l2: 0.196383, l3: 0.213237, l4: 0.218112, l5: 0.241483, l6: 0.267138\n",
            "\n",
            "[epoch:  28/100000, batch:   396/  840, ite: 1923] train loss: 1.968651, tar: 0.259219 \n",
            "l0: 0.124405, l1: 0.124288, l2: 0.126262, l3: 0.135741, l4: 0.143604, l5: 0.156863, l6: 0.207701\n",
            "\n",
            "[epoch:  28/100000, batch:   408/  840, ite: 1924] train loss: 1.968158, tar: 0.259149 \n",
            "l0: 0.189425, l1: 0.186986, l2: 0.191763, l3: 0.235294, l4: 0.236747, l5: 0.252622, l6: 0.276479\n",
            "\n",
            "[epoch:  28/100000, batch:   420/  840, ite: 1925] train loss: 1.967950, tar: 0.259112 \n",
            "l0: 0.348966, l1: 0.379812, l2: 0.357488, l3: 0.378124, l4: 0.365155, l5: 0.341592, l6: 0.329103\n",
            "\n",
            "[epoch:  28/100000, batch:   432/  840, ite: 1926] train loss: 1.968227, tar: 0.259159 \n",
            "l0: 0.175137, l1: 0.188409, l2: 0.182274, l3: 0.200999, l4: 0.195158, l5: 0.212192, l6: 0.253679\n",
            "\n",
            "[epoch:  28/100000, batch:   444/  840, ite: 1927] train loss: 1.967936, tar: 0.259115 \n",
            "l0: 0.201899, l1: 0.205066, l2: 0.213350, l3: 0.212999, l4: 0.215607, l5: 0.229802, l6: 0.256541\n",
            "\n",
            "[epoch:  28/100000, batch:   456/  840, ite: 1928] train loss: 1.967712, tar: 0.259086 \n",
            "l0: 0.242106, l1: 0.259173, l2: 0.239560, l3: 0.250523, l4: 0.251999, l5: 0.264192, l6: 0.310316\n",
            "\n",
            "[epoch:  28/100000, batch:   468/  840, ite: 1929] train loss: 1.967634, tar: 0.259077 \n",
            "l0: 0.137051, l1: 0.139026, l2: 0.141944, l3: 0.153177, l4: 0.158856, l5: 0.178451, l6: 0.221578\n",
            "\n",
            "[epoch:  28/100000, batch:   480/  840, ite: 1930] train loss: 1.967200, tar: 0.259014 \n",
            "l0: 0.183849, l1: 0.181457, l2: 0.188913, l3: 0.188064, l4: 0.191871, l5: 0.197353, l6: 0.241454\n",
            "\n",
            "[epoch:  28/100000, batch:   492/  840, ite: 1931] train loss: 1.966892, tar: 0.258975 \n",
            "l0: 0.169015, l1: 0.167798, l2: 0.174407, l3: 0.189205, l4: 0.196605, l5: 0.208808, l6: 0.241554\n",
            "\n",
            "[epoch:  28/100000, batch:   504/  840, ite: 1932] train loss: 1.966572, tar: 0.258928 \n",
            "l0: 0.143107, l1: 0.145444, l2: 0.152372, l3: 0.159812, l4: 0.161305, l5: 0.170920, l6: 0.197076\n",
            "\n",
            "[epoch:  28/100000, batch:   516/  840, ite: 1933] train loss: 1.966139, tar: 0.258868 \n",
            "l0: 0.167258, l1: 0.183423, l2: 0.169945, l3: 0.162546, l4: 0.167441, l5: 0.182568, l6: 0.203743\n",
            "\n",
            "[epoch:  28/100000, batch:   528/  840, ite: 1934] train loss: 1.965762, tar: 0.258821 \n",
            "l0: 0.187834, l1: 0.185902, l2: 0.195112, l3: 0.203178, l4: 0.209637, l5: 0.209745, l6: 0.237146\n",
            "\n",
            "[epoch:  28/100000, batch:   540/  840, ite: 1935] train loss: 1.965484, tar: 0.258784 \n",
            "l0: 0.163712, l1: 0.181579, l2: 0.174558, l3: 0.145804, l4: 0.151855, l5: 0.166717, l6: 0.193030\n",
            "\n",
            "[epoch:  28/100000, batch:   552/  840, ite: 1936] train loss: 1.965077, tar: 0.258735 \n",
            "l0: 0.281200, l1: 0.265718, l2: 0.294352, l3: 0.305145, l4: 0.316359, l5: 0.320873, l6: 0.343048\n",
            "\n",
            "[epoch:  28/100000, batch:   564/  840, ite: 1937] train loss: 1.965160, tar: 0.258747 \n",
            "l0: 0.161105, l1: 0.167373, l2: 0.170881, l3: 0.166694, l4: 0.171761, l5: 0.186142, l6: 0.225518\n",
            "\n",
            "[epoch:  28/100000, batch:   576/  840, ite: 1938] train loss: 1.964791, tar: 0.258696 \n",
            "l0: 0.127866, l1: 0.131428, l2: 0.133934, l3: 0.140940, l4: 0.149825, l5: 0.159203, l6: 0.200258\n",
            "\n",
            "[epoch:  28/100000, batch:   588/  840, ite: 1939] train loss: 1.964316, tar: 0.258629 \n",
            "l0: 0.139574, l1: 0.139176, l2: 0.139262, l3: 0.149094, l4: 0.155586, l5: 0.175421, l6: 0.239199\n",
            "\n",
            "[epoch:  28/100000, batch:   600/  840, ite: 1940] train loss: 1.963890, tar: 0.258568 \n",
            "l0: 0.205321, l1: 0.196099, l2: 0.209890, l3: 0.227838, l4: 0.232827, l5: 0.250742, l6: 0.287451\n",
            "\n",
            "[epoch:  28/100000, batch:   612/  840, ite: 1941] train loss: 1.963707, tar: 0.258540 \n",
            "l0: 0.214257, l1: 0.213212, l2: 0.213693, l3: 0.228140, l4: 0.236261, l5: 0.251285, l6: 0.288641\n",
            "\n",
            "[epoch:  28/100000, batch:   624/  840, ite: 1942] train loss: 1.963544, tar: 0.258517 \n",
            "l0: 0.213266, l1: 0.219008, l2: 0.224829, l3: 0.222342, l4: 0.218188, l5: 0.229800, l6: 0.258944\n",
            "\n",
            "[epoch:  28/100000, batch:   636/  840, ite: 1943] train loss: 1.963350, tar: 0.258494 \n",
            "l0: 0.209775, l1: 0.220355, l2: 0.207070, l3: 0.211321, l4: 0.220101, l5: 0.235971, l6: 0.285920\n",
            "\n",
            "[epoch:  28/100000, batch:   648/  840, ite: 1944] train loss: 1.963158, tar: 0.258469 \n",
            "l0: 0.135280, l1: 0.142656, l2: 0.151512, l3: 0.149535, l4: 0.148685, l5: 0.146163, l6: 0.181549\n",
            "\n",
            "[epoch:  28/100000, batch:   660/  840, ite: 1945] train loss: 1.962691, tar: 0.258406 \n",
            "l0: 0.210496, l1: 0.198798, l2: 0.215179, l3: 0.226643, l4: 0.239064, l5: 0.248272, l6: 0.270442\n",
            "\n",
            "[epoch:  28/100000, batch:   672/  840, ite: 1946] train loss: 1.962509, tar: 0.258381 \n",
            "l0: 0.362745, l1: 0.358484, l2: 0.386943, l3: 0.378317, l4: 0.378890, l5: 0.357411, l6: 0.349425\n",
            "\n",
            "[epoch:  28/100000, batch:   684/  840, ite: 1947] train loss: 1.962822, tar: 0.258435 \n",
            "l0: 0.194525, l1: 0.193362, l2: 0.201570, l3: 0.203944, l4: 0.206293, l5: 0.215285, l6: 0.253613\n",
            "\n",
            "[epoch:  28/100000, batch:   696/  840, ite: 1948] train loss: 1.962569, tar: 0.258402 \n",
            "l0: 0.173011, l1: 0.175389, l2: 0.175979, l3: 0.175432, l4: 0.182846, l5: 0.191125, l6: 0.243933\n",
            "\n",
            "[epoch:  28/100000, batch:   708/  840, ite: 1949] train loss: 1.962238, tar: 0.258358 \n",
            "l0: 0.122908, l1: 0.121518, l2: 0.142948, l3: 0.133401, l4: 0.131907, l5: 0.138511, l6: 0.158849\n",
            "\n",
            "[epoch:  28/100000, batch:   720/  840, ite: 1950] train loss: 1.961719, tar: 0.258289 \n",
            "l0: 0.228607, l1: 0.224048, l2: 0.240848, l3: 0.237990, l4: 0.241170, l5: 0.253632, l6: 0.279376\n",
            "\n",
            "[epoch:  28/100000, batch:   732/  840, ite: 1951] train loss: 1.961587, tar: 0.258273 \n",
            "l0: 0.127590, l1: 0.129882, l2: 0.127373, l3: 0.140519, l4: 0.149370, l5: 0.170529, l6: 0.190272\n",
            "\n",
            "[epoch:  28/100000, batch:   744/  840, ite: 1952] train loss: 1.961113, tar: 0.258206 \n",
            "l0: 0.164007, l1: 0.172738, l2: 0.168090, l3: 0.173248, l4: 0.176781, l5: 0.192565, l6: 0.224874\n",
            "\n",
            "[epoch:  28/100000, batch:   756/  840, ite: 1953] train loss: 1.960760, tar: 0.258158 \n",
            "l0: 0.180887, l1: 0.181184, l2: 0.179715, l3: 0.191971, l4: 0.196490, l5: 0.208789, l6: 0.242373\n",
            "\n",
            "[epoch:  28/100000, batch:   768/  840, ite: 1954] train loss: 1.960464, tar: 0.258119 \n",
            "l0: 0.205476, l1: 0.219187, l2: 0.204683, l3: 0.212357, l4: 0.217554, l5: 0.216785, l6: 0.255015\n",
            "\n",
            "[epoch:  28/100000, batch:   780/  840, ite: 1955] train loss: 1.960244, tar: 0.258092 \n",
            "l0: 0.201478, l1: 0.201264, l2: 0.201971, l3: 0.212046, l4: 0.224400, l5: 0.226271, l6: 0.274610\n",
            "\n",
            "[epoch:  28/100000, batch:   792/  840, ite: 1956] train loss: 1.960030, tar: 0.258063 \n",
            "l0: 0.133063, l1: 0.131010, l2: 0.136268, l3: 0.144292, l4: 0.150402, l5: 0.167640, l6: 0.211412\n",
            "\n",
            "[epoch:  28/100000, batch:   804/  840, ite: 1957] train loss: 1.959577, tar: 0.257999 \n",
            "l0: 0.253849, l1: 0.249808, l2: 0.266577, l3: 0.256085, l4: 0.263176, l5: 0.257299, l6: 0.289042\n",
            "\n",
            "[epoch:  28/100000, batch:   816/  840, ite: 1958] train loss: 1.959514, tar: 0.257997 \n",
            "l0: 0.158155, l1: 0.167831, l2: 0.158990, l3: 0.166476, l4: 0.170461, l5: 0.191425, l6: 0.244417\n",
            "\n",
            "[epoch:  28/100000, batch:   828/  840, ite: 1959] train loss: 1.959156, tar: 0.257946 \n",
            "l0: 0.083360, l1: 0.085890, l2: 0.082630, l3: 0.085067, l4: 0.088936, l5: 0.112014, l6: 0.161616\n",
            "\n",
            "[epoch:  28/100000, batch:   840/  840, ite: 1960] train loss: 1.958513, tar: 0.257857 \n",
            "l0: 0.190920, l1: 0.181376, l2: 0.189405, l3: 0.204724, l4: 0.209553, l5: 0.245200, l6: 0.302059\n",
            "\n",
            "[epoch:  29/100000, batch:    12/  840, ite: 1961] train loss: 1.958292, tar: 0.257823 \n",
            "l0: 0.315643, l1: 0.348901, l2: 0.313871, l3: 0.334288, l4: 0.322279, l5: 0.303351, l6: 0.334412\n",
            "\n",
            "[epoch:  29/100000, batch:    24/  840, ite: 1962] train loss: 1.958452, tar: 0.257852 \n",
            "l0: 0.176603, l1: 0.167601, l2: 0.180428, l3: 0.215989, l4: 0.221350, l5: 0.227716, l6: 0.245169\n",
            "\n",
            "[epoch:  29/100000, batch:    36/  840, ite: 1963] train loss: 1.958185, tar: 0.257811 \n",
            "l0: 0.214001, l1: 0.237201, l2: 0.213809, l3: 0.221476, l4: 0.221820, l5: 0.235496, l6: 0.315267\n",
            "\n",
            "[epoch:  29/100000, batch:    48/  840, ite: 1964] train loss: 1.958033, tar: 0.257788 \n",
            "l0: 0.244617, l1: 0.214103, l2: 0.256134, l3: 0.319371, l4: 0.342695, l5: 0.297701, l6: 0.287767\n",
            "\n",
            "[epoch:  29/100000, batch:    60/  840, ite: 1965] train loss: 1.958035, tar: 0.257782 \n",
            "l0: 0.252393, l1: 0.239993, l2: 0.269048, l3: 0.317162, l4: 0.315644, l5: 0.283389, l6: 0.284500\n",
            "\n",
            "[epoch:  29/100000, batch:    72/  840, ite: 1966] train loss: 1.958037, tar: 0.257779 \n",
            "l0: 0.314291, l1: 0.317930, l2: 0.323188, l3: 0.325336, l4: 0.343771, l5: 0.308203, l6: 0.330384\n",
            "\n",
            "[epoch:  29/100000, batch:    84/  840, ite: 1967] train loss: 1.958192, tar: 0.257808 \n",
            "l0: 0.149976, l1: 0.146142, l2: 0.158610, l3: 0.170602, l4: 0.172500, l5: 0.181877, l6: 0.229988\n",
            "\n",
            "[epoch:  29/100000, batch:    96/  840, ite: 1968] train loss: 1.957812, tar: 0.257753 \n",
            "l0: 0.206961, l1: 0.221171, l2: 0.197019, l3: 0.203273, l4: 0.199218, l5: 0.279559, l6: 0.344491\n",
            "\n",
            "[epoch:  29/100000, batch:   108/  840, ite: 1969] train loss: 1.957656, tar: 0.257727 \n",
            "l0: 0.140796, l1: 0.152611, l2: 0.138214, l3: 0.163193, l4: 0.162227, l5: 0.220041, l6: 0.251637\n",
            "\n",
            "[epoch:  29/100000, batch:   120/  840, ite: 1970] train loss: 1.957286, tar: 0.257668 \n",
            "l0: 0.152430, l1: 0.153011, l2: 0.157034, l3: 0.161754, l4: 0.163200, l5: 0.179213, l6: 0.230778\n",
            "\n",
            "[epoch:  29/100000, batch:   132/  840, ite: 1971] train loss: 1.956901, tar: 0.257614 \n",
            "l0: 0.184253, l1: 0.180326, l2: 0.191155, l3: 0.209980, l4: 0.215924, l5: 0.227603, l6: 0.246852\n",
            "\n",
            "[epoch:  29/100000, batch:   144/  840, ite: 1972] train loss: 1.956647, tar: 0.257577 \n",
            "l0: 0.191444, l1: 0.173275, l2: 0.202421, l3: 0.215418, l4: 0.218034, l5: 0.255990, l6: 0.292327\n",
            "\n",
            "[epoch:  29/100000, batch:   156/  840, ite: 1973] train loss: 1.956440, tar: 0.257544 \n",
            "l0: 0.148719, l1: 0.151238, l2: 0.154858, l3: 0.161211, l4: 0.169908, l5: 0.183256, l6: 0.218642\n",
            "\n",
            "[epoch:  29/100000, batch:   168/  840, ite: 1974] train loss: 1.956051, tar: 0.257488 \n",
            "l0: 0.188099, l1: 0.187889, l2: 0.190703, l3: 0.199327, l4: 0.204262, l5: 0.232504, l6: 0.274956\n",
            "\n",
            "[epoch:  29/100000, batch:   180/  840, ite: 1975] train loss: 1.955809, tar: 0.257453 \n",
            "l0: 0.178580, l1: 0.196259, l2: 0.175858, l3: 0.176509, l4: 0.188383, l5: 0.200217, l6: 0.235946\n",
            "\n",
            "[epoch:  29/100000, batch:   192/  840, ite: 1976] train loss: 1.955503, tar: 0.257413 \n",
            "l0: 0.302180, l1: 0.306346, l2: 0.303460, l3: 0.318003, l4: 0.335381, l5: 0.337582, l6: 0.359471\n",
            "\n",
            "[epoch:  29/100000, batch:   204/  840, ite: 1977] train loss: 1.955658, tar: 0.257436 \n",
            "l0: 0.179407, l1: 0.188156, l2: 0.183988, l3: 0.198985, l4: 0.202762, l5: 0.201668, l6: 0.229532\n",
            "\n",
            "[epoch:  29/100000, batch:   216/  840, ite: 1978] train loss: 1.955369, tar: 0.257397 \n",
            "l0: 0.135795, l1: 0.136304, l2: 0.137759, l3: 0.146906, l4: 0.157556, l5: 0.181131, l6: 0.229907\n",
            "\n",
            "[epoch:  29/100000, batch:   228/  840, ite: 1979] train loss: 1.954950, tar: 0.257335 \n",
            "l0: 0.112023, l1: 0.112885, l2: 0.114926, l3: 0.135091, l4: 0.138842, l5: 0.150461, l6: 0.202128\n",
            "\n",
            "[epoch:  29/100000, batch:   240/  840, ite: 1980] train loss: 1.954451, tar: 0.257262 \n",
            "l0: 0.190770, l1: 0.193893, l2: 0.196689, l3: 0.206652, l4: 0.214035, l5: 0.236213, l6: 0.263296\n",
            "\n",
            "[epoch:  29/100000, batch:   252/  840, ite: 1981] train loss: 1.954222, tar: 0.257228 \n",
            "l0: 0.162156, l1: 0.160889, l2: 0.165627, l3: 0.177109, l4: 0.181664, l5: 0.210199, l6: 0.248867\n",
            "\n",
            "[epoch:  29/100000, batch:   264/  840, ite: 1982] train loss: 1.953895, tar: 0.257180 \n",
            "l0: 0.132222, l1: 0.133933, l2: 0.135173, l3: 0.145584, l4: 0.152724, l5: 0.162878, l6: 0.206556\n",
            "\n",
            "[epoch:  29/100000, batch:   276/  840, ite: 1983] train loss: 1.953449, tar: 0.257117 \n",
            "l0: 0.183228, l1: 0.184665, l2: 0.183548, l3: 0.203525, l4: 0.214389, l5: 0.206140, l6: 0.242685\n",
            "\n",
            "[epoch:  29/100000, batch:   288/  840, ite: 1984] train loss: 1.953179, tar: 0.257080 \n",
            "l0: 0.111199, l1: 0.112259, l2: 0.114648, l3: 0.130797, l4: 0.138198, l5: 0.147910, l6: 0.192292\n",
            "\n",
            "[epoch:  29/100000, batch:   300/  840, ite: 1985] train loss: 1.952672, tar: 0.257006 \n",
            "l0: 0.214561, l1: 0.228934, l2: 0.222027, l3: 0.225483, l4: 0.230691, l5: 0.241083, l6: 0.278414\n",
            "\n",
            "[epoch:  29/100000, batch:   312/  840, ite: 1986] train loss: 1.952515, tar: 0.256985 \n",
            "l0: 0.347801, l1: 0.356428, l2: 0.373386, l3: 0.341734, l4: 0.341611, l5: 0.337125, l6: 0.350038\n",
            "\n",
            "[epoch:  29/100000, batch:   324/  840, ite: 1987] train loss: 1.952765, tar: 0.257031 \n",
            "l0: 0.153850, l1: 0.158630, l2: 0.165779, l3: 0.177303, l4: 0.176963, l5: 0.180698, l6: 0.210009\n",
            "\n",
            "[epoch:  29/100000, batch:   336/  840, ite: 1988] train loss: 1.952398, tar: 0.256979 \n",
            "l0: 0.251114, l1: 0.264268, l2: 0.259724, l3: 0.257252, l4: 0.248103, l5: 0.248065, l6: 0.277177\n",
            "\n",
            "[epoch:  29/100000, batch:   348/  840, ite: 1989] train loss: 1.952324, tar: 0.256976 \n",
            "l0: 0.174647, l1: 0.172673, l2: 0.178277, l3: 0.190241, l4: 0.195638, l5: 0.211608, l6: 0.264355\n",
            "\n",
            "[epoch:  29/100000, batch:   360/  840, ite: 1990] train loss: 1.952040, tar: 0.256935 \n",
            "l0: 0.239254, l1: 0.230433, l2: 0.251427, l3: 0.263946, l4: 0.262535, l5: 0.256804, l6: 0.280832\n",
            "\n",
            "[epoch:  29/100000, batch:   372/  840, ite: 1991] train loss: 1.951957, tar: 0.256926 \n",
            "l0: 0.124919, l1: 0.133400, l2: 0.126074, l3: 0.129361, l4: 0.132744, l5: 0.155231, l6: 0.202319\n",
            "\n",
            "[epoch:  29/100000, batch:   384/  840, ite: 1992] train loss: 1.951481, tar: 0.256859 \n",
            "l0: 0.172621, l1: 0.175568, l2: 0.175861, l3: 0.156232, l4: 0.161524, l5: 0.174610, l6: 0.219063\n",
            "\n",
            "[epoch:  29/100000, batch:   396/  840, ite: 1993] train loss: 1.951122, tar: 0.256817 \n",
            "l0: 0.143348, l1: 0.147980, l2: 0.147334, l3: 0.179450, l4: 0.181112, l5: 0.188059, l6: 0.222317\n",
            "\n",
            "[epoch:  29/100000, batch:   408/  840, ite: 1994] train loss: 1.950750, tar: 0.256760 \n",
            "l0: 0.154216, l1: 0.144643, l2: 0.169248, l3: 0.176890, l4: 0.178419, l5: 0.183899, l6: 0.206605\n",
            "\n",
            "[epoch:  29/100000, batch:   420/  840, ite: 1995] train loss: 1.950380, tar: 0.256709 \n",
            "l0: 0.160330, l1: 0.169288, l2: 0.164504, l3: 0.166368, l4: 0.173489, l5: 0.174499, l6: 0.200361\n",
            "\n",
            "[epoch:  29/100000, batch:   432/  840, ite: 1996] train loss: 1.950009, tar: 0.256661 \n",
            "l0: 0.178635, l1: 0.177682, l2: 0.186479, l3: 0.197294, l4: 0.204947, l5: 0.215263, l6: 0.231742\n",
            "\n",
            "[epoch:  29/100000, batch:   444/  840, ite: 1997] train loss: 1.949729, tar: 0.256622 \n",
            "l0: 0.216652, l1: 0.216588, l2: 0.228492, l3: 0.244743, l4: 0.247971, l5: 0.237572, l6: 0.255865\n",
            "\n",
            "[epoch:  29/100000, batch:   456/  840, ite: 1998] train loss: 1.949578, tar: 0.256602 \n",
            "l0: 0.340034, l1: 0.323380, l2: 0.359508, l3: 0.372325, l4: 0.384109, l5: 0.414793, l6: 0.404125\n",
            "\n",
            "[epoch:  29/100000, batch:   468/  840, ite: 1999] train loss: 1.949903, tar: 0.256643 \n",
            "l0: 0.164005, l1: 0.168084, l2: 0.171368, l3: 0.190839, l4: 0.183037, l5: 0.184332, l6: 0.218973\n",
            "\n",
            "[epoch:  29/100000, batch:   480/  840, ite: 2000] train loss: 1.949568, tar: 0.256597 \n",
            "l0: 0.132512, l1: 0.135753, l2: 0.149246, l3: 0.145662, l4: 0.141624, l5: 0.154615, l6: 0.181170\n",
            "\n",
            "[epoch:  29/100000, batch:   492/  840, ite: 2001] train loss: 1.040583, tar: 0.132512 \n",
            "l0: 0.213074, l1: 0.220308, l2: 0.223160, l3: 0.238275, l4: 0.236821, l5: 0.249603, l6: 0.292967\n",
            "\n",
            "[epoch:  29/100000, batch:   504/  840, ite: 2002] train loss: 1.357396, tar: 0.172793 \n",
            "l0: 0.225005, l1: 0.229970, l2: 0.228621, l3: 0.257915, l4: 0.248477, l5: 0.250573, l6: 0.293187\n",
            "\n",
            "[epoch:  29/100000, batch:   516/  840, ite: 2003] train loss: 1.482847, tar: 0.190197 \n",
            "l0: 0.087490, l1: 0.092591, l2: 0.093950, l3: 0.105836, l4: 0.105057, l5: 0.114871, l6: 0.151693\n",
            "\n",
            "[epoch:  29/100000, batch:   528/  840, ite: 2004] train loss: 1.300007, tar: 0.164520 \n",
            "l0: 0.134341, l1: 0.131948, l2: 0.148456, l3: 0.157597, l4: 0.154099, l5: 0.159568, l6: 0.194938\n",
            "\n",
            "[epoch:  29/100000, batch:   540/  840, ite: 2005] train loss: 1.256195, tar: 0.158485 \n",
            "l0: 0.225206, l1: 0.222837, l2: 0.224740, l3: 0.237807, l4: 0.243080, l5: 0.259341, l6: 0.313995\n",
            "\n",
            "[epoch:  29/100000, batch:   552/  840, ite: 2006] train loss: 1.334663, tar: 0.169605 \n",
            "l0: 0.110968, l1: 0.113695, l2: 0.121989, l3: 0.119965, l4: 0.121358, l5: 0.132082, l6: 0.171878\n",
            "\n",
            "[epoch:  29/100000, batch:   564/  840, ite: 2007] train loss: 1.271417, tar: 0.161228 \n",
            "l0: 0.298860, l1: 0.300769, l2: 0.302932, l3: 0.326528, l4: 0.320782, l5: 0.321441, l6: 0.338661\n",
            "\n",
            "[epoch:  29/100000, batch:   576/  840, ite: 2008] train loss: 1.388736, tar: 0.178432 \n",
            "l0: 0.170322, l1: 0.184220, l2: 0.183841, l3: 0.180620, l4: 0.172718, l5: 0.177375, l6: 0.200766\n",
            "\n",
            "[epoch:  29/100000, batch:   588/  840, ite: 2009] train loss: 1.375528, tar: 0.177531 \n",
            "l0: 0.235788, l1: 0.214862, l2: 0.244767, l3: 0.281672, l4: 0.280306, l5: 0.314089, l6: 0.364768\n",
            "\n",
            "[epoch:  29/100000, batch:   600/  840, ite: 2010] train loss: 1.431600, tar: 0.183357 \n",
            "l0: 0.173208, l1: 0.185065, l2: 0.168297, l3: 0.198492, l4: 0.203467, l5: 0.210292, l6: 0.248739\n",
            "\n",
            "[epoch:  29/100000, batch:   612/  840, ite: 2011] train loss: 1.427597, tar: 0.182434 \n",
            "l0: 0.166021, l1: 0.168207, l2: 0.170570, l3: 0.175210, l4: 0.177805, l5: 0.193619, l6: 0.230282\n",
            "\n",
            "[epoch:  29/100000, batch:   624/  840, ite: 2012] train loss: 1.415440, tar: 0.181066 \n",
            "l0: 0.284318, l1: 0.286985, l2: 0.293729, l3: 0.303819, l4: 0.310917, l5: 0.322549, l6: 0.347303\n",
            "\n",
            "[epoch:  29/100000, batch:   636/  840, ite: 2013] train loss: 1.471915, tar: 0.189009 \n",
            "l0: 0.174047, l1: 0.180233, l2: 0.177525, l3: 0.178076, l4: 0.184858, l5: 0.198813, l6: 0.245455\n",
            "\n",
            "[epoch:  29/100000, batch:   648/  840, ite: 2014] train loss: 1.462422, tar: 0.187940 \n",
            "l0: 0.110739, l1: 0.106984, l2: 0.122898, l3: 0.113701, l4: 0.115340, l5: 0.135140, l6: 0.186687\n",
            "\n",
            "[epoch:  29/100000, batch:   660/  840, ite: 2015] train loss: 1.424360, tar: 0.182793 \n",
            "l0: 0.192040, l1: 0.186024, l2: 0.211019, l3: 0.198747, l4: 0.198537, l5: 0.209062, l6: 0.257608\n",
            "\n",
            "[epoch:  29/100000, batch:   672/  840, ite: 2016] train loss: 1.426152, tar: 0.183371 \n",
            "l0: 0.140532, l1: 0.154680, l2: 0.141044, l3: 0.139440, l4: 0.146279, l5: 0.165118, l6: 0.217374\n",
            "\n",
            "[epoch:  29/100000, batch:   684/  840, ite: 2017] train loss: 1.407229, tar: 0.180851 \n",
            "l0: 0.202952, l1: 0.214400, l2: 0.205668, l3: 0.192316, l4: 0.200790, l5: 0.221538, l6: 0.244384\n",
            "\n",
            "[epoch:  29/100000, batch:   696/  840, ite: 2018] train loss: 1.411386, tar: 0.182079 \n",
            "l0: 0.166723, l1: 0.158845, l2: 0.177172, l3: 0.191259, l4: 0.196359, l5: 0.204624, l6: 0.234263\n",
            "\n",
            "[epoch:  29/100000, batch:   708/  840, ite: 2019] train loss: 1.407063, tar: 0.181271 \n",
            "l0: 0.151794, l1: 0.159151, l2: 0.155256, l3: 0.156261, l4: 0.159223, l5: 0.173519, l6: 0.221728\n",
            "\n",
            "[epoch:  29/100000, batch:   720/  840, ite: 2020] train loss: 1.395556, tar: 0.179797 \n",
            "l0: 0.165148, l1: 0.164426, l2: 0.171270, l3: 0.173336, l4: 0.174995, l5: 0.182349, l6: 0.231215\n",
            "\n",
            "[epoch:  29/100000, batch:   732/  840, ite: 2021] train loss: 1.389232, tar: 0.179099 \n",
            "l0: 0.133142, l1: 0.129013, l2: 0.142379, l3: 0.158550, l4: 0.160293, l5: 0.173428, l6: 0.206112\n",
            "\n",
            "[epoch:  29/100000, batch:   744/  840, ite: 2022] train loss: 1.376217, tar: 0.177011 \n",
            "l0: 0.095204, l1: 0.109504, l2: 0.094159, l3: 0.102533, l4: 0.110671, l5: 0.111479, l6: 0.148563\n",
            "\n",
            "[epoch:  29/100000, batch:   756/  840, ite: 2023] train loss: 1.349952, tar: 0.173454 \n",
            "l0: 0.104231, l1: 0.126447, l2: 0.099423, l3: 0.109536, l4: 0.114918, l5: 0.127032, l6: 0.155869\n",
            "\n",
            "[epoch:  29/100000, batch:   768/  840, ite: 2024] train loss: 1.328598, tar: 0.170569 \n",
            "l0: 0.150018, l1: 0.166694, l2: 0.147810, l3: 0.156885, l4: 0.158915, l5: 0.164093, l6: 0.215471\n",
            "\n",
            "[epoch:  29/100000, batch:   780/  840, ite: 2025] train loss: 1.321849, tar: 0.169747 \n",
            "l0: 0.193011, l1: 0.202954, l2: 0.201220, l3: 0.205067, l4: 0.207617, l5: 0.225163, l6: 0.251800\n",
            "\n",
            "[epoch:  29/100000, batch:   792/  840, ite: 2026] train loss: 1.328195, tar: 0.170642 \n",
            "l0: 0.114257, l1: 0.121818, l2: 0.117908, l3: 0.132314, l4: 0.135188, l5: 0.157603, l6: 0.199403\n",
            "\n",
            "[epoch:  29/100000, batch:   804/  840, ite: 2027] train loss: 1.315243, tar: 0.168554 \n",
            "l0: 0.115858, l1: 0.117253, l2: 0.117717, l3: 0.132873, l4: 0.138566, l5: 0.163263, l6: 0.219652\n",
            "\n",
            "[epoch:  29/100000, batch:   816/  840, ite: 2028] train loss: 1.304169, tar: 0.166672 \n",
            "l0: 0.220748, l1: 0.209190, l2: 0.234116, l3: 0.252527, l4: 0.256052, l5: 0.260704, l6: 0.301122\n",
            "\n",
            "[epoch:  29/100000, batch:   828/  840, ite: 2029] train loss: 1.319007, tar: 0.168537 \n",
            "l0: 0.156753, l1: 0.153540, l2: 0.172247, l3: 0.165678, l4: 0.168080, l5: 0.172042, l6: 0.195514\n",
            "\n",
            "[epoch:  29/100000, batch:   840/  840, ite: 2030] train loss: 1.314502, tar: 0.168144 \n",
            "l0: 0.129216, l1: 0.133006, l2: 0.135664, l3: 0.138154, l4: 0.142665, l5: 0.150657, l6: 0.186132\n",
            "\n",
            "[epoch:  30/100000, batch:    12/  840, ite: 2031] train loss: 1.304856, tar: 0.166888 \n",
            "l0: 0.138736, l1: 0.143807, l2: 0.135390, l3: 0.140272, l4: 0.145090, l5: 0.167876, l6: 0.233439\n",
            "\n",
            "[epoch:  30/100000, batch:    24/  840, ite: 2032] train loss: 1.298599, tar: 0.166008 \n",
            "l0: 0.300617, l1: 0.310166, l2: 0.299566, l3: 0.314684, l4: 0.308246, l5: 0.333392, l6: 0.334801\n",
            "\n",
            "[epoch:  30/100000, batch:    36/  840, ite: 2033] train loss: 1.325958, tar: 0.170087 \n",
            "l0: 0.151417, l1: 0.154793, l2: 0.168573, l3: 0.146812, l4: 0.143759, l5: 0.146486, l6: 0.188830\n",
            "\n",
            "[epoch:  30/100000, batch:    48/  840, ite: 2034] train loss: 1.319332, tar: 0.169538 \n",
            "l0: 0.215613, l1: 0.209191, l2: 0.224446, l3: 0.226491, l4: 0.229227, l5: 0.218518, l6: 0.237357\n",
            "\n",
            "[epoch:  30/100000, batch:    60/  840, ite: 2035] train loss: 1.326233, tar: 0.170855 \n",
            "l0: 0.114251, l1: 0.116473, l2: 0.119097, l3: 0.116788, l4: 0.118929, l5: 0.131455, l6: 0.172451\n",
            "\n",
            "[epoch:  30/100000, batch:    72/  840, ite: 2036] train loss: 1.314100, tar: 0.169282 \n",
            "l0: 0.183587, l1: 0.185183, l2: 0.189375, l3: 0.176982, l4: 0.181063, l5: 0.201770, l6: 0.241212\n",
            "\n",
            "[epoch:  30/100000, batch:    84/  840, ite: 2037] train loss: 1.315318, tar: 0.169669 \n",
            "l0: 0.112845, l1: 0.109190, l2: 0.115317, l3: 0.122417, l4: 0.126618, l5: 0.149402, l6: 0.203636\n",
            "\n",
            "[epoch:  30/100000, batch:    96/  840, ite: 2038] train loss: 1.305426, tar: 0.168174 \n",
            "l0: 0.192331, l1: 0.184580, l2: 0.199835, l3: 0.202397, l4: 0.207467, l5: 0.231743, l6: 0.274524\n",
            "\n",
            "[epoch:  30/100000, batch:   108/  840, ite: 2039] train loss: 1.310233, tar: 0.168793 \n",
            "l0: 0.179855, l1: 0.185679, l2: 0.186197, l3: 0.181190, l4: 0.178946, l5: 0.192728, l6: 0.223215\n",
            "\n",
            "[epoch:  30/100000, batch:   120/  840, ite: 2040] train loss: 1.310672, tar: 0.169070 \n",
            "l0: 0.181920, l1: 0.187731, l2: 0.185879, l3: 0.203287, l4: 0.197522, l5: 0.221733, l6: 0.291528\n",
            "\n",
            "[epoch:  30/100000, batch:   132/  840, ite: 2041] train loss: 1.314548, tar: 0.169383 \n",
            "l0: 0.223645, l1: 0.220206, l2: 0.227468, l3: 0.246221, l4: 0.247307, l5: 0.259561, l6: 0.293060\n",
            "\n",
            "[epoch:  30/100000, batch:   144/  840, ite: 2042] train loss: 1.324142, tar: 0.170675 \n",
            "l0: 0.152375, l1: 0.153432, l2: 0.162758, l3: 0.162936, l4: 0.163556, l5: 0.174542, l6: 0.223339\n",
            "\n",
            "[epoch:  30/100000, batch:   156/  840, ite: 2043] train loss: 1.321090, tar: 0.170249 \n",
            "l0: 0.149730, l1: 0.149024, l2: 0.155143, l3: 0.165751, l4: 0.166952, l5: 0.188366, l6: 0.241538\n",
            "\n",
            "[epoch:  30/100000, batch:   168/  840, ite: 2044] train loss: 1.318714, tar: 0.169783 \n",
            "l0: 0.135560, l1: 0.134237, l2: 0.134954, l3: 0.143622, l4: 0.151595, l5: 0.173297, l6: 0.231754\n",
            "\n",
            "[epoch:  30/100000, batch:   180/  840, ite: 2045] train loss: 1.313965, tar: 0.169022 \n",
            "l0: 0.162538, l1: 0.172094, l2: 0.166706, l3: 0.185816, l4: 0.184380, l5: 0.194701, l6: 0.241003\n",
            "\n",
            "[epoch:  30/100000, batch:   192/  840, ite: 2046] train loss: 1.313818, tar: 0.168882 \n",
            "l0: 0.109026, l1: 0.114543, l2: 0.117609, l3: 0.112794, l4: 0.112443, l5: 0.129097, l6: 0.178784\n",
            "\n",
            "[epoch:  30/100000, batch:   204/  840, ite: 2047] train loss: 1.304467, tar: 0.167608 \n",
            "l0: 0.214531, l1: 0.221797, l2: 0.227741, l3: 0.239811, l4: 0.236338, l5: 0.244731, l6: 0.271998\n",
            "\n",
            "[epoch:  30/100000, batch:   216/  840, ite: 2048] train loss: 1.311810, tar: 0.168586 \n",
            "l0: 0.181122, l1: 0.188201, l2: 0.209280, l3: 0.208500, l4: 0.196645, l5: 0.191778, l6: 0.196040\n",
            "\n",
            "[epoch:  30/100000, batch:   228/  840, ite: 2049] train loss: 1.313030, tar: 0.168841 \n",
            "l0: 0.210346, l1: 0.205686, l2: 0.224734, l3: 0.218393, l4: 0.222804, l5: 0.230436, l6: 0.270312\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}